{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c7c5b4b",
   "metadata": {},
   "source": [
    "# Data Augmentation - Conditional Wasserstein GANs - GP\n",
    "\n",
    "### Dataset: Liver Graft Dataset\n",
    "\n",
    "This notebook presents the CWGAN-GP model to generate treated Intensity Data from the experimental liver graft dataset.\n",
    "\n",
    "Notebook Organization:\n",
    "- Read the dataset\n",
    "- Treatment, Univariate Analysis and PLS-DA of the dataset\n",
    "- Setup the CWGAN-GP model and train the model with intensity data\n",
    "- Generate artificial samples in an artificial dataset and compare them to the experimental data\n",
    "\n",
    "#### Due to stochasticity, re-running the notebook will get slightly different results. Thus, figures in the paper can be slightly different."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db562b1",
   "metadata": {},
   "source": [
    "\n",
    "#### Needed Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30b0029",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from time import perf_counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import scipy.spatial.distance as dist\n",
    "import scipy.cluster.hierarchy as hier\n",
    "import scipy.stats as stats\n",
    "import scipy.spatial\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.colors\n",
    "from matplotlib import ticker\n",
    "\n",
    "import seaborn as sns\n",
    "from collections import namedtuple, Counter\n",
    "\n",
    "from tqdm import tqdm\n",
    "from IPython import display as ipythondisplay\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "import sklearn.ensemble as skensemble\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "import sklearn.cluster as skclust\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, cross_validate, GridSearchCV\n",
    "from sklearn.metrics import (cohen_kappa_score, mean_squared_error, r2_score,\n",
    "                            adjusted_rand_score, roc_auc_score, roc_curve, auc, f1_score, silhouette_score)\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras import backend\n",
    "\n",
    "import pickle\n",
    "\n",
    "# Metabolinks package\n",
    "import metabolinks as mtl\n",
    "import metabolinks.transformations as transf\n",
    "\n",
    "# Python files in the repository\n",
    "import multianalysis as ma\n",
    "from elips import plot_confidence_ellipse\n",
    "import gan_evaluation_metrics as gem\n",
    "import linear_augmentation_functions as laf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e142d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import needed functions from GAN_functions\n",
    "from GAN_functions import gradient_penalty_cwgan\n",
    "from GAN_functions import critic_loss_wgan\n",
    "from GAN_functions import generator_loss_wgan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ba8112",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0d2aa2",
   "metadata": {},
   "source": [
    "# Liver Graft Dataset\n",
    "\n",
    "91 samples belonging to 37 liver graft (for transplant) biopsies, 27 after donor brain death and 10 after cardiac death and 11 Quality Controls. They were obtained in positive ionization mode in a FT-ICR-MS.\n",
    "\n",
    "Here, we excluded samples analysed during donor surgical phase and quality control, selecting the 74 samples from grafts analysed during cold-phase (after organ retrieval and transportation) and post reperfusion (reconnected graft and stabilised patient haemodynamic). After removing the outlier sample 21, we had 36 samples for each time point.\n",
    "\n",
    "Class labels: 'Cold-phase' and post 'Reperfusion'.\n",
    "selected 100 cell extract samples, 50 of them obtained in Complete medium - CC/Complete - and 50 in Defined medium - DC/Defined.\n",
    "\n",
    "Data acquired by:\n",
    "\n",
    "- Hrydziuszko, O.; Thamara, M.T.P.R.; Laing, R.; Kirwan, J.; Silva, M.A.; Richards, D.A.; Murphy, N.; Mirza, D.F.; Viant, M.R. Mass Spectrometry Based Metabolomics Comparison of Liver Grafts from Donors after Circulatory Death (DCD) and Donors after Brain Death (DBD) Used in Human Orthotopic Liver Transplantation. PLoS One 2016, 11, 1–16, doi:10.1371/journal.pone.0165884.\n",
    "\n",
    "- Data available in the paper.\n",
    "\n",
    "Data matrices retained only features that occur (globally) at least twice in all samples of the dataset (filtering/alignment)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70ad6b0",
   "metadata": {},
   "source": [
    "### Reading the data to be analysed and augmented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8fae69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_file = pd.read_excel('LGD_Dataset.XLSX').set_index('StudyID')\n",
    "base_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4da18ae",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Excluding the QC, Donor phase and sample 21\n",
    "selection = []\n",
    "for i in range(len(base_file.iloc[:,1])):\n",
    "    if base_file.iloc[i,1] in [3,4]:\n",
    "        selection.append(False)\n",
    "    elif base_file.index[i].startswith('21'):\n",
    "        selection.append(False)\n",
    "    else:\n",
    "        selection.append(True)\n",
    "base_file = base_file.loc[selection]\n",
    "base_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901d85d7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Creating the list of 'targets' (labels of samples) of the dataset\n",
    "labels_temp = list(base_file.iloc[:,1])\n",
    "labels = []\n",
    "for i in labels_temp:\n",
    "    if i == 1:\n",
    "        labels.append('Cold-phase')\n",
    "    elif i == 2:\n",
    "        labels.append('Reperfusion')\n",
    "labels\n",
    "pd.Series(labels).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be568a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = base_file.iloc[:,2:].copy()\n",
    "df = df.replace({0:np.nan})\n",
    "df_initial = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae10e871",
   "metadata": {},
   "outputs": [],
   "source": [
    "def characterize_data(dataset, name='dataset', target=None):\n",
    "    \"Returns some basic characteristics about the dataset.\"\n",
    "\n",
    "    n_samples, n_feats = dataset.shape\n",
    "\n",
    "    if target:\n",
    "        n_classes = len(np.unique(target))\n",
    "        Samp_Class = len(target)/len(np.unique(target)) # Number of Sample per Class\n",
    "\n",
    "    avg_feature_value = dataset.values.flatten()[~np.isnan(dataset.values.flatten())].mean() # Mean value in the dataset\n",
    "    max_feature_value = dataset.values.flatten()[~np.isnan(dataset.values.flatten())].max() # Maximum value in the dataset\n",
    "    min_feature_value = dataset.values.flatten()[~np.isnan(dataset.values.flatten())].min() # Minimum value in the dataset\n",
    "    std_feature_value = dataset.values.flatten()[~np.isnan(dataset.values.flatten())].std() # Standard Deviation value\n",
    "    median_feature_value = np.median(dataset.values.flatten()[~np.isnan(dataset.values.flatten())]) # Median value in the dataset\n",
    "\n",
    "    if target:\n",
    "        return {'Dataset': name,\n",
    "                '# samples': n_samples,\n",
    "                '# features': n_feats,\n",
    "                'feature value average (std)': f'{avg_feature_value} ({std_feature_value})',\n",
    "                'feature value ranges': f'({min_feature_value} - {max_feature_value})',\n",
    "                'feature value median': median_feature_value,\n",
    "                '# classes': n_classes,\n",
    "                'samples / class': Samp_Class,\n",
    "                } \n",
    "    else:\n",
    "        return {'Dataset': name,\n",
    "                '# samples': n_samples,\n",
    "                '# features': n_feats,\n",
    "                'Feature value average (std)': f'{avg_feature_value} ({std_feature_value})',\n",
    "                'Feature value ranges': f'({min_feature_value} - {max_feature_value})',\n",
    "                'Feature value median': median_feature_value,\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777368b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_characteristics = characterize_data(df, target=labels)\n",
    "data_characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc97c790",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the differences in intensities between samples to see if normalization is required\n",
    "f, ax = plt.subplots(1,1, figsize=(16,6))\n",
    "\n",
    "plt.bar(df.index, df.sum(axis=1))\n",
    "plt.ylabel('Sum of Intensities', fontsize=15)\n",
    "plt.xlabel('Samples', fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de2efdf",
   "metadata": {},
   "source": [
    "### Data Pre-Treatments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43552430",
   "metadata": {},
   "outputs": [],
   "source": [
    "df  = transf.keep_atleast(df, 2)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44ebc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Represents Binary Simplification pre-treatment\n",
    "def df_to_bool(df):\n",
    "    \"Transforms data into 'binary' matrices.\"\n",
    "    return df.mask(df.notnull(), 1).mask(df.isnull(), 0)\n",
    "\n",
    "# Performs pre-treatment combinations\n",
    "def compute_transf(df, norm_ref=None, lamb=None):\n",
    "    \"Computes combinations of pre-treatments and BinSim and returns after treatment datasets in a dict.\"\n",
    "    \n",
    "    data = df.copy()\n",
    "    \n",
    "    # Imputation of Missing Values\n",
    "    imputed = transf.fillna_frac_min_feature(data.T, fraction=0.2).T\n",
    "\n",
    "    # Normalization\n",
    "    if norm_ref is not None:\n",
    "        # Normalization by a reference feature\n",
    "        norm = transf.normalize_ref_feature(imputed, norm_ref, remove=True)\n",
    "    else:\n",
    "        # Normalization by the total sum of intensities\n",
    "        #norm = transf.normalize_sum(imputed)\n",
    "        # Normalization by PQN\n",
    "        norm = transf.normalize_PQN(imputed, ref_sample='mean')\n",
    "    \n",
    "    # Pareto Scaling and Generalized Logarithmic Transformation\n",
    "    P = transf.pareto_scale(imputed)\n",
    "    NP = transf.pareto_scale(norm)\n",
    "    NGP = transf.pareto_scale(transf.glog(norm, lamb=lamb))\n",
    "    GP = transf.pareto_scale(transf.glog(imputed, lamb=lamb))\n",
    "    \n",
    "    # Store results\n",
    "    dataset = {}\n",
    "    dataset['data'] = df\n",
    "\n",
    "    dataset['BinSim'] = df_to_bool(data)\n",
    "    dataset['Ionly'] = imputed\n",
    "    dataset['P'] = P\n",
    "    dataset['NP'] = NP\n",
    "    dataset['GP'] = GP\n",
    "    dataset['NGP'] = NGP\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc8ffa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.replace({0:np.nan})\n",
    "p_df = compute_transf(df, norm_ref=None, lamb=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb8c94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257a95a9",
   "metadata": {},
   "source": [
    "#### Colors for plots to ensure consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5a9526",
   "metadata": {},
   "outputs": [],
   "source": [
    "colours = sns.color_palette('Set1', 6)\n",
    "\n",
    "ordered_labels = set(labels)\n",
    "\n",
    "label_colors = {lbl: c for lbl, c in zip(ordered_labels, colours)}\n",
    "sample_colors = [label_colors[lbl] for lbl in labels]\n",
    "\n",
    "sns.palplot(label_colors.values())\n",
    "new_ticks = plt.xticks(range(len(ordered_labels)), ordered_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2e8af1",
   "metadata": {},
   "source": [
    "### Calculating Silhouette Coefficient\n",
    "\n",
    "The Silhouette coefficient was calculated (with scikit learn's `silhouette score`) using the class labels as the clusters in order to give a measure of how overlapped the biological classes are between the different datasets.\n",
    "\n",
    "The coefficient was calculated using all features of the dataset after pre-treatment and using the PCA Scores of the 2 first Principal Components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17fa8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Silhoutte Coefficient based on all the features of the dataset after pre-treatment\n",
    "print('Calculation based on all the features of the dataset after pre-treatment')\n",
    "print('Silhouette Coefficient:', silhouette_score(p_df['NGP'], labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56128a01",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Silhoutte Coefficient based on the PCA Scores of the samples on the 2 first Principal Components\n",
    "principaldf, var = ma.compute_df_with_PCs(p_df['NGP'], n_components=2, whiten=True,\n",
    "                                          labels=labels, return_var_ratios=True)\n",
    "s_score = silhouette_score(principaldf.iloc[:,:2], labels)\n",
    "print('Calculation based on the PCA Scores of the samples on the 2 first Principal Components')\n",
    "print('Silhouette Coefficient:', s_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5e05bc",
   "metadata": {},
   "source": [
    "## Unsupervised Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65f5350",
   "metadata": {},
   "source": [
    "#### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67384d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_PCA(principaldf, label_colors, components=(1,2), title=\"PCA\", ax=None):\n",
    "    \"Plot the projection of samples in the 2 main components of a PCA model.\"\n",
    "    \n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    \n",
    "    loc_c1, loc_c2 = [c - 1 for c in components]\n",
    "    col_c1_name, col_c2_name = principaldf.columns[[loc_c1, loc_c2]]\n",
    "    \n",
    "    #ax.axis('equal')\n",
    "    ax.set_xlabel(f'{col_c1_name}')\n",
    "    ax.set_ylabel(f'{col_c2_name}')\n",
    "\n",
    "    unique_labels = principaldf['Label'].unique()\n",
    "\n",
    "    for lbl in unique_labels:\n",
    "        subset = principaldf[principaldf['Label']==lbl]\n",
    "        ax.scatter(subset[col_c1_name],\n",
    "                   subset[col_c2_name],\n",
    "                   s=50, color=label_colors[lbl], label=lbl)\n",
    "\n",
    "    #ax.legend(framealpha=1)\n",
    "    ax.set_title(title, fontsize=15)\n",
    "\n",
    "def plot_ellipses_PCA(principaldf, label_colors, components=(1,2),ax=None, q=None, nstd=2):\n",
    "    \"Plot confidence ellipses of a class' samples based on their projection in the 2 main components of a PCA model.\"\n",
    "    \n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    \n",
    "    loc_c1, loc_c2 = [c - 1 for c in components]\n",
    "    points = principaldf.iloc[:, [loc_c1, loc_c2]]\n",
    "    \n",
    "    #ax.axis('equal')\n",
    "\n",
    "    unique_labels = principaldf['Label'].unique()\n",
    "\n",
    "    for lbl in unique_labels:\n",
    "        subset_points = points[principaldf['Label']==lbl]\n",
    "        plot_confidence_ellipse(subset_points, q, nstd, ax=ax, ec=label_colors[lbl], fc='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f3943f",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1, 1, figsize=(8,8))\n",
    "\n",
    "tf_fs = transf.FeatureScaler(method='standard')\n",
    "df = p_df['NGP']\n",
    "\n",
    "ax.axis('equal')\n",
    "principaldf, var = ma.compute_df_with_PCs(df, n_components=2, whiten=True, labels=labels, return_var_ratios=True)\n",
    "\n",
    "lcolors = label_colors\n",
    "\n",
    "plot_PCA(principaldf, lcolors, components=(1,2), title='', ax=ax)\n",
    "plot_ellipses_PCA(principaldf, lcolors, components=(1,2),ax=ax, q=0.95)\n",
    "\n",
    "ax.set_xlabel(f'PC 1 ({var[0] * 100:.1f} %)')\n",
    "ax.set_ylabel(f'PC 2 ({var[1] * 100:.1f} %)')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9defd88d",
   "metadata": {},
   "source": [
    "#### HCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b962aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_HCA(df, metric='euclidean', method='average'):\n",
    "    \"Performs Hierarchical Clustering Analysis of a data set with chosen linkage method and distance metric.\"\n",
    "    \n",
    "    distances = dist.pdist(df, metric=metric)\n",
    "    \n",
    "    # method is one of\n",
    "    # ward, average, centroid, single, complete, weighted, median\n",
    "    Z = hier.linkage(distances, method=method)\n",
    "\n",
    "    # Cophenetic Correlation Coefficient\n",
    "    # (see how the clustering - from hier.linkage - preserves the original distances)\n",
    "    coph = hier.cophenet(Z, distances)\n",
    "    # Baker's gamma\n",
    "    mr = ma.mergerank(Z)\n",
    "    bg = mr[mr!=0]\n",
    "\n",
    "    return {'Z': Z, 'distances': distances, 'coph': coph, 'merge_rank': mr, \"Baker's Gamma\": bg}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00106eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "HCA_all = {}\n",
    "for treat in 'Ionly', 'P', 'NP', 'GP', 'NGP', 'BinSim':\n",
    "    print(f'Performing HCA with treatment {treat}', end=' ...')\n",
    "    metric = 'jaccard' if treat == 'BinSim' else 'euclidean'\n",
    "    HCA_all[treat] = perform_HCA(p_df[treat], metric=metric, method='ward')\n",
    "    print('done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d432fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "\n",
    "def color_list_to_matrix_and_cmap(colors, ind, axis=0):\n",
    "        if any(issubclass(type(x), list) for x in colors):\n",
    "            all_colors = set(itertools.chain(*colors))\n",
    "            n = len(colors)\n",
    "            m = len(colors[0])\n",
    "        else:\n",
    "            all_colors = set(colors)\n",
    "            n = 1\n",
    "            m = len(colors)\n",
    "            colors = [colors]\n",
    "        color_to_value = dict((col, i) for i, col in enumerate(all_colors))\n",
    "\n",
    "        matrix = np.array([color_to_value[c]\n",
    "                           for color in colors for c in color])\n",
    "\n",
    "        matrix = matrix.reshape((n, m))\n",
    "        matrix = matrix[:, ind]\n",
    "        if axis == 0:\n",
    "            # row-side:\n",
    "            matrix = matrix.T\n",
    "\n",
    "        cmap = mpl.colors.ListedColormap(all_colors)\n",
    "        return matrix, cmap\n",
    "\n",
    "def plot_dendogram2(Z, leaf_names, label_colors, title='', ax=None, no_labels=False, labelsize=12, **kwargs):\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    hier.dendrogram(Z, labels=leaf_names, leaf_font_size=10, above_threshold_color='0.2', orientation='left',\n",
    "                    ax=ax, **kwargs)\n",
    "    #Coloring labels\n",
    "    #ax.set_ylabel('Distance (AU)')\n",
    "    ax.set_xlabel('Distance (AU)')\n",
    "    ax.set_title(title, fontsize = 15)\n",
    "    \n",
    "    #ax.tick_params(axis='x', which='major', pad=12)\n",
    "    ax.tick_params(axis='y', which='major', labelsize=labelsize, pad=12)\n",
    "    ax.spines['left'].set_visible(False)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    \n",
    "    #xlbls = ax.get_xmajorticklabels()\n",
    "    xlbls = ax.get_ymajorticklabels()\n",
    "    rectimage = []\n",
    "    for lbl in xlbls:\n",
    "        col = label_colors[lbl.get_text()]\n",
    "        lbl.set_color(col)\n",
    "        #lbl.set_fontweight('bold')\n",
    "        if no_labels:\n",
    "            lbl.set_color('w')\n",
    "        rectimage.append(col)\n",
    "\n",
    "    cols, cmap = color_list_to_matrix_and_cmap(rectimage, range(len(rectimage)), axis=0)\n",
    "\n",
    "    axins = inset_axes(ax, width=\"5%\", height=\"100%\",\n",
    "                   bbox_to_anchor=(1, 0, 1, 1),\n",
    "                   bbox_transform=ax.transAxes, loc=3, borderpad=0)\n",
    "\n",
    "    axins.pcolor(cols, cmap=cmap, edgecolors='w', linewidths=1)\n",
    "    axins.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eeb1d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with sns.axes_style(\"white\"):\n",
    "    f, axs = plt.subplots(1, 4, figsize=(14, 8), constrained_layout=True)\n",
    "    \n",
    "    for treatment, ax in zip(('P', 'NP', 'GP', 'NGP'), axs.ravel()):\n",
    "        plot_dendogram2(HCA_all[treatment]['Z'], \n",
    "                       labels, ax=ax,\n",
    "                       label_colors=label_colors,\n",
    "                       title=treatment, color_threshold=0)\n",
    "\n",
    "    st = f.suptitle(f\"1/5 Min Imputation\", fontsize=16)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63db81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with sns.axes_style(\"white\"):\n",
    "    f, axs = plt.subplots(1, 1, figsize=(4, 8), constrained_layout=True)\n",
    "    \n",
    "    treatment = 'BinSim'\n",
    "    ax = axs\n",
    "    plot_dendogram2(HCA_all[treatment]['Z'], \n",
    "                   labels, ax=ax,\n",
    "                   label_colors=label_colors,\n",
    "                   title=treatment, color_threshold=0)\n",
    "\n",
    "    #st = f.suptitle(f\"1/5 Min Imputation\", fontsize=16)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585aae24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_clustering_metrics(res_dict, labels):\n",
    "    \"\"\"Fill dict with clustering performance metrics.\"\"\"\n",
    "    \n",
    "    discrim = ma.dist_discrim(res_dict['Z'], labels, # all samples have the same order\n",
    "                              method = 'average')\n",
    "    res_dict['Average discrim dist'] = discrim[0]\n",
    "    correct = np.array(list(discrim[1].values()))\n",
    "    \n",
    "    classes = pd.unique(labels)\n",
    "    res_dict['% correct clustering'] = (100/len(classes)) * len(correct[correct>0])\n",
    "\n",
    "    # Correct First Cluster Percentage\n",
    "    res_dict['% correct 1st clustering'] = 100 * ma.correct_1stcluster_fraction(res_dict['Z'],labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce1f91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, res_dict in HCA_all.items():\n",
    "    compute_clustering_metrics(res_dict, labels)\n",
    "\n",
    "# Build table - summary of results\n",
    "clust_performance = {}\n",
    "\n",
    "for metric in ('Average discrim dist', '% correct clustering', '% correct 1st clustering'):\n",
    "    clust_performance[metric] = {d: HCA_all[d][metric] for d in HCA_all}\n",
    "clust_performance = pd.DataFrame(clust_performance, index=HCA_all)\n",
    "clust_performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12577109",
   "metadata": {},
   "source": [
    "#### K-Means Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b7c04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_KMeans(dataset, treatment, iter_num=150, best_fraction=0.1):\n",
    "    \"Perform K-means Clustering Analysis and calculate discrimination evaluation metrics.\"\n",
    "    \n",
    "    sample_labels = labels\n",
    "    n_classes = len(pd.unique(sample_labels))\n",
    "    \n",
    "    df = dataset[treatment]\n",
    "    \n",
    "    discrim = ma.Kmeans_discrim(df, sample_labels,\n",
    "                                method='average', \n",
    "                                iter_num=iter_num,\n",
    "                                best_fraction=best_fraction)\n",
    "\n",
    "    # Lists for the results of the best k-means clustering\n",
    "    average = []\n",
    "    correct = []\n",
    "    rand = []\n",
    "    \n",
    "    for j in discrim:\n",
    "        global_disc_dist, disc_dists, rand_index, SSE = discrim[j]\n",
    "        \n",
    "        # Average of discrimination distances\n",
    "        average.append(global_disc_dist) \n",
    "        \n",
    "        # Correct Clustering Percentages\n",
    "        all_correct = np.array(list(disc_dists.values()))\n",
    "        correct.append(len(all_correct[all_correct>0]))\n",
    "        \n",
    "        # Adjusted Rand Index\n",
    "        rand.append(rand_index) \n",
    "    \n",
    "    return{'dataset': dataset,\n",
    "           'treatment': treatment,\n",
    "           'Discrimination Distance': np.median(average),\n",
    "           '% correct clusters':np.median(correct)*100/n_classes,\n",
    "           'Rand Index': np.median(rand)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff702d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_num=15\n",
    "\n",
    "KMeans_all = []\n",
    "\n",
    "for treatment in ('Ionly', 'P', 'NP', 'GP', 'NGP', 'BinSim'):\n",
    "    print(f'performing KMeans with treatment {treatment}' , end=' ...')\n",
    "    KMeans_all.append(perform_KMeans(p_df, treatment, iter_num=iter_num))\n",
    "    print('done!')        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13525a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "KMeans_all = pd.DataFrame(KMeans_all).iloc[:,1:]\n",
    "KMeans_all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954c7e06",
   "metadata": {},
   "source": [
    "### Supervised Analysis\n",
    "\n",
    "#### RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3c7f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = labels\n",
    "GENERATE = True\n",
    "np.random.seed(5)\n",
    "if GENERATE:\n",
    "\n",
    "    # Vector with values for the parameter n_estimators\n",
    "    # Models will be built from 10 to 200 trees in 5 tree intervals\n",
    "    top_tree_in_grid=200\n",
    "    values = {'n_estimators': range(10,top_tree_in_grid,5)}\n",
    "\n",
    "    rf = skensemble.RandomForestClassifier(n_estimators=200)\n",
    "    clf = GridSearchCV(rf, values, cv=5)\n",
    "\n",
    "    # For each dataset,building the Random Forest models with the different number of trees\n",
    "    # and storing the predictive accuracy\n",
    "    RF_optim = {}\n",
    "    for treatment in ('Ionly','P', 'NP', 'GP', 'NGP', 'BinSim'):\n",
    "        print('Fitting to pre-treatment', treatment, '...', end=' ')\n",
    "        rfname = treatment\n",
    "        RF_optim[rfname] = {'treatment':treatment}\n",
    "        clf.fit(p_df[treatment], target)\n",
    "\n",
    "        RF_optim[rfname]['scores'] = list(clf.cv_results_['mean_test_score'])\n",
    "        RF_optim[rfname]['n_trees'] = list(clf.cv_results_['param_n_estimators'])\n",
    "\n",
    "        print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea9c242",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the results and adjusting parameters of the plot\n",
    "\n",
    "def plot_RF_otimization_ntrees(RF_optim, ax=None, ylabel='', title='', ylim=(30,101)):\n",
    "    p7 = sns.color_palette('tab20', 9)\n",
    "    to_plot = [optim for key, optim in RF_optim.items()]\n",
    "    treatments = ('Ionly', 'P', 'NP', 'GP', 'NGP', 'BinSim')\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    for treatment, color in zip(treatments, p7):\n",
    "        for optim in to_plot:\n",
    "            if optim['treatment'] == treatment:\n",
    "                break\n",
    "        ax.plot(optim['n_trees'], [s*100 for s in optim['scores']], label=treatment, color=color)\n",
    "    ax.set(ylabel=ylabel, xlabel='Number of Trees', ylim=ylim, title=title)\n",
    "    ax.legend()\n",
    "\n",
    "with sns.axes_style(\"whitegrid\"):\n",
    "    with sns.plotting_context(\"notebook\", font_scale=1.2):\n",
    "        f, ax = plt.subplots(1, 1, figsize=(6,6), constrained_layout=True)\n",
    "        \n",
    "        plot_RF_otimization_ntrees(RF_optim, ax=ax,\n",
    "                                   ylabel='Random Forest CV Mean Accuracy (%)',\n",
    "                                   title='')\n",
    "\n",
    "        f.suptitle('Optimization of the number of trees')\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355fb7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RF_model_CV - RF application and result extraction.\n",
    "def RF_model_CV(df, y, iter_num=1, n_fold=5, n_trees=200):\n",
    "    \"\"\"Performs stratified k-fold cross validation on Random Forest classifier of a dataset n times giving its accuracy and ordered\n",
    "    most important features.\n",
    "\n",
    "       Parameters are estimated by stratified k-fold cross-validation. Iteration changes the random sampling of the k-folds for\n",
    "    cross-validation.\n",
    "       Feature importance in the Random Forest models is calculated by the Gini Importance (feature_importances_) of scikit-learn.\n",
    "\n",
    "       df: Pandas DataFrame.\n",
    "       y: the target array (following scikit learn conventions)\n",
    "       iter_num: int (default - 1); number of iterations for CV.\n",
    "       n_fold: int (default - 5); number of groups to divide dataset in for stratified k-fold cross-validation\n",
    "            (max n_fold = minimum number of samples belonging to one group).\n",
    "       n_trees: int (default - 200); number of trees in each Random Forest.\n",
    "\n",
    "       Returns: (scores, import_features); \n",
    "            scores: list of the scores/accuracies of k-fold cross-validation of the random forests\n",
    "                (one score for each iteration and each group)\n",
    "            import_features: list of tuples (index number of feature, feature importance, feature name)\n",
    "                ordered by decreasing feature importance.\n",
    "    \"\"\"\n",
    "\n",
    "    nfeats = df.shape[1]\n",
    "\n",
    "    # Setting up variables for result storing\n",
    "    imp_feat = np.zeros((iter_num * n_fold, nfeats))\n",
    "    accuracy_scores = []\n",
    "    f1_scores = []\n",
    "    f = 0\n",
    "\n",
    "    # Number of times Random Forest cross-validation is made\n",
    "    # with `n_fold` randomly generated folds.\n",
    "    for _ in range(iter_num):\n",
    "        # Use stratified n_fold cross validation\n",
    "        kf = StratifiedKFold(n_fold, shuffle=True)\n",
    "        CV_accuracy_scores = []\n",
    "        CV_f1_scores = []\n",
    "        # Fit and evaluate a Random Forest model for each fold\n",
    "        for train_index, test_index in kf.split(df, y):\n",
    "            # Random Forest setup and fit\n",
    "            rf = skensemble.RandomForestClassifier(n_estimators=n_trees)\n",
    "            X_train, X_test = df.iloc[train_index, :], df.iloc[test_index, :]\n",
    "            y_train, y_test = [y[i] for i in train_index], [y[i] for i in test_index]\n",
    "            rf.fit(X_train, y_train)\n",
    "\n",
    "            # Compute performance and important features\n",
    "            CV_accuracy_scores.append(rf.score(X_test, y_test)) # Predictive Accuracy\n",
    "            #print(y_test, rf.predict(X_test))\n",
    "            CV_f1_scores.append(f1_score(y_true=y_test, y_pred=rf.predict(X_test), average='weighted')) # F1-Scores\n",
    "            #print(f1_score(y_true=y_test, y_pred=rf.predict(X_test), pos_label='Parkinson'))\n",
    "            imp_feat[f, :] = rf.feature_importances_ # Importance of each feature\n",
    "            f = f + 1\n",
    "\n",
    "        # Average Predictive Accuracy in this iteration\n",
    "        accuracy_scores.append(np.mean(CV_accuracy_scores))\n",
    "        f1_scores.append(np.mean(CV_f1_scores))\n",
    "\n",
    "    # Collect and order all important features values from each Random Forest\n",
    "    imp_feat_sum = imp_feat.sum(axis=0) / (iter_num * n_fold)\n",
    "    sorted_imp_feat = sorted(enumerate(imp_feat_sum), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # locs are sufficient as a reference to features\n",
    "    #imp_feat_tuples = [(loc, importance) for loc, importance in sorted_imp_feat]\n",
    "    \n",
    "    if iter_num == 1:\n",
    "        return {'accuracy': accuracy_scores[0], 'f1-score': f1_scores[0], 'important_features': sorted_imp_feat}\n",
    "    else:\n",
    "        return {'accuracy': accuracy_scores, 'f1-score': f1_scores, 'important_features': sorted_imp_feat}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75bcfa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_num=20\n",
    "\n",
    "RF_all = {}\n",
    "\n",
    "# Application of the Random Forests for each differently-treated dataset\n",
    "for treatment in ('Ionly', 'P', 'NP', 'GP', 'NGP', 'BinSim'):\n",
    "    print(f'Fitting random forest with treatment {treatment}', end=' ...')\n",
    "    rfname = treatment\n",
    "    RF_all[rfname] = {'treatment':treatment}\n",
    "    n_fold = 5\n",
    "\n",
    "    fit = RF_model_CV(p_df[treatment], target, iter_num=iter_num, n_fold=n_fold, n_trees=100)\n",
    "    RF_all[rfname].update(fit)\n",
    "\n",
    "    print(f'done')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ef0551",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy across the iterations\n",
    "accuracies = pd.DataFrame({name: RF_all[name]['accuracy'] for name in RF_all})\n",
    "# F1-Score across the iterations\n",
    "f1scores = pd.DataFrame({name: RF_all[name]['f1-score'] for name in RF_all})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc72632",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_stats = pd.DataFrame({'Average accuracy': accuracies.mean(axis=0),\n",
    "                               'STD': accuracies.std(axis=0)})\n",
    "accuracy_stats = accuracy_stats.assign(treatment=[RF_all[name]['treatment'] for name in RF_all])\n",
    "accuracy_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1c40c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1scores_stats = pd.DataFrame({'Average f1-scores': f1scores.mean(axis=0),\n",
    "                               'STD': f1scores.std(axis=0)})\n",
    "f1scores_stats = f1scores_stats.assign(treatment=[RF_all[name]['treatment'] for name in RF_all])\n",
    "f1scores_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2dc9e8",
   "metadata": {},
   "source": [
    "#### PLS-DA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c80882b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stdout\n",
    "# above is to supress PLS warnings\n",
    "\n",
    "max_comp=20\n",
    "np.random.seed(9125)\n",
    "# Store Results\n",
    "PLS_optim = {}\n",
    "\n",
    "# Build and extract metrics from models build with different number of components by using the optim_PLS function.\n",
    "for treatment in ('Ionly', 'P', 'NP', 'GP', 'NGP', 'BinSim'):\n",
    "    print(f'Fitting PLS-DA model with treatment {treatment}', end=' ...')\n",
    "    plsdaname = treatment\n",
    "    PLS_optim[plsdaname] = {'treatment':treatment}\n",
    "    n_fold = 5\n",
    "    optim = ma.optim_PLSDA_n_components(p_df[treatment], target,\n",
    "                                        max_comp=max_comp, n_fold=n_fold).CVscores\n",
    "    PLS_optim[plsdaname]['CV_scores'] = optim\n",
    "    print(f'done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71d527f",
   "metadata": {},
   "outputs": [],
   "source": [
    "PLS_optim['NGP']['CV_scores']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a912afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the results and adjusting plot parameters\n",
    "with sns.axes_style(\"whitegrid\"):\n",
    "    with sns.plotting_context(\"notebook\", font_scale=1.2):\n",
    "        f, ax = plt.subplots(1, 1, figsize = (8,8))\n",
    "        for name, data in PLS_optim.items():\n",
    "\n",
    "            # Negative Grapevine Dataset\n",
    "            ax.plot(range(1, len(data['CV_scores']) + 1), data['CV_scores'],\n",
    "                     label=data['treatment'])\n",
    "            ax.set(xlabel='Number of Components',\n",
    "                    ylabel='PLS Score (1 - PRESS/SS)',\n",
    "                    title='Negative Mode Grapevine Dataset')\n",
    "            ax.legend()\n",
    "            ax.set_ylim([-1, 1])\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af997bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PLSDA_model_CV(df, labels, n_comp=10,\n",
    "                   n_fold=5,\n",
    "                   iter_num=1,\n",
    "                   encode2as1vector=True,\n",
    "                   feat_type='Coef'):\n",
    "    \n",
    "    \"\"\"Perform PLS-DA with n-fold cross-validation.\n",
    "\n",
    "    df: pandas DataFrame; includes X equivalent in PLS-DA (training vectors).\n",
    "    labels: target labels.\n",
    "    n_comp: integer; number of components to use in PLS-DA.\n",
    "    n_fold: int (default: 5); number of groups to divide dataset in for k-fold cross-validation\n",
    "        (NOTE: max n_fold can not exceed minimum number of samples per class).\n",
    "    iter_num: int (default: 1); number of iterations that cross validation is repeated.\n",
    "    feat_type: string (default: 'Coef'); types of feature importance metrics to use; accepted: {'VIP', 'Coef', 'Weights'}.\n",
    "\n",
    "    Returns: (accuracy, n-fold score, r2score, import_features);\n",
    "        accuracy: list of accuracy values in group selection\n",
    "        n-fold score : n-fold cross-validation score\n",
    "        r2score: r2 score of the model\n",
    "        import_features: list of tuples (index number of feature, feature importance, feature name)\n",
    "            ordered by decreasing feature importance.\n",
    "    \"\"\"\n",
    "    # Setting up lists and matrices to store results\n",
    "    CVR2 = []\n",
    "    accuracies = []\n",
    "    f1scores = []\n",
    "    Imp_Feat = np.zeros((iter_num * n_fold, df.shape[1]))\n",
    "    f = 0\n",
    "\n",
    "    unique_labels = list(pd.unique(labels))\n",
    "\n",
    "    is1vector = len(unique_labels) == 2 and encode2as1vector\n",
    "\n",
    "    matrix = ma._generate_y_PLSDA(labels, unique_labels, is1vector)\n",
    "\n",
    "    if is1vector:\n",
    "        # keep a copy to use later\n",
    "        target1D = matrix.copy()\n",
    "\n",
    "    # Number of iterations equal to iter_num\n",
    "    for i in range(iter_num):\n",
    "        # use startified n-fold cross-validation with shuffling\n",
    "        kf = StratifiedKFold(n_fold, shuffle=True)\n",
    "        \n",
    "        # Setting up storing variables for n-fold cross-validation\n",
    "        nright = 0\n",
    "        cvr2 = []\n",
    "        cvf1scores = []\n",
    "\n",
    "        # Iterate through cross-validation procedure\n",
    "        for train_index, test_index in kf.split(df, labels):\n",
    "            plsda = PLSRegression(n_components=n_comp, scale=False)\n",
    "            X_train, X_test = df.iloc[train_index, :], df.iloc[test_index, :]\n",
    "            if not is1vector:\n",
    "                y_train = matrix.iloc[train_index, :].copy()\n",
    "                y_test = matrix.iloc[test_index, :].copy()\n",
    "\n",
    "            else:\n",
    "                y_train, y_test = target1D[train_index], target1D[test_index]\n",
    "                correct = target1D[test_index]\n",
    "\n",
    "            # Fit PLS model\n",
    "            plsda.fit(X=X_train, Y=y_train)\n",
    "\n",
    "            # Obtain results with the test group\n",
    "            y_pred = plsda.predict(X_test)\n",
    "            cvr2.append(r2_score(y_test, y_pred))\n",
    "            \n",
    "            if not is1vector:\n",
    "                rounded_pred = y_pred.copy()\n",
    "                for i in range(len(y_pred)):\n",
    "                    \n",
    "                    for l in range(len(y_pred[i])):\n",
    "                        if y_pred[i,l] > 0.5:\n",
    "                            rounded_pred[i, l] = 1\n",
    "                        else:\n",
    "                            rounded_pred[i, l] = 0\n",
    "\n",
    "                cvf1scores.append(f1_score(y_test, rounded_pred, average='weighted'))\n",
    "    \n",
    "\n",
    "            if not is1vector:\n",
    "                for i in range(len(y_pred)):\n",
    "                    if list(y_test.iloc[i, :]).index(max(y_test.iloc[i, :])) == np.argmax(\n",
    "                        y_pred[i]\n",
    "                    ):\n",
    "                        nright += 1  # Correct prediction\n",
    "            else:\n",
    "                \n",
    "                rounded = np.round(y_pred)\n",
    "                for p in range(len(y_pred)):\n",
    "                    if rounded[p] >= 1:\n",
    "                        pred = 1\n",
    "                        rounded[p] = 1\n",
    "                    else:\n",
    "                        pred = 0\n",
    "                        rounded[p] = 0\n",
    "                    if pred == correct[p]:\n",
    "                        nright += 1  # Correct prediction\n",
    "\n",
    "                # Calculate accuracy for this iteration\n",
    "                cvf1scores.append(f1_score(correct, rounded, pos_label=0))\n",
    "\n",
    "            # Calculate important features (3 different methods to choose from)\n",
    "            if feat_type == 'VIP':\n",
    "                Imp_Feat[f, :] = ma._calculate_vips(plsda)\n",
    "            elif feat_type == 'Coef':\n",
    "                Imp_Feat[f, :] = abs(plsda.coef_).sum(axis=1)\n",
    "            elif feat_type == 'Weights':\n",
    "                Imp_Feat[f, :] = abs(plsda.x_weights_).sum(axis=1)\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    'Type not Recognized. Types accepted: \"VIP\", \"Coef\", \"Weights\"'\n",
    "                )\n",
    "\n",
    "            f += 1\n",
    "\n",
    "        # Calculate the accuracy of the group predicted and storing score results\n",
    "        accuracies.append(nright / len(labels))\n",
    "        CVR2.append(np.mean(cvr2))\n",
    "        f1scores.append(np.mean(cvf1scores))\n",
    "\n",
    "    # Join and sort all important features values from each cross validation group and iteration.\n",
    "    Imp_sum = Imp_Feat.sum(axis=0) / (iter_num * n_fold)\n",
    "    imp_features = sorted(enumerate(Imp_sum), key=lambda x: x[1], reverse=True)\n",
    "    if iter_num == 1:\n",
    "        return {'accuracy': accuracies[0], 'Q2': CVR2[0], 'f1-score': f1scores[0], 'important_features': imp_features}\n",
    "    else:\n",
    "        return {'accuracy': accuracies, 'Q2': CVR2, 'f1-score': f1scores, 'important_features': imp_features}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0220b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stdout\n",
    "\n",
    "PLSDA_all = {}\n",
    "\n",
    "iter_num=20\n",
    "\n",
    "for treatment in ('Ionly', 'P', 'NP', 'GP', 'NGP', 'BinSim'):\n",
    "    print(f'Fitting a PLS-DA model with treatment {treatment}', end=' ...')\n",
    "    plsdaname = treatment\n",
    "    PLSDA_all[plsdaname] = {'treatment':treatment}\n",
    "    n_comp = 4\n",
    "    n_fold = 5\n",
    "    fit = PLSDA_model_CV(p_df[treatment], target,\n",
    "                            n_comp=n_comp, n_fold=n_fold,\n",
    "                            iter_num=iter_num,\n",
    "                            feat_type='VIP')\n",
    "    PLSDA_all[plsdaname].update(fit)\n",
    "    print(f'done')     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833e8d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = pd.DataFrame({name: PLSDA_all[name]['accuracy'] for name in PLSDA_all})\n",
    "f1scores = pd.DataFrame({name: PLSDA_all[name]['f1-score'] for name in PLSDA_all})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d27c181",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_stats = pd.DataFrame({'Average accuracy': accuracies.mean(axis=0),\n",
    "                               'STD': accuracies.std(axis=0)})\n",
    "accuracy_stats = accuracy_stats.assign(treatment=[PLSDA_all[name]['treatment'] for name in PLSDA_all])\n",
    "accuracy_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ebcb9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1scores_stats = pd.DataFrame({'Average f1scores': f1scores.mean(axis=0),\n",
    "                               'STD': f1scores.std(axis=0)})\n",
    "f1scores_stats = f1scores_stats.assign(treatment=[PLSDA_all[name]['treatment'] for name in PLSDA_all])\n",
    "f1scores_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f67577",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = 4 # Nº of componentes\n",
    "\n",
    "model, scores = ma.fit_PLSDA_model(p_df['NGP'], target,\n",
    "                                n_comp=n_components, scale=False, # Only true if scaling was not done earlier\n",
    "                                encode2as1vector=True,\n",
    "                                lv_prefix='LV ', label_name='Label')\n",
    "\n",
    "lcolors = label_colors\n",
    "\n",
    "with sns.axes_style(\"whitegrid\"):\n",
    "    with sns.plotting_context(\"notebook\", font_scale=1.2):\n",
    "        fig, ax = plt.subplots(1,1, figsize=(6,6)) # Set up fig size\n",
    "        plot_PCA(scores, lcolors, title=\"PLS Projection\", ax=ax,\n",
    "                components=(1,2)) # Select components to see\n",
    "        plt.title('PLS Projection', fontsize=20) # Title\n",
    "        plt.legend(loc='upper right', ncol=1, fontsize=15)  # Legend           \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17a4a87",
   "metadata": {},
   "source": [
    "Data pre-treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44b4813",
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_feat_filtering(file, target=None, filt_method='total_samples', filt_kw=2,\n",
    "                  extra_filt=None, extra_filt_kw='Formula', extra_filt_data=None):\n",
    "    \"Performs feature filtering in 2 steps.\"\n",
    "    \n",
    "    # Filtering based on the sampels each feature appears in\n",
    "    if filt_method == 'total_samples': # Filter features based on the times (filt_kw) they appear in the dataset\n",
    "        # Minimum can be a percentage if it is a value between 0 and 1!\n",
    "        data_filt = transf.keep_atleast(file, minimum=filt_kw)\n",
    "    elif filt_method == 'class_samples': # Features retained if they appear filt_kw times in the samples of at least one class\n",
    "        # Minimum can be a percentage if it is a value between 0 and 1!\n",
    "        data_filt = transf.keep_atleast(file, minimum=filt_kw, y=np.array(target))\n",
    "    elif filt_method == None: # No filtering\n",
    "        data_filt = file.copy()\n",
    "    else:\n",
    "        raise ValueError('Feature Filtering strategy not accepted/implemented in function. Implement if new strategy.')\n",
    "        \n",
    "    # Extra filtering based if the features are annotated\n",
    "    if extra_filt == 'Formula': # Filter features based if they have a formula on the dataset\n",
    "        idxs_to_keep = [i for i in data_filt.index if type(extra_filt_data.loc[i, 'Formula']) == str]\n",
    "        data_filt = data_filt.loc[idxs_to_keep]\n",
    "    elif extra_filt == 'Name': # Filter features based if they have an annotated name on the dataset\n",
    "        idxs_to_keep = [i for i in data_filt.index if type(extra_filt_data.loc[i, 'Name']) == str]\n",
    "        data_filt = data_filt.loc[idxs_to_keep]\n",
    "    elif extra_filt == None: # No extra filtering\n",
    "        data_filt = data_filt.copy()\n",
    "    else:\n",
    "        raise ValueError('Feature Filtering strategy not accepted/implemented in function. Implement if new strategy.')\n",
    "    \n",
    "    return data_filt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e40a7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "filt_sample_data = basic_feat_filtering(df_initial, target, filt_method='total_samples', filt_kw=2).T\n",
    "imputed = transf.fillna_frac_min_feature(filt_sample_data, fraction=0.2).T\n",
    "normalized = transf.normalize_sum(imputed)\n",
    "real_samples_all = transf.pareto_scale(transf.glog(normalized, lamb=None))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43233e04",
   "metadata": {},
   "source": [
    "#### Univariate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f78e08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_results = ma.compute_FC_pvalues_2groups(normalized, real_samples_all,\n",
    "                               labels=target,\n",
    "                               equal_var=True,\n",
    "                               alpha=0.05, useMW=False)\n",
    "uni_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4869db",
   "metadata": {},
   "outputs": [],
   "source": [
    "pvalue = 0.0001\n",
    "log2FC = 1\n",
    "\n",
    "b = uni_results[uni_results['FDR adjusted p-value'] < pvalue]\n",
    "uni_results_filt = b[abs(b['log2FC']) > log2FC]\n",
    "uni_results_filt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c8ab05",
   "metadata": {},
   "source": [
    "#### Linear Augmentation / Interpolation of the experimental data and corresponding sample labels/classes to use as training data for the GAN network.\n",
    "\n",
    "Pre-treatment on the linearly augmented data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302a00d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = perf_counter()\n",
    "data, lbls = laf.artificial_dataset_generator(imputed, labels=labels,\n",
    "                                    max_new_samples_per_label=512, binary=False, rnd=list(np.linspace(0.15,0.9,6)), \n",
    "                                    binary_rnd_state=None, rnd_state=45236)\n",
    "\n",
    "data_N = transf.normalize_sum(data)\n",
    "data_treated = transf.pareto_scale(transf.glog(data_N, lamb=None))\n",
    "\n",
    "end = perf_counter()\n",
    "print(f'Simple augmentation of data done! Took {(end - start):.3f} s')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d50f520",
   "metadata": {},
   "source": [
    "### Inputs for GAN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4dfa176",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get distribution of intensity values of the dataset\n",
    "hist = np.histogram(real_samples_all.values.flatten(), bins=100)\n",
    "input_realdata_dist = stats.rv_histogram(hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770c784c",
   "metadata": {},
   "source": [
    "Set up colours for each of the classes. Generated samples will have the corresponding label with '- GAN' after."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f3b2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colors to use in plots\n",
    "colours2 = sns.color_palette('tab20', 4)#[:6]\n",
    "\n",
    "ordered_labels_test = []\n",
    "for i in ['Cold-phase', 'Reperfusion']:\n",
    "    ordered_labels_test.extend([i, i + ' - GAN'])\n",
    "label_colors_test = {lbl: c for lbl, c in zip(ordered_labels_test, colours2)}\n",
    "\n",
    "sns.palplot(label_colors_test.values())\n",
    "new_ticks_test = plt.xticks(range(len(ordered_labels_test)), ordered_labels_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0520e74a",
   "metadata": {},
   "source": [
    "## Conditional Wasserstein GAN - GP model\n",
    "\n",
    "This model construction was made by joining WGAN-GP models with Conditional GAN models. WGAN-GP models were originally made according to / originally based in https://keras.io/examples/generative/wgan_gp/#wasserstein-gan-wgan-with-gradient-penalty-gp and Conditional GAN models - https://machinelearningmastery.com/how-to-develop-a-conditional-generative-adversarial-network-from-scratch/ (generator and discriminator model) and https://keras.io/examples/generative/conditional_gan/ without using OOP (loss functions and training/training steps)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f5a8bf",
   "metadata": {},
   "source": [
    "Functions for the generator and critic (discriminator) models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43aa5021",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_model(len_input, len_output, n_hidden_nodes, n_labels): \n",
    "    \"Make the generator model of CWGAN-GP.\"\n",
    "\n",
    "    data_input = tf.keras.Input(shape=(len_input,), name='data') # Take intensity input\n",
    "    label_input = tf.keras.Input(shape=(1,), name='label') # Take Label Input\n",
    "    \n",
    "    # Treat label input to concatenate to intensity data after\n",
    "    label_m = tf.keras.layers.Embedding(n_labels, 30, input_length=1)(label_input)\n",
    "    label_m = tf.keras.layers.Dense(256, activation='linear', use_bias=True)(label_m)\n",
    "    #label_m = tf.keras.layers.Reshape((len_input,1,))(label_m)\n",
    "    label_m2 = tf.keras.layers.Reshape((256,))(label_m)\n",
    "\n",
    "    joined_data = tf.keras.layers.Concatenate()([data_input, label_m2]) # Concatenate intensity and label data\n",
    "    # Hidden Dense Layer and Normalization\n",
    "    joined_data = tf.keras.layers.Dense(n_hidden_nodes, activation=tf.nn.leaky_relu, use_bias=True)(joined_data)\n",
    "    joined_data = tf.keras.layers.Dense(256, activation=tf.nn.leaky_relu, use_bias=True)(joined_data)\n",
    "    joined_data = tf.keras.layers.BatchNormalization()(joined_data)\n",
    "    \n",
    "    # Output - number of features of sample to make\n",
    "    output = tf.keras.layers.Dense(len_output, activation='linear', use_bias=True)(joined_data)\n",
    "    \n",
    "    generator = tf.keras.Model(inputs=[data_input, label_input], outputs=output)\n",
    "    \n",
    "    return generator\n",
    "\n",
    "\n",
    "def critic_model(len_input, n_hidden_nodes, n_labels):\n",
    "    \"Make the critic model of CWGAN-GP.\"\n",
    "    \n",
    "    label_input = tf.keras.Input(shape=(1,)) # Take intensity input\n",
    "    data_input = tf.keras.Input(shape=(len_input,)) # Take Label Input\n",
    "\n",
    "    # Treat label input to concatenate to intensity data after\n",
    "    label_m = tf.keras.layers.Embedding(n_labels, 30, input_length=1)(label_input)\n",
    "    label_m = tf.keras.layers.Dense(256, activation='linear', use_bias=True)(label_m)\n",
    "    #label_m = tf.keras.layers.Reshape((len_input,1,))(label_m)\n",
    "    label_m = tf.keras.layers.Reshape((256,))(label_m)\n",
    "\n",
    "    joined_data = tf.keras.layers.Concatenate()([data_input, label_m]) # Concatenate intensity and label data\n",
    "    # Hidden Dense Layer (Normalization worsened results here)\n",
    "    joined_data = tf.keras.layers.Dense(n_hidden_nodes, activation=tf.nn.leaky_relu, use_bias=True)(joined_data)\n",
    "    joined_data = tf.keras.layers.Dense(128, activation=tf.nn.leaky_relu, use_bias=True)(joined_data)\n",
    "    joined_data = tf.keras.layers.Dense(256, activation=tf.nn.leaky_relu, use_bias=True)(joined_data)\n",
    "    #joined_data = tf.keras.layers.BatchNormalization()(joined_data)\n",
    "\n",
    "    # Output Layer - 1 node for critic decision\n",
    "    output = tf.keras.layers.Dense(1, activation='linear', use_bias=True)(joined_data)\n",
    "    \n",
    "    critic = tf.keras.Model(inputs=[data_input, label_input], outputs=output)\n",
    "\n",
    "    return critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f1c898",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_predictions(model, num_examples_to_generate, len_input, input_dist, uni_lbls):\n",
    "    \"Generate sample predictions based on a Generator model.\"\n",
    "    \n",
    "    test_input =  tf.constant(input_dist.rvs(size=len_input*num_examples_to_generate), shape=[\n",
    "        num_examples_to_generate,len_input]) \n",
    "    \n",
    "    if len(uni_lbls) < 3:\n",
    "        test_labels = tf.constant([1.0]*(num_examples_to_generate//2) + [0.0]*(num_examples_to_generate//2), \n",
    "                                  shape=(num_examples_to_generate,1))\n",
    "    else:\n",
    "        test_labels = []\n",
    "        for i in range(len(uni_lbls)):\n",
    "            test_labels.extend([i]*(num_examples_to_generate//len(uni_lbls)))\n",
    "        test_labels = np.array(pd.get_dummies(test_labels))\n",
    "        #np.array(pd.get_dummies([i for i in range(len(uni_lbls))]*(num_examples_to_generate//len(uni_lbls))))\n",
    "    predictions = model([test_input, test_labels], training=False) # `training` is set to False.\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d29dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_montage(train_data_o, train_lbls, test_data, test_lbls,\n",
    "                     epochs, generator, critic, generator_optimizer, critic_optimizer, input_dist,\n",
    "                    batch_size, grad_pen_weight=10, k_cov_den=50, k_crossLID=15, random_seed=145,\n",
    "                    n_generated_samples=96):\n",
    "    \"\"\"Train a generator and critic of CWGAN-GP.\n",
    "    \n",
    "       Receives training data and respective class labels (train_data_o and train_lbls) and trains a generator and a critic\n",
    "        model (generator, critic) over a number of epochs (epochs) with a set batch size (batch_size) with the respective \n",
    "        optimizers and learning rate (generator_optimizer, critic_optimizer). Gradient Penalty is calculated with\n",
    "        grad_pen_weight as the weight of the penalty.\n",
    "       The functions returns at time intervals three graphs to evaluate the progression of the models (Loss plots,\n",
    "        coverage, density, crossLID and correct first cluster plots and PCA plot with generated and test data). To this\n",
    "        end, samples need to be generated requiring the distribution to sample the initial input values from (input_dist),\n",
    "        and test data and respective labels has to be given (test_data and test_lbls). Finally the number of neighbors to\n",
    "        consider for coverage/density and crossLID calculation is also needed (k_cov_den, k_crossLID).\n",
    "    \n",
    "       train_data_o: Pandas DataFrame with training data;\n",
    "       train_lbls: List with training data class labels;\n",
    "       test_data: Pandas DataFrame with test data to evaluate the model;\n",
    "       test_lbls: List with test data class labels to evaluate the model;\n",
    "       epochs: Int value with the number of epochs to train the model;\n",
    "       generator: tensorflow keras.engine.functional.Functional model for the generator;\n",
    "       critic: tensorflow keras.engine.functional.Functional model for the critic;\n",
    "       generator_optimizer: tensorflow keras optimizer (with learning rate) for generator;\n",
    "       critic_optimizer: tensorflow keras optimizer (with learning rate) for critic;\n",
    "       input_dist: scipy.stats._continuous_distns.rv_histogram object - distribution to sample input values for generator;\n",
    "       batch_size: int value with size of batch for model training;\n",
    "       grad_pen_weight: int value (default 10) for penalty weight in gradient penalty calculation;\n",
    "       k_cov_den: int value (default 50) for number of neighbors to consider for coverage and density calculation in\n",
    "       generated samples evaluation;\n",
    "       k_crossLID: int value (default 15) for number of neighbors to consider for crossLID calculation in generated samples\n",
    "        evaluation.\n",
    "       random_seed: int value (default 145) for numpy random seeding when randomly organizing samples in the data that\n",
    "        will be split into batches.\n",
    "       n_generated_samples: int value (default 96) for number of samples generated to test the model during training.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Obtaining the train data, randomize its order and divide it be twice the standard deviation of its values\n",
    "    all_data = train_data_o.iloc[\n",
    "        np.random.RandomState(seed=random_seed).permutation(len(train_data_o))]/(2*train_data_o.values.std())\n",
    "    \n",
    "    # Same treatment for the test data\n",
    "    test_data = (test_data/(2*test_data.values.std())).values\n",
    "    training_data = all_data\n",
    "    train_data = all_data.values\n",
    "    \n",
    "    # Change class labels to numerical values while following the randomized ordered of samples\n",
    "    if len(set(train_lbls)) < 3: # 1 and 0 for when there are only two classes\n",
    "        train_labels = pd.get_dummies(\n",
    "            np.array(train_lbls)[np.random.RandomState(seed=random_seed).permutation(len(train_data))]).values[:,0]\n",
    "        test_labels = pd.get_dummies(np.array(test_lbls)).values[:,0]\n",
    "    else: # One hot encoding for when there are more than two classes\n",
    "        train_labels = pd.get_dummies(\n",
    "            np.array(train_lbls)[np.random.RandomState(seed=random_seed).permutation(len(train_data))]).values\n",
    "        test_labels = pd.get_dummies(np.array(test_lbls)).values\n",
    "    # Save the order of the labels\n",
    "    ordered_labels = pd.get_dummies(\n",
    "            np.array(train_lbls)[np.random.RandomState(seed=random_seed).permutation(len(train_data_o))]).columns\n",
    "\n",
    "    batch_divisions = int(batch_size / len(set(train_lbls))) # See how many samples of each class will be in each batch\n",
    "    n_steps = epochs * int(training_data.shape[0] / batch_size) # Number of steps: nº of batches per epoch * nº of epochs\n",
    "    n_critic = 5\n",
    "    \n",
    "    # Set up the evaluating images printed during training and the intervals they will be updated\n",
    "    f, (axl, axc, axr) = plt.subplots(1, 3, figsize = (16,5))\n",
    "    update1 = n_steps//200\n",
    "    update2 = n_steps//20\n",
    "\n",
    "    if hasattr(tqdm, '_instances'):\n",
    "        tqdm._instances.clear() # clear if it exists\n",
    "\n",
    "    i=0\n",
    "\n",
    "    for step in tqdm(range(n_steps)):\n",
    "        \n",
    "        # Critic Training\n",
    "        crit_loss_temp = []\n",
    "        \n",
    "        # Select real samples for this batch on training and order samples to put samples of the same class together\n",
    "        real_samp = train_data[i*batch_size:(i+1)*batch_size]\n",
    "        real_lbls = train_labels[i*batch_size:(i+1)*batch_size]\n",
    "\n",
    "        real_samples = np.empty(real_samp.shape)\n",
    "        real_labels = np.empty(real_lbls.shape)\n",
    "        a = 0\n",
    "        if len(set(train_lbls)) < 3:\n",
    "            for l,s in sorted(zip(real_lbls, real_samp), key=lambda pair: pair[0], reverse=True):\n",
    "                real_samples[a] = s\n",
    "                real_labels[a] = l\n",
    "                a = a+1\n",
    "        else:\n",
    "            for l,s in sorted(zip(real_lbls, real_samp), key=lambda pair: np.argmax(pair[0]), reverse=False):\n",
    "                #print(l, np.argmax(l))\n",
    "                real_samples[a] = s\n",
    "                real_labels[a] = l\n",
    "                a = a+1\n",
    "\n",
    "        for _ in range(n_critic): # For each step, train critic n_critic times\n",
    "            \n",
    "            # Generate input for generator\n",
    "            artificial_samples = tf.constant(input_dist.rvs(size=all_data.shape[1]*batch_size), shape=[\n",
    "                batch_size,all_data.shape[1]])\n",
    "            artificial_labels = real_labels.copy()\n",
    "\n",
    "            # Generate artificial samples from the latent vector\n",
    "            artificial_samples = generator([artificial_samples, artificial_labels], training=True)\n",
    "            \n",
    "            with tf.GradientTape() as crit_tape: # See the gradient for the critic\n",
    "\n",
    "                # Get the logits for the generated samples\n",
    "                X_artificial = critic([artificial_samples, artificial_labels], training=True)\n",
    "                # Get the logits for the real samples\n",
    "                X_true = critic([real_samples, real_labels], training=True)\n",
    "\n",
    "                # Calculate the critic loss using the generated and real sample results\n",
    "                c_cost = critic_loss_wgan(X_true, X_artificial)\n",
    "\n",
    "                # Calculate the gradient penalty\n",
    "                grad_pen = gradient_penalty_cwgan(batch_size, real_samples, artificial_samples,\n",
    "                                                  real_labels, artificial_labels, critic)\n",
    "                # Add the gradient penalty to the original discriminator loss\n",
    "                crit_loss = c_cost + grad_pen * grad_pen_weight\n",
    "                \n",
    "            crit_loss_temp.append(crit_loss)\n",
    "\n",
    "            # Calculate and apply the gradients obtained from the loss on the trainable variables\n",
    "            gradients_of_critic = crit_tape.gradient(crit_loss, critic.trainable_variables)\n",
    "            critic_optimizer.apply_gradients(zip(gradients_of_critic, critic.trainable_variables))\n",
    "\n",
    "        i = i + 1\n",
    "        if (step+1) % (n_steps//epochs) == 0:\n",
    "            i=0\n",
    "\n",
    "        crit_loss_all.append(np.mean(crit_loss_temp))\n",
    "        \n",
    "        # Generator Training\n",
    "        # Generate inputs for generator, values and labels\n",
    "        artificial_samples = tf.constant(input_dist.rvs(size=all_data.shape[1]*batch_size), shape=[\n",
    "                batch_size,all_data.shape[1]])\n",
    "        \n",
    "        if len(set(train_lbls)) < 3:\n",
    "            artificial_labels = tf.constant([1.0]*(batch_size//2) + [0.0]*(batch_size//2), shape=(batch_size,1))\n",
    "        else:\n",
    "            artificial_labels = np.array(pd.get_dummies([i for i in range(len(set(train_lbls)))]*batch_divisions))\n",
    "    \n",
    "        with tf.GradientTape() as gen_tape: # See the gradient for the generator\n",
    "            # Generate artificial samples\n",
    "            artificial_samples = generator([artificial_samples, artificial_labels], training=True)\n",
    "            \n",
    "            # Get the critic results for generated samples\n",
    "            X_artificial = critic([artificial_samples, artificial_labels], training=True)\n",
    "            # Calculate the generator loss\n",
    "            gen_loss = generator_loss_wgan(X_artificial)\n",
    "\n",
    "        # Calculate and apply the gradients obtained from the loss on the trainable variables\n",
    "        gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "        generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "        gen_loss_all.append(gen_loss)\n",
    "\n",
    "        # Update the progress bar and evaluation graphs every update1 steps for loss plots and update2 for the others.\n",
    "        if (step + 1) % update1 == 0:\n",
    "            \n",
    "            # Update the evaluating figures at the set intervals\n",
    "            axl.clear() # Always clear the corresponding ax before redrawing it\n",
    "            \n",
    "            # Loss Plot\n",
    "            axl.plot(gen_loss_all, color = 'blue', label='Generator Loss')\n",
    "            axl.plot(crit_loss_all,color = 'red', label='Critic Loss')\n",
    "            axl.set_xlabel('Number of Steps')\n",
    "            axl.set_ylabel('Loss')\n",
    "            axl.legend()\n",
    "            \n",
    "            ipythondisplay.clear_output(wait=True)\n",
    "            ipythondisplay.display(plt.gcf())\n",
    "\n",
    "        if (step + 1) % update2 == 0:\n",
    "\n",
    "            saved_predictions.append(generate_predictions(generator, n_generated_samples, all_data.shape[1], \n",
    "                                                          input_realdata_dist, ordered_labels))\n",
    "            # See density and coverage and crossLID (divided by 25 to be in the same order as the rest) \n",
    "            # of latest predictions\n",
    "            den, cov = gem.evaluation_coverage_density(test_data, saved_predictions[-1], k= k_cov_den, metric='euclidean')\n",
    "            clid = gem.cross_LID_estimator_byMLE(test_data, saved_predictions[-1], k=k_crossLID, metric='euclidean')/25\n",
    "            density.append(den)\n",
    "            coverage.append(cov)\n",
    "            crossLID.append(clid)\n",
    "\n",
    "            # PCA of the latest predictions and training data\n",
    "            # Divide by twice the standard deviation to be the same as the generated data\n",
    "            dfs_temp = pd.concat((train_data_o/(2*train_data_o.values.std()),pd.DataFrame(\n",
    "                saved_predictions[-1].numpy(), columns=train_data_o.columns))) \n",
    "            temp_lbls = train_lbls.copy()\n",
    "            for l in ordered_labels:\n",
    "                temp_lbls.extend([l+' - GAN']*(n_generated_samples//len(ordered_labels)))\n",
    "            principaldf = gem.pca_sample_projection(dfs_temp, temp_lbls, pca, whiten=True, \n",
    "                                                samp_number=len(train_data_o.index))\n",
    "            lcolors = label_colors_test\n",
    "\n",
    "            # Hierarchical clustering of the latest predictions and testing data, \n",
    "            # saving the correct 1st cluster fraction results\n",
    "            dfs_temp = np.concatenate((test_data, saved_predictions[-1].numpy()))\n",
    "            temp_lbls = ['real']*len(test_data) + ['gen']*len(saved_predictions[-1])\n",
    "            hca_results = gem.perform_HCA(dfs_temp, temp_lbls, metric='euclidean', method='ward')\n",
    "            corr1stcluster.append(hca_results['correct 1st clustering'])\n",
    "            \n",
    "            # Plots\n",
    "            axc.clear()\n",
    "            axc.plot(range(update2, step+2, update2), coverage, label='coverage')\n",
    "            axc.plot(range(update2, step+2, update2), density, label='density')\n",
    "            axc.plot(range(update2, step+2, update2), crossLID, color='red', label='crossLID')\n",
    "            axc.plot(range(update2, step+2, update2), corr1stcluster, color='purple', label='corr_cluster')\n",
    "            axc.legend()\n",
    "\n",
    "            axr.clear()\n",
    "            gem.plot_PCA(principaldf, lcolors, components=(1,2), title='', ax=axr)\n",
    "            axr.legend(loc='upper right', ncol=1, framealpha=1)\n",
    "            \n",
    "            ipythondisplay.clear_output(wait=True)\n",
    "            ipythondisplay.display(plt.gcf())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59bae48a",
   "metadata": {},
   "source": [
    "### Training the GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b969781",
   "metadata": {},
   "outputs": [],
   "source": [
    "GENERATE=True\n",
    "\n",
    "epochs = 600\n",
    "batch_size = 32\n",
    "k_cov_den = 20\n",
    "k_crossLID = 15\n",
    "random_seed = 145\n",
    "n_generated_samples = 48*len(pd.unique(lbls))\n",
    "\n",
    "if GENERATE:\n",
    "    # Store results\n",
    "    gen_loss_all = []\n",
    "    crit_loss_all = []\n",
    "    saved_predictions = []\n",
    "    coverage = []\n",
    "    density = []\n",
    "    crossLID = []\n",
    "    corr1stcluster = []\n",
    "    \n",
    "    df = data_treated\n",
    "    pca = PCA(n_components=2, svd_solver='full', whiten=True)\n",
    "    pc_coords = pca.fit_transform(df)\n",
    "\n",
    "    generator_optimizer = tf.keras.optimizers.RMSprop(1e-4)\n",
    "    critic_optimizer = tf.keras.optimizers.RMSprop(1e-4)\n",
    "\n",
    "    generator = generator_model(data_treated.shape[1], data_treated.shape[1], 128, 2)\n",
    "    critic = critic_model(data_treated.shape[1], 512, 2)\n",
    "\n",
    "    training_montage(data_treated, lbls, real_samples_all, labels, epochs, generator, critic, generator_optimizer,\n",
    "                     critic_optimizer,\n",
    "                     input_realdata_dist, batch_size, grad_pen_weight=10, k_cov_den=k_cov_den, k_crossLID=k_crossLID,\n",
    "                     random_seed=random_seed, n_generated_samples=n_generated_samples)\n",
    "\n",
    "    results={'gen_loss': gen_loss_all, 'crit_loss': crit_loss_all, 'saved_pred': saved_predictions,\n",
    "             'coverage': coverage, 'density': density, 'crossLID': crossLID, 'corr1st_cluster': corr1stcluster}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed177715",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the models and results\n",
    "import pickle\n",
    "if GENERATE:\n",
    "    # Store data (serialize)\n",
    "    with open('gan_models/LGDint_results.pickle', 'wb') as handle:\n",
    "        pickle.dump(results, handle)\n",
    "\n",
    "    # Save the generator and critic models' weights.\n",
    "    generator.save_weights('gan_models/LGDint_gen')\n",
    "    critic.save_weights('gan_models/LGDint_crit')\n",
    "\n",
    "# Read back the models\n",
    "if not GENERATE:\n",
    "    # Setting the model up\n",
    "    generator_optimizer = tf.keras.optimizers.RMSprop(1e-4)\n",
    "    critic_optimizer = tf.keras.optimizers.RMSprop(1e-4)\n",
    "\n",
    "    generator = generator_model(data_treated.shape[1], data_treated.shape[1], 128, 2)\n",
    "    critic = critic_model(data_treated.shape[1], 512, 2)\n",
    "\n",
    "    # Load Previously saved models\n",
    "    generator.load_weights('./gan_models/LGDint_gen')\n",
    "    critic.load_weights('./gan_models/LGDint_crit')\n",
    "\n",
    "    with open('gan_models/LGDint_results.pickle', 'rb') as handle:\n",
    "        results = pickle.load(handle)\n",
    "        \n",
    "    gen_loss_all, crit_loss_all, saved_predictions = results['gen_loss'], results['crit_loss'], results['saved_pred']\n",
    "    coverage, density, crossLID = results['coverage'], results['density'], results['crossLID']\n",
    "    corr1stcluster = results['corr1st_cluster']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2017d17",
   "metadata": {},
   "source": [
    "#### Generate examples from our new code\n",
    "\n",
    "\n",
    "- Generate examples in bulk - predictions (GAN data)\n",
    "- Select only the 5 most correlated generated samples with each of the original samples - corr_preds (CorrGAN Data)\n",
    "\n",
    "#### CorrGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c13c102",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_examples_to_generate = 2048\n",
    "# Get distribution of intensity values of the dataset\n",
    "hist = np.histogram(real_samples_all.values.flatten(), bins=100)\n",
    "input_realdata_dist = stats.rv_histogram(hist)\n",
    "\n",
    "test_input = tf.constant(input_realdata_dist.rvs(size=data_treated.shape[1]*num_examples_to_generate), \n",
    "                         shape=[num_examples_to_generate, data_treated.shape[1]])\n",
    "test_labels = tf.constant([1]*(num_examples_to_generate//2) + [0]*(num_examples_to_generate//2), shape=[\n",
    "        num_examples_to_generate,1])\n",
    "\n",
    "predictions = generator([test_input, test_labels], training=False)\n",
    "predictions = pd.DataFrame(predictions.numpy(), columns=data_treated.columns) * 2*data_treated.values.std()\n",
    "predictions\n",
    "\n",
    "test_labels = tf.constant([1]*(num_examples_to_generate//2) + [0]*(num_examples_to_generate//2), shape=[\n",
    "        num_examples_to_generate,])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db4e13e",
   "metadata": {},
   "source": [
    "See correlation between samples and choose the 5 most correlated generated samples for each of the original samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13d18a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = real_samples_all\n",
    "# Calculate all correlations between all samples of real and artificial data and store them in a dataframe\n",
    "correlations = pd.DataFrame(index=predictions.index, columns=df.index).astype('float')\n",
    "\n",
    "for i in df.index:\n",
    "    for j in predictions.index:\n",
    "        correlations.loc[j,i] = stats.pearsonr(df.loc[i],\n",
    "                                               predictions.loc[j])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30478dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indices to keep in the correlated GAN data\n",
    "idx_to_keep = []\n",
    "for i in correlations:\n",
    "    idx_to_keep.extend(correlations[i].sort_values(ascending=False).index[:5])\n",
    "    \n",
    "print('Nº of total idx :', len(idx_to_keep))\n",
    "print('Nº of unique idx:', len(set(idx_to_keep)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614a08df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the correlation GAN dataframe and corresponding label targets\n",
    "corr_preds = predictions.loc[list(set(idx_to_keep))]\n",
    "corr_lbls  = list(np.array(test_labels)[list(set(idx_to_keep))])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79a92bd",
   "metadata": {},
   "source": [
    "#### GAN Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c05695e",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed=145\n",
    "ordered_labels = pd.get_dummies(\n",
    "            np.array(lbls)[np.random.RandomState(seed=random_seed).permutation(len(lbls))]).columns\n",
    "ordered_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f506ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_examples_to_generate = 1024\n",
    "# Get distribution of intensity values of the dataset\n",
    "hist = np.histogram(real_samples_all.values.flatten(), bins=100)\n",
    "input_realdata_dist = stats.rv_histogram(hist)\n",
    "\n",
    "test_input = tf.constant(input_realdata_dist.rvs(size=data_treated.shape[1]*num_examples_to_generate), \n",
    "                         shape=[num_examples_to_generate, data_treated.shape[1]])\n",
    "test_labels = tf.constant([1]*(num_examples_to_generate//2) + [0]*(num_examples_to_generate//2), shape=[\n",
    "        num_examples_to_generate,1])\n",
    "\n",
    "predictions = generator([test_input, test_labels], training=False)\n",
    "predictions = pd.DataFrame(predictions.numpy(), columns=data_treated.columns) \n",
    "predictions = predictions * 2*data_treated.values.std()\n",
    "\n",
    "test_labels = tf.constant([1]*(num_examples_to_generate//2) + [0]*(num_examples_to_generate//2), shape=[\n",
    "        num_examples_to_generate,])\n",
    "last_preds_labels = ['Cold-phase']*(len(predictions)//2) + ['Reperfusion']*(len(predictions)//2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6246d2",
   "metadata": {},
   "source": [
    "Transforming the 96 generated samples for 20 different time points during GAN training (stored in results) into DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0dbc5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_predictions = []\n",
    "# Transform predictions into Pandas DataFrames\n",
    "for i in range(len(results['saved_pred'])):\n",
    "    saved_predictions.append(pd.DataFrame(results['saved_pred'][i].numpy(), \n",
    "                                          columns=data_treated.columns)* 2*data_treated.values.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae20351",
   "metadata": {},
   "source": [
    "### Loss Plot and PCAs and tSNEs representation on the evolution of generated samples with epochs\n",
    "\n",
    "Measures of progression of the model in time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d4ddf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with sns.axes_style(\"whitegrid\"):\n",
    "    with sns.plotting_context(\"notebook\", font_scale=1.2):\n",
    "        steps_per_epoch = int(data_treated.shape[0] / batch_size)\n",
    "        f, ax = plt.subplots(1, 1, figsize=(4,4))\n",
    "        ax.plot(range(1,len(gen_loss_all)+1), gen_loss_all, label='Generator Loss')\n",
    "        ax.plot(range(1,len(crit_loss_all)+1), crit_loss_all, label='Critic Loss')\n",
    "        ax.set_xticks(range(0, (epochs+1)*steps_per_epoch, 200*steps_per_epoch))\n",
    "        ax.set_xticklabels(range(0, (epochs+1), 200))\n",
    "\n",
    "        ax.legend(fontsize=12)\n",
    "        ax.set_xlim([0*steps_per_epoch,(epochs)*steps_per_epoch])\n",
    "        ax.set_xlabel('Nº of Epochs', fontsize=15)\n",
    "        ax.set_ylabel('Loss', fontsize=15)\n",
    "        ax.set_title('Loss Plot', fontsize=15)\n",
    "        plt.tight_layout()\n",
    "        f.savefig('images/LGDint_LossPlot.png' , dpi=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b56558c",
   "metadata": {},
   "source": [
    "**PCA and tSNE of GAN generated data and the linearly generated 'test data'**\n",
    "\n",
    "Progression with number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3407632",
   "metadata": {},
   "outputs": [],
   "source": [
    "with sns.axes_style(\"whitegrid\"):\n",
    "    with sns.plotting_context(\"notebook\", font_scale=1):\n",
    "        f, axs = plt.subplots(2,5, figsize=(16,8), constrained_layout=True)\n",
    "        \n",
    "        for i, ax in zip(range(0, len(saved_predictions),2), axs.ravel()):\n",
    "            dfs_temp = pd.concat((data_treated, saved_predictions[i]))\n",
    "            temp_lbls = lbls.copy()\n",
    "            temp_lbls.extend(['Cold-phase - GAN']*(len(saved_predictions[-1])//2))\n",
    "            temp_lbls.extend(['Reperfusion - GAN']*(len(saved_predictions[-1])//2))\n",
    "            \n",
    "            principaldf = ma.compute_df_with_PCs(dfs_temp, n_components=2, whiten=True, labels=temp_lbls,\n",
    "                                                 return_var_ratios=False)\n",
    "\n",
    "            lcolors = label_colors_test\n",
    "            \n",
    "            gem.plot_PCA(principaldf, lcolors, components=(1,2), title='', ax=ax)\n",
    "            #gem.plot_ellipses_PCA(principaldf, lcolors, components=(1,2),ax=ax, q=0.95)\n",
    "            ax.legend(loc='upper right', ncol=1, framealpha=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da95c982",
   "metadata": {},
   "outputs": [],
   "source": [
    "with sns.axes_style(\"whitegrid\"):\n",
    "    with sns.plotting_context(\"notebook\", font_scale=1):\n",
    "        f, axs = plt.subplots(2,5, figsize=(16,8), constrained_layout=True)\n",
    "        \n",
    "        for i, ax in zip(range(0, len(saved_predictions), 2), axs.ravel()):\n",
    "            \n",
    "            dfs_temp = pd.concat((data_treated, saved_predictions[i]))\n",
    "            temp_lbls = lbls.copy()\n",
    "            temp_lbls.extend(['Cold-phase - GAN']*(len(saved_predictions[-1])//2))\n",
    "            temp_lbls.extend(['Reperfusion - GAN']*(len(saved_predictions[-1])//2))\n",
    "            \n",
    "            X = dfs_temp.copy()\n",
    "            X_embedded = TSNE(n_components=2, perplexity=30, learning_rate='auto',\n",
    "                              init='random', verbose=0).fit_transform(X)\n",
    "\n",
    "            df = X_embedded\n",
    "            labels = temp_lbls\n",
    "            lcolors = label_colors_test\n",
    "            \n",
    "            gem.plot_tSNE(df, labels, lcolors, components=(1,2), title='', ax=ax)\n",
    "            gem.plot_ellipses_tSNE(df, labels, lcolors, components=(1,2),ax=ax, q=0.95)\n",
    "            ax.legend(loc='upper right', ncol=1, framealpha=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3e1843",
   "metadata": {},
   "source": [
    "#### PCA of GAN generated data and experimental (real) data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b04ef3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with sns.axes_style(\"whitegrid\"):\n",
    "    with sns.plotting_context(\"notebook\", font_scale=1):\n",
    "        f, ax = plt.subplots(1,1, figsize=(4,4), constrained_layout=True)\n",
    "            \n",
    "        p = saved_predictions[-1].copy()\n",
    "        p.columns = real_samples_all.columns\n",
    "        dfs_temp = pd.concat((real_samples_all, p))\n",
    "        temp_lbls = target.copy()\n",
    "        temp_lbls.extend(['Cold-phase - GAN']*(len(saved_predictions[-1])//2))\n",
    "        temp_lbls.extend(['Reperfusion - GAN']*(len(saved_predictions[-1])//2))\n",
    "\n",
    "        principaldf = ma.compute_df_with_PCs(dfs_temp, n_components=2, whiten=True, labels=temp_lbls,\n",
    "                                                 return_var_ratios=False)\n",
    "\n",
    "        lcolors = label_colors_test\n",
    "\n",
    "        gem.plot_PCA(principaldf, lcolors, components=(1,2), title='', ax=ax)\n",
    "        gem.plot_ellipses_PCA(principaldf, lcolors, components=(1,2),ax=ax, q=0.95)\n",
    "        ax.set_ylabel('PC 2', fontsize=15)\n",
    "        ax.set_xlabel('PC 1', fontsize=15)\n",
    "        #ax.legend(loc='upper left', bbox_to_anchor=(1, 1), fontsize=14)\n",
    "        f.savefig('images/LGDint_PCAPlot.png' , dpi=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d712d78",
   "metadata": {},
   "source": [
    "### Comparing GAN Generated Data Characteristics with experimental data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b3a82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['Experimental data', 'GAN data', 'CorrGAN data']\n",
    "data_repo = [real_samples_all, predictions, corr_preds]\n",
    "tgs = [target, last_preds_labels, corr_lbls]\n",
    "data_characteristics = [gem.characterize_data(ds, name, tg) for ds,name,tg in zip(data_repo, names, tgs)]\n",
    "data_characteristics = pd.DataFrame(data_characteristics).set_index('Dataset')\n",
    "data_characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce854b9c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "f, (axl, axr) = plt.subplots(1,2, figsize=(8,4))\n",
    "\n",
    "names = ['Experimental', 'GAN', 'CorrGAN']\n",
    "axl.boxplot([ds.values.flatten() for ds in data_repo])\n",
    "axl.set_ylabel('Distribution of scaled intensities', fontsize=15)\n",
    "axl.set_xticklabels(names, fontsize=12)\n",
    "#axl.set_yticks([-2, 0, 2, 4])\n",
    "\n",
    "axr.boxplot([ds.sum(axis=1) for ds in data_repo])\n",
    "axr.set_ylabel('Distribution of feature sum of intensities', fontsize=12)\n",
    "axr.set_xticklabels(names, fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "f.savefig('images/LGDint_characteristics.png' , dpi=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa85d70",
   "metadata": {},
   "source": [
    "### Hierarchical Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb596301",
   "metadata": {},
   "source": [
    "Hierarchical clustering of 96 GAN samples and experimental data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddfaf699",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hierarchical clustering of 96 GAN samples and real data\n",
    "dt = real_samples_all\n",
    "dfs_temp = np.concatenate((dt, saved_predictions[-1].values))\n",
    "test_labels = target.copy()\n",
    "temp_lbls = list(test_labels)\n",
    "for i in ['Cold-phase','Reperfusion']:\n",
    "    temp_lbls.extend([i+' - GAN']*(n_generated_samples//2))\n",
    "\n",
    "hca_results = gem.perform_HCA(dfs_temp, temp_lbls, metric='euclidean', method='ward')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2055d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(3, 10))\n",
    "gem.plot_dendogram(hca_results['Z'], \n",
    "               temp_lbls, ax=ax,\n",
    "               label_colors=label_colors_test, title='',\n",
    "               color_threshold=0)\n",
    "ax.set_yticklabels([])\n",
    "ax.set_xticks([])\n",
    "plt.show()\n",
    "f.savefig('images/LGDint_HCAPlot.png' , dpi=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dfba05e",
   "metadata": {},
   "source": [
    "### Coverage and Density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20007239",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = data_treated\n",
    "\n",
    "density_list, coverage_list = gem.evaluation_coverage_density_all_k_at_once(training_set, \n",
    "                                                                            predictions, \n",
    "                                                                            metric='Euclidean')\n",
    "\n",
    "density_list_test, coverage_list_test = gem.evaluation_coverage_density_all_k_at_once(training_set,\n",
    "                                                                                      real_samples_all, \n",
    "                                                                                      metric='Euclidean')\n",
    "\n",
    "density_list_real, coverage_list_real = gem.evaluation_coverage_density_all_k_at_once(real_samples_all, \n",
    "                                                                                      predictions,\n",
    "                                                                                      metric='Euclidean')\n",
    "\n",
    "density_list_corr, coverage_list_corr = gem.evaluation_coverage_density_all_k_at_once(training_set, \n",
    "                                                                                      corr_preds,\n",
    "                                                                                      metric='Euclidean')\n",
    "\n",
    "density_list_linaug_real, coverage_list_linaug_real = gem.evaluation_coverage_density_all_k_at_once(\n",
    "                                                                                      real_samples_all, \n",
    "                                                                                      training_set,\n",
    "                                                                                      metric='Euclidean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7155737",
   "metadata": {},
   "outputs": [],
   "source": [
    "with sns.axes_style(\"whitegrid\"):\n",
    "    with sns.plotting_context(\"notebook\", font_scale=1.2):\n",
    "        f, (axl, axr) = plt.subplots(1, 2, figsize = (8,4))#, sharey='row')#, sharex='col')\n",
    "        \n",
    "        axl.plot(range(1,len(training_set)), density_list, label='Density', color='blue')\n",
    "        axl.plot(range(1,len(training_set)), coverage_list, label='Coverage', color='red')\n",
    "        axl.set_title('GAN - Interpolated Data', fontsize=15)\n",
    "        axl.set_ylabel('Density / Coverage', fontsize=15)\n",
    "        axl.set_ylim([0,5])\n",
    "        axl.set_xlabel('Nº of Nearest Neighbors', fontsize=15)\n",
    "\n",
    "        axr.plot(range(1,len(real_samples_all)), density_list_real, label='Density', color='blue')\n",
    "        axr.plot(range(1,len(real_samples_all)), coverage_list_real, label='Coverage', color='red')\n",
    "        axr.set_title('GAN - Experimental Data', fontsize=15)\n",
    "        axr.set_ylim([0,5])\n",
    "        \n",
    "        axr.legend()\n",
    "        axl.legend()\n",
    "        axr.set_xlabel('Nº of Nearest Neighbors', fontsize=15)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    f.savefig('images/LGDint_DenCovPlot.png' , dpi=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c83753f",
   "metadata": {},
   "source": [
    "### Histograms\n",
    " \n",
    "Histograms of Values of normal Experimental (Real), Generated and GAN Generated Data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714d1d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions\n",
    "last_preds = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fa0f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "with sns.axes_style(\"whitegrid\"):\n",
    "    with sns.plotting_context(\"notebook\", font_scale=1.2):\n",
    "        f, ax = plt.subplots(1, 1, figsize = (6,4))#, sharey='row')#, sharex='col')\n",
    "        X = np.arange(-8, 8.01, 0.05)\n",
    "        hist = np.histogram(real_samples_all.values.flatten(), bins=50)\n",
    "        hist_dist = stats.rv_histogram(hist)\n",
    "        ax.plot(X, hist_dist.pdf(X), color='blue', label='Experimental')\n",
    "        \n",
    "        hist = np.histogram(data_treated.values.flatten(), bins=50)\n",
    "        hist_dist = stats.rv_histogram(hist)\n",
    "        ax.plot(X, hist_dist.pdf(X), color='orange', label='Interpolated')\n",
    "        ax.set_ylabel('PDF', fontsize=15)\n",
    "        \n",
    "        hist = np.histogram(last_preds.values.flatten(), bins=50)\n",
    "        hist_dist = stats.rv_histogram(hist)\n",
    "        ax.plot(X, hist_dist.pdf(X), color='red', label='GAN')\n",
    "        \n",
    "        hist = np.histogram(corr_preds.values.flatten(), bins=50)\n",
    "        hist_dist = stats.rv_histogram(hist)\n",
    "        ax.plot(X, hist_dist.pdf(X), color='green', label='CorrGAN')\n",
    "        \n",
    "        ax.set_ylabel('PDF', fontsize=15)\n",
    "        ax.set_xlabel('Scaled Intensities', fontsize=15)\n",
    "        #ax.set_xticks([-8, -4, 0, 4, 8])\n",
    "        ax.set_ylim([0,0.6])\n",
    "\n",
    "        ax.legend(loc='upper left', bbox_to_anchor=(1, 1), fontsize=12, handlelength=1)\n",
    "        ax.set_title('Intensities Distribution', fontsize=18)\n",
    "        plt.tight_layout()\n",
    "        f.savefig('images/LGDint_IntPlot.png', dpi=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a43a03f",
   "metadata": {},
   "source": [
    "### Correlations between samples of Experimental (Real) Data, Linearly Interpolated and GAN Generated Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644aaa14",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_real_real = np.corrcoef(real_samples_all)\n",
    "print('Correlation Experimental-Experimental calculation ended.')\n",
    "\n",
    "correlation_lin_lin = np.corrcoef(data_treated)\n",
    "print('Correlation Linear-Linear calculation ended.')\n",
    "\n",
    "correlation_gan_gan = np.corrcoef(last_preds)\n",
    "print('Correlation GAN-GAN calculation ended.')\n",
    "\n",
    "correlation_corr_corr = np.corrcoef(corr_preds)\n",
    "print('Correlation CorrGAN-CorrGAN calculation ended.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748870be",
   "metadata": {},
   "outputs": [],
   "source": [
    "with sns.axes_style(\"whitegrid\"):\n",
    "    with sns.plotting_context(\"notebook\", font_scale=1.2):\n",
    "        f, ax = plt.subplots(1, 1, figsize = (5,4))#, sharey='row')#, sharex='col')\n",
    "        X = np.arange(-1, 1.01, 0.05)\n",
    "        hist = np.histogram(correlation_real_real.flatten(), bins=50)\n",
    "        hist_dist = stats.rv_histogram(hist)\n",
    "        ax.plot(X, hist_dist.pdf(X), color='blue', label='Experimental')\n",
    "        ax.set_ylabel('PDF', fontsize=15)\n",
    "\n",
    "        hist = np.histogram(correlation_lin_lin.flatten(), bins=50)\n",
    "        hist_dist = stats.rv_histogram(hist)\n",
    "        ax.plot(X, hist_dist.pdf(X), color='orange', label='Interpolated')\n",
    "\n",
    "        hist = np.histogram(correlation_gan_gan.flatten(), bins=50)\n",
    "        hist_dist = stats.rv_histogram(hist)\n",
    "        ax.plot(X, hist_dist.pdf(X), color='red', label='GAN')\n",
    "\n",
    "        hist = np.histogram(correlation_corr_corr.flatten(), bins=50)\n",
    "        hist_dist = stats.rv_histogram(hist)\n",
    "        ax.plot(X, hist_dist.pdf(X), color='green', label='CorrGAN')\n",
    "\n",
    "        ax.legend(loc='upper left', bbox_to_anchor=(1, 1), fontsize=12, handlelength=0.9, handletextpad=0.5)\n",
    "        ax.set_title('Sample Correlations', fontsize=18)\n",
    "        ax.set_xlabel('Correlation', fontsize=15)\n",
    "        #ax.set_ylim([0,3.6])\n",
    "    #f.text(0.5, 0.05, 'Correlation', ha='center', va='top', fontsize=15)\n",
    "    plt.tight_layout()\n",
    "    f.savefig('images/LGDint_SampCorrPlot.png', dpi=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7873a4d5",
   "metadata": {},
   "source": [
    "### Correlations between features of Experimental (Real), Linearly Interpolated and GAN Generated Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000682b0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "correlation_real_real = np.corrcoef(real_samples_all.T)\n",
    "print('Correlation Experimental-Experimental calculation ended.')\n",
    "\n",
    "correlation_gen_gen = np.corrcoef(data_treated.T)\n",
    "print('Correlation Generated-Generated calculation ended.')\n",
    "\n",
    "correlation_gan_gan = np.corrcoef(last_preds.T)\n",
    "print('Correlation GAN-GAN calculation ended.')\n",
    "\n",
    "correlation_corr_corr = np.corrcoef(corr_preds.T)\n",
    "print('Correlation CorrGAN-CorrGAN calculation ended.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e2c53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with sns.axes_style(\"whitegrid\"):\n",
    "    with sns.plotting_context(\"notebook\", font_scale=1.2):\n",
    "        f, ax = plt.subplots(1, 1, figsize = (5,4))#, sharey='row')#, sharex='col')\n",
    "        X = np.arange(-1, 1.01, 0.05)\n",
    "        hist = np.histogram(correlation_real_real.flatten()[~np.isnan(correlation_real_real.flatten())],\n",
    "                            bins=50)\n",
    "        hist_dist = stats.rv_histogram(hist)\n",
    "        ax.plot(X, hist_dist.pdf(X), color='blue', label='Experimental')\n",
    "        ax.set_ylabel('PDF', fontsize=15)\n",
    "        \n",
    "        hist = np.histogram(correlation_gen_gen.flatten()[~np.isnan(correlation_gen_gen.flatten())], bins=50)\n",
    "        hist_dist = stats.rv_histogram(hist)\n",
    "        ax.plot(X, hist_dist.pdf(X), color='orange', label='Interpolated')\n",
    "        \n",
    "        hist = np.histogram(correlation_gan_gan.flatten()[~np.isnan(correlation_gan_gan.flatten())], bins=50)\n",
    "        hist_dist = stats.rv_histogram(hist)\n",
    "        ax.plot(X, hist_dist.pdf(X), color='red', label='GAN')\n",
    "        \n",
    "        hist = np.histogram(correlation_corr_corr.flatten()[~np.isnan(correlation_corr_corr.flatten())],\n",
    "                            bins=50)\n",
    "        hist_dist = stats.rv_histogram(hist)\n",
    "        ax.plot(X, hist_dist.pdf(X), color='green', label='CorrGAN')\n",
    "        \n",
    "        \n",
    "        ax.legend(loc='upper left', bbox_to_anchor=(1, 1), fontsize=12, handlelength=0.8, handletextpad=0.4)\n",
    "        ax.set_title('Feature Correlations', fontsize=18)\n",
    "        ax.set_xlabel('Correlation', fontsize=15)\n",
    "        ax.set_ylim([0,2])\n",
    "\n",
    "    #f.text(0.5, 0.05, 'Correlation', ha='center', va='top', fontsize=15)\n",
    "    plt.tight_layout()\n",
    "    f.savefig('images/LGDint_FeatCorrPlot.png', dpi=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd94e09",
   "metadata": {},
   "source": [
    "### Sample Correlation Matrix\n",
    "\n",
    "Between samples of the experimental (real) data and a set of generated GAN samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2b141b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experimental (Real) Data, organize it to have first all samples of a class, then all samples of the other class\n",
    "df = real_samples_all.copy()\n",
    "\n",
    "samp = df.index\n",
    "tg = target.copy()\n",
    "new_order = [x for _, x in sorted(zip(tg, samp))]\n",
    "new_tg = [x for x, _ in sorted(zip(tg, samp))]\n",
    "\n",
    "df = df.loc[new_order]\n",
    "#df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f02a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate all correlations between all samples of experimental (real) and generated data and store them in a dataframe\n",
    "correlations = pd.DataFrame(index=last_preds.index, columns=df.index).astype('float')\n",
    "\n",
    "for i in df.index:\n",
    "    for j in last_preds.index:\n",
    "        correlations.loc[j,i] = stats.spearmanr(df.loc[i],\n",
    "                                               last_preds.loc[j])[0]\n",
    "\n",
    "correlations.columns = new_tg\n",
    "correlations.index = ['Cold-phase - GAN']*(len(last_preds)//2) + ['Reperfusion - GAN']*(len(last_preds)//2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5467554b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw the clustermap\n",
    "\n",
    "row_cols = [label_colors_test[lbl] for lbl in new_tg]\n",
    "row_cols2 = [label_colors_test[lbl] for lbl in correlations.index]\n",
    "co = sns.diverging_palette(160, 310, s=90, as_cmap=True, l=40)\n",
    "g = sns.clustermap(correlations, col_colors=row_cols, cmap=co, row_colors= row_cols2, vmin=-1, vmax=1, method='ward',\n",
    "                  cbar_pos = (-0.15, 0.1, 0.05, 0.5))\n",
    "g.fig.set_size_inches((6,9))\n",
    "# some tweaks\n",
    "patches = []\n",
    "for lbl in ordered_labels_test:\n",
    "    patches.append(mpatches.Patch(color=label_colors_test[lbl], label=lbl))\n",
    "leg = plt.legend(handles=patches, loc=3, bbox_to_anchor=(-1.35, 1.35, 0.5, 1),\n",
    "                     frameon=False, fontsize=14) \n",
    "g.ax_heatmap.set_ylabel('GAN Data', fontsize=20)\n",
    "g.ax_heatmap.set_xlabel('Experimental Data', fontsize=20)\n",
    "g.ax_heatmap.tick_params(axis='both', which='both', bottom=False, top=False, labelbottom=False, right=False,\n",
    "                         labelright=False)\n",
    "\n",
    "# Manually specify colorbar labelling after it's been generated\n",
    "colorbar = g.ax_heatmap.collections[0].colorbar\n",
    "colorbar.ax.tick_params(labelsize=13) \n",
    "plt.text(1, 1.10, 'Correlations', fontsize=14, horizontalalignment='center')\n",
    "plt.show()\n",
    "g.savefig('images/LGDint_Clustermap.png', dpi=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5f3916",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = scipy.spatial.distance_matrix(last_preds, df)\n",
    "sample_distances = pd.DataFrame(a, index=last_preds.index, columns=df.index).astype('float')\n",
    "sample_distances.index = ['Cold-phase - GAN']*(len(last_preds)//2) + ['Reperfusion - GAN']*(len(last_preds)//2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ff6d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw the clustermap\n",
    "row_cols = [label_colors_test[lbl] for lbl in new_tg]\n",
    "row_cols2 = [label_colors_test[lbl] for lbl in sample_distances.index]\n",
    "g = sns.clustermap(sample_distances, col_colors=row_cols, cmap='bwr', row_colors= row_cols2,\n",
    "                   #norm=matplotlib.colors.LogNorm(vmin=25, vmax=175),\n",
    "                   vmin=20, vmax=100,\n",
    "                   method='ward',\n",
    "                  cbar_pos = (-0.13, 0.1, 0.05, 0.5))\n",
    "g.fig.set_size_inches((6,9))\n",
    "# some tweaks\n",
    "patches = []\n",
    "for lbl in ordered_labels_test:\n",
    "    patches.append(mpatches.Patch(color=label_colors_test[lbl], label=lbl))\n",
    "leg = plt.legend(handles=patches, loc=3, bbox_to_anchor=(-2.2, 1.45, 0.5, 1),\n",
    "                     frameon=False, fontsize=14) \n",
    "g.ax_heatmap.set_ylabel('GAN Data', fontsize=20)\n",
    "g.ax_heatmap.set_xlabel('Experimental Data', fontsize=20)\n",
    "g.ax_heatmap.tick_params(axis='both', which='both', bottom=False, top=False, labelbottom=False, right=False,\n",
    "                         labelright=False)\n",
    "\n",
    "# Manually specify colorbar labelling after it's been generated\n",
    "colorbar = g.ax_heatmap.collections[0].colorbar\n",
    "colorbar.ax.tick_params(labelsize=13) \n",
    "plt.text(1, 105.10, 'Distances', fontsize=14, horizontalalignment='center')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56589017",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_distances.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa74e7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_distances.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff3c738",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experimental (Real) Data, organize it to have first all samples of a class, then all samples of the other class\n",
    "df = real_samples_all.copy()\n",
    "\n",
    "samp = df.index\n",
    "tg = target.copy()\n",
    "new_order = [x for _, x in sorted(zip(tg, samp))]\n",
    "new_tg = [x for x, _ in sorted(zip(tg, samp))]\n",
    "\n",
    "df = df.loc[new_order]\n",
    "#df\n",
    "a = scipy.spatial.distance_matrix(df, df)\n",
    "sample_distances = pd.DataFrame(a, index=df.index, columns=df.index).astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18879706",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_distances.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115a4555",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Draw the clustermap\n",
    "\n",
    "row_cols = [label_colors_test[lbl] for lbl in new_tg]\n",
    "row_cols2 = [label_colors_test[lbl] for lbl in new_tg]\n",
    "g = sns.clustermap(sample_distances, col_colors=row_cols, cmap='bwr', row_colors= row_cols2,\n",
    "                   #norm=matplotlib.colors.LogNorm(vmin=30, vmax=250),\n",
    "                   vmin=20, vmax=100,\n",
    "                   method='ward',\n",
    "                  cbar_pos = (-0.13, 0.1, 0.05, 0.5))\n",
    "g.fig.set_size_inches((6,9))\n",
    "# some tweaks\n",
    "patches = []\n",
    "for lbl in ordered_labels_test:\n",
    "    patches.append(mpatches.Patch(color=label_colors_test[lbl], label=lbl))\n",
    "leg = plt.legend(handles=patches, loc=3, bbox_to_anchor=(-1.2, 1.35, 0.5, 1),\n",
    "                     frameon=False, fontsize=14) \n",
    "g.ax_heatmap.set_ylabel('Experimental Data', fontsize=20)\n",
    "g.ax_heatmap.set_xlabel('Experimental Data', fontsize=20)\n",
    "g.ax_heatmap.tick_params(axis='both', which='both', bottom=False, top=False, labelbottom=False, right=False, \n",
    "                         labelright=False)\n",
    "\n",
    "# Manually specify colorbar labelling after it's been generated\n",
    "colorbar = g.ax_heatmap.collections[0].colorbar\n",
    "colorbar.ax.tick_params(labelsize=13) \n",
    "plt.text(1, 110.10, 'Distances', fontsize=14, horizontalalignment='center')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3f0a3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "e03b612d84ba21ce95ed447e81b3062e1eb99b56c6d885cdab4aaa12f1b8e240"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
