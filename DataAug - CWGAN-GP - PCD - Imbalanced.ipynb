{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f24a9a01",
   "metadata": {},
   "source": [
    "# Data Augmentation - Conditional Wasserstein GANs - GP\n",
    "\n",
    "### Dataset: Prostate Cancer Dataset\n",
    "\n",
    "This notebook presents the CWGAN-GP model to generate treated Intensity Data and test GAN data augmentation efficacy in mitigating class imbalanced dataset issues (in supervised analysis) by supplementing the minority class with GAN samples.\n",
    "\n",
    "Notebook Organization:\n",
    "- Read the dataset\n",
    "- Treatment and Univariate Analysis of the dataset\n",
    "- Split of the Dataset into 10 folds (5 for each minority class) training and test sets (details on that section).\n",
    "- Setup the CWGAN-GP model and train 10 models, once for each fold, with the corresponding training data.\n",
    "- Generate GAN samples and add them to the corresponding imbalanced training sets in small increments.\n",
    "- Build and evaluate performance of RF and PLS-DA models from the imbalanced dataset, the imbalanced dataset supplemented with minority class samples and purely GAN samples dataset.\n",
    "- Compare important features in the RF and PLS-DA models against important features of models built with the complete dataset.\n",
    "\n",
    "#### Due to stochasticity, re-running the notebook will get slightly different results. Thus, figures in the paper can be slightly different."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db562b1",
   "metadata": {},
   "source": [
    "#### Needed Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba1d3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from time import perf_counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib import ticker\n",
    "\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from IPython import display as ipythondisplay\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support, f1_score\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras import backend\n",
    "\n",
    "import pickle\n",
    "\n",
    "# Metabolinks package\n",
    "import metabolinks as mtl\n",
    "import metabolinks.transformations as transf\n",
    "\n",
    "# Python files in the repository\n",
    "import multianalysis as ma\n",
    "import gan_evaluation_metrics as gem\n",
    "import linear_augmentation_functions as laf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5e2e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import needed functions from GAN_functions\n",
    "from GAN_functions import gradient_penalty_cwgan\n",
    "from GAN_functions import critic_loss_wgan\n",
    "from GAN_functions import generator_loss_wgan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ef8607",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70ad6b0",
   "metadata": {},
   "source": [
    "### Description of the Prostate Cancer dataset\n",
    "\n",
    "249 samples obtained from 80 subjects belonging to 2 different classes of HILIC-MS data obtained in positive ionization mode. The Chromatography system was a Thermo Dionex Ultimate 3000 with the column being a Waters Xbridge BEH HILIC (75 x 2.1mm, 2.5um). Mass Spectrometry was performed in a Thermo Q Exactive HF hybrid Orbitrap operating in positive electrospray ionization mode. Further information in the data deposition site mentioned below.\n",
    "\n",
    "The 2 classes are:\n",
    "\n",
    "- 135 pre-operative blood (serum) samples from patients with **'No Recurrence'** of Prostate Cancer after Radical Prostatectomy\n",
    "- 114 pre-operative blood (serum) samples from patients with **'Recurrence'** of Prostate Cancer after Radical Prostatectomy\n",
    "\n",
    "Data acquired by:\n",
    "\n",
    "- Clendinen, C.S.; Gaul, D.A.; Monge, M.E.; Arnold, R.S.; Edison, A.S.; Petros, J.A.; Fernández, F.M. Preoperative Metabolic Signatures of Prostate Cancer Recurrence Following Radical  Prostatectomy. J. Proteome Res. 2019, 18, 1316–1327, doi:10.1021/acs.jproteome.8b00926.\n",
    "\n",
    "Data obtained from Metabolomics Workbench project ID PR000724, study ID: ST001082, Analysis ID: AN001766 (https://www.metabolomicsworkbench.org/data/DRCCMetadata.php?Mode=Study&DataMode=AllData&StudyID=ST001082&StudyType=MS&ResultType=5#DataTabs)\n",
    "\n",
    "Data was taken already aligned in a 2D numerical data matrix. It had 2 copies of each sample, where the 2nd copies were equal to the first ones multiplied by a constant. This constant was different for each sample. We removed the presence of the 2nd copies.\n",
    "\n",
    "Data matrices retained only features that occur (globally) at least twice in all samples of the dataset (filtering/alignment)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc86f4c8",
   "metadata": {},
   "source": [
    "### Reading File and Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aab89a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the dataset, this dataset has two copies of the samples.The 2nd copy is equal to the 1st multiplied by a constant\n",
    "# unique to each sample.\n",
    "human_datamatrix_base = pd.read_excel('ST001082_AN001766_HD.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b73f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "human_datamatrix = human_datamatrix_base.iloc[:-1, :-4] # Just select the rows corresponding to the dataset\n",
    "mz_list = human_datamatrix_base.iloc[:-1, -4] # Select column with list of m/z values\n",
    "\n",
    "human_datamatrix = human_datamatrix.set_index(human_datamatrix.columns[0]) # Set index as the metabolites name\n",
    "human_datamatrix = human_datamatrix.replace({0:np.nan}) # Replacing 0 values as missing values\n",
    "\n",
    "human_datamatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a3e520",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select one of the two copies of samples in the dataset by removing the samples ending with '.1'\n",
    "human_datamatrix = human_datamatrix[[i for i in human_datamatrix.columns if not i.endswith('.1')]]\n",
    "\n",
    "human_datamatrix = human_datamatrix.T # Transpose the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a571e010",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Current sample labels. Sample will consist of 'Sample Type:No Recurrence' and 'Sample Type:Recurrence'\n",
    "hd_labels = list(human_datamatrix['Factors'])\n",
    "set(hd_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6bda40",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# How many 'No Recurrence' samples in the dataset\n",
    "hd_labels.count('Sample Type:No Recurrence')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1babbb25",
   "metadata": {},
   "source": [
    "### Blank Treatment\n",
    "\n",
    "- If blank_treatment = True:\n",
    "\n",
    "An average of the blanks will be subtracted to the remaining dataset. Missing values in the blanks were replaced by 0 to calculate the average of the blanks. Negative values that arise in the dataset from subtracting the blanks will be coded as 0/missing values in the dataset.\n",
    "\n",
    "- If blank_treatment = False:\n",
    "\n",
    "Blank samples are removed and not accounted for. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293f2252",
   "metadata": {},
   "outputs": [],
   "source": [
    "blank_treatment = True\n",
    "#blank_treatment = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2946fb88",
   "metadata": {},
   "outputs": [],
   "source": [
    "if blank_treatment:\n",
    "    blanks = human_datamatrix[human_datamatrix['Factors'] == 'Sample Type:Blank'].iloc[:,1:] # Select blank samples\n",
    "    blanks = blanks.replace({np.nan:0}) \n",
    "    blanks = blanks.astype(float) # Get blank samples with floats (needed since datamatrix has strings)\n",
    "    blanks_average = blanks.mean() # Average of the blanks\n",
    "    blanks_average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58569e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting the samples belonging to either the 'No Recurrence' or 'Recurrence' \n",
    "selection = []\n",
    "for i in human_datamatrix.loc[:, 'Factors']:\n",
    "    if i in ['Sample Type:No Recurrence', 'Sample Type:Recurrence']:\n",
    "        selection.append(True)\n",
    "    else:\n",
    "        selection.append(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef0ae60",
   "metadata": {},
   "outputs": [],
   "source": [
    "human_datamatrix = human_datamatrix[selection]\n",
    "# Creating the list of 'targets' (labels of samples) of the dataset with the 'No Recurrence' and 'Recurrence' classes \n",
    "hd_labels = []\n",
    "for i in list(human_datamatrix.iloc[:,0]):\n",
    "    if i == 'Sample Type:No Recurrence':\n",
    "        hd_labels.append('No Recurrence')\n",
    "    else:\n",
    "        hd_labels.append('Recurrence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4d2c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "human_datamatrix = human_datamatrix.iloc[:,1:]\n",
    "human_datamatrix = human_datamatrix.astype(float) # Passing the values from strings to floats.\n",
    "human_datamatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a65206",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subtract the average of the 5 blanks from the samples in the dataset\n",
    "if blank_treatment:\n",
    "    human_datamatrix = human_datamatrix.replace({np.nan:0}) - blanks_average\n",
    "    human_datamatrix[human_datamatrix<0] = 0\n",
    "    human_datamatrix = human_datamatrix.replace({0:np.nan})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61528e66",
   "metadata": {},
   "source": [
    "'Neutralization' of the _m/z_ values by subtracting the mass of a proton to the _m/z_ peaks.\n",
    "\n",
    "4 peaks (2 pairs) with different retention time have the exact same _m/z_ values. These 2 pairs of peaks will be treated as only 2 different peaks (instead of four)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49803a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Atomic masses - https://ciaaw.org/atomic-masses.htm\n",
    "#Isotopic abundances-https://ciaaw.org/isotopic-abundances.htm/https://www.degruyter.com/view/journals/pac/88/3/article-p293.xml\n",
    "# Isotopic abundances from Pure Appl. Chem. 2016; 88(3): 293–306,\n",
    "# Isotopic compositions of the elements 2013 (IUPAC Technical Report), doi: 10.1515/pac-2015-0503\n",
    "\n",
    "chemdict = {'H':(1.0078250322, 0.999844),\n",
    "            'C':(12.000000000, 0.988922),\n",
    "            'N':(14.003074004, 0.996337),\n",
    "            'O':(15.994914619, 0.9976206),\n",
    "            'Na':(22.98976928, 1.0),\n",
    "            'P':(30.973761998, 1.0),\n",
    "            'S':(31.972071174, 0.9504074),\n",
    "            'Cl':(34.9688527, 0.757647),\n",
    "            'F':(18.998403163, 1.0),\n",
    "            'C13':(13.003354835, 0.011078) # Carbon 13 isotope\n",
    "           } \n",
    "\n",
    "# electron mass from NIST http://physics.nist.gov/cgi-bin/cuu/Value?meu|search_for=electron+mass\n",
    "electron_mass = 0.000548579909065"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be837647",
   "metadata": {},
   "outputs": [],
   "source": [
    "mz_list = mz_list[1:] - chemdict['H'][0] + electron_mass\n",
    "counts = human_datamatrix.count(axis=0)\n",
    "final_mz_list = list(mz_list[list(counts >= 2)])\n",
    "final_mz_list = set(final_mz_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de2efdf",
   "metadata": {},
   "source": [
    "### Data Pre-Treatments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6c4d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "human_datamatrix = transf.keep_atleast(human_datamatrix, minimum=2) # Keep features that appear in at least two samples\n",
    "human_datamatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0dd09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For missing value imputation based on constants relative to the minimum of each feature instead of the full dataset\n",
    "def fillna_frac_feat_min(df, fraction=0.2):\n",
    "    \"\"\"Set NaN to a fraction of the minimum value in each column of the DataFrame.\"\"\"\n",
    "\n",
    "    minimum = df.min(axis=0) * fraction\n",
    "    return df.fillna(minimum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2e1098",
   "metadata": {},
   "outputs": [],
   "source": [
    "human_datamatrix_I = fillna_frac_feat_min(human_datamatrix, fraction=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd0ad29",
   "metadata": {},
   "outputs": [],
   "source": [
    "hd_matrix = human_datamatrix_I.copy()\n",
    "hd_datamatrix_N = transf.normalize_PQN(hd_matrix, ref_sample='mean')\n",
    "hd_datamatrix_treated = transf.pareto_scale(transf.glog(hd_datamatrix_N, lamb=None))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4888d02",
   "metadata": {},
   "source": [
    "# Univariate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db16fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_results = ma.compute_FC_pvalues_2groups(hd_datamatrix_N,\n",
    "                                  hd_datamatrix_treated,\n",
    "                               labels=hd_labels,\n",
    "                               equal_var=True,\n",
    "                               alpha=0.05, useMW=False)\n",
    "uni_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8273a235",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = uni_results[uni_results['FDR adjusted p-value'] < 0.01]\n",
    "uni_results_filt = b[abs(b['log2FC']) > 1]\n",
    "uni_results_filt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "428ec98c",
   "metadata": {},
   "source": [
    "### Fitting RF and PLS-DA models to the complete dataset\n",
    "\n",
    "Results and Feature Importance are estimated by stratified n_fold-cross validation averaged across 20 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f9ba90",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(485)\n",
    "n_fold = 5\n",
    "\n",
    "RF_model_real = ma.RF_model_CV(hd_datamatrix_treated, hd_labels,\n",
    "                               iter_num=20, n_fold=n_fold, n_trees=200) \n",
    "RF_feats_real = pd.DataFrame(RF_model_real['important_features']).set_index(0).sort_values(by=1, ascending=False)\n",
    "RF_feats_real.index = [hd_datamatrix_treated.columns[i] for i in RF_feats_real.index]\n",
    "#RF_feats_real "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a546717d",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(485)\n",
    "n_fold = 5\n",
    "n_comp = 9 # Nº of Components\n",
    "\n",
    "PLSDA_model_real = ma.PLSDA_model_CV(hd_datamatrix_treated, hd_labels,\n",
    "                                     n_comp=n_comp, iter_num=20, n_fold=n_fold, feat_type='VIP') \n",
    "\n",
    "PLSDA_feats_real = pd.DataFrame(PLSDA_model_real['important_features']).set_index(0).sort_values(by=1, ascending=False)\n",
    "PLSDA_feats_real.index = [hd_datamatrix_treated.columns[i] for i in PLSDA_feats_real.index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8c7da8",
   "metadata": {},
   "source": [
    "## Creating Imbalanced Datasets\n",
    "\n",
    "This dataset has two approximately balanced classes. Thus we considered cases where both were once the minority class.\n",
    "\n",
    "For one class chosen as the minority class:\n",
    "\n",
    "First we randomly select 100 samples of each of the classes: Recurrence (out of 114) and No Recurrence (out of 135).\n",
    "\n",
    "Then, considering one of them as the minority class, we split the dataset in 5 different ways where each had 80 samples of the majority class and 20 samples of the minority class in the training set. Thus, this left 20 samples of the majority and 80 of the minority class to be the test sets plus the 14 Recurrence samples and 35 No Recurrence samples originally left out that were always added to the test set.\n",
    "\n",
    "This was made by putting the set of 100 samples of a class into 5 folds of 20, combining 4 for the majority class for the training set. This split happenned before data pre-treatment. Then both sets were independently trained with the same pre-treatment pipeline but on the test set we performed a 'faux' Pareto scaling using the features standard deviation and mean of the training set. The training and test sets have (in most cases) a vastly different balance of class samples. Thus, feature averages and standard deviations can be quite different between them, especially in key features for discrimination. To compensate for this, the ‘faux’ Pareto scaling was applied. The untreated training sets were linearly augmented, which was then treated (using a normal Pareto scaling, in this case), to generate samples to train the CWGAN-GP models.\n",
    "\n",
    "This process was then repeated considering the other class as the minority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feec9b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_storage_train = {}\n",
    "df_storage_test = {}\n",
    "lbl_storage_train = {}\n",
    "lbl_storage_test = {}\n",
    "real_samples = {}\n",
    "\n",
    "rng = np.random.default_rng(7519)\n",
    "\n",
    "# Randomly order the samples of each class to separate them into the folds after\n",
    "permutations = {}\n",
    "permutations['Recurrence'] = list(rng.permutation(114))\n",
    "permutations['No Recurrence'] = list(rng.permutation(135) + 114)\n",
    "\n",
    "for lbl in ['Recurrence', 'No Recurrence']:\n",
    "    df_storage_train[lbl] = {}\n",
    "    df_storage_test[lbl] = {}\n",
    "    lbl_storage_train[lbl] = {}\n",
    "    lbl_storage_test[lbl] = {}\n",
    "    real_samples[lbl] = {}\n",
    "    for i in range(5):\n",
    "        train_idxs = {}\n",
    "        test_idxs = {}\n",
    "        for cl in permutations.keys():\n",
    "            if cl == lbl:\n",
    "                train_idxs[cl] = list(np.array(permutations[cl])[i*20: (i+1)*20])\n",
    "                test_idxs[cl] = list(np.array(permutations[cl])[: i*20]) + list(\n",
    "                    np.array(permutations[cl])[(i+1)*20:])\n",
    "            else:\n",
    "                train_idxs[cl] = list(np.array(permutations[cl])[: i*20]) + list(\n",
    "                    np.array(permutations[cl])[(i+1)*20:100])\n",
    "                test_idxs[cl] = list(np.array(permutations[cl])[i*20: (i+1)*20]) + list(\n",
    "                    np.array(permutations[cl])[100:])\n",
    "        \n",
    "        print('Minority Class:', lbl, 'Fold nº:', i+1)\n",
    "        print('Training Set, Recurrence:   ', len(train_idxs['Recurrence']))#, train_idxs['Recurrence'])\n",
    "        print('Training Set, No Recurrence:', len(train_idxs['No Recurrence']))#, train_idxs['No Recurrence'])\n",
    "        \n",
    "        print('Test Set,     Recurrence:   ', len(test_idxs['Recurrence']))\n",
    "        print('Test Set,     No Recurrence:', len(test_idxs['No Recurrence']))#, test_idxs['No Recurrence'])\n",
    "        print('----------------------')\n",
    "        \n",
    "        train_idxs = train_idxs['Recurrence'] + train_idxs['No Recurrence']\n",
    "        test_idxs = test_idxs['Recurrence'] + test_idxs['No Recurrence']\n",
    "\n",
    "        # Create the imbalanced and test set\n",
    "        df_storage_train[lbl][i+1] = human_datamatrix_I.iloc[train_idxs]\n",
    "        lbl_storage_train[lbl][i+1] = list(np.array(hd_labels)[train_idxs])\n",
    "        \n",
    "        human_datamatrix_N = transf.normalize_PQN(df_storage_train[lbl][i+1], ref_sample='mean')\n",
    "        human_datamatrix_G = transf.glog(human_datamatrix_N, lamb=None)\n",
    "        real_samples[lbl][i+1] = transf.pareto_scale(human_datamatrix_G)\n",
    "\n",
    "        df_storage_test[lbl][i+1] = human_datamatrix_I.iloc[test_idxs]\n",
    "        lbl_storage_test[lbl][i+1] = list(np.array(hd_labels)[test_idxs])\n",
    "        \n",
    "        test_datamatrix_N = transf.normalize_PQN(df_storage_test[lbl][i+1], ref_sample='mean')\n",
    "        test_datamatrix_G = transf.glog(test_datamatrix_N, lamb=None)\n",
    "        #df_storage_test = transf.pareto_scale(test_datamatrix_G)\n",
    "        # 'Faux' Pareto Scale\n",
    "        df_storage_test[lbl][i+1] = (test_datamatrix_G - human_datamatrix_G.mean())/np.sqrt(human_datamatrix_G.std())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7054a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "GENERATE_LIN=True\n",
    "if GENERATE_LIN:\n",
    "    aug_df_storage_train = {}\n",
    "    aug_lbl_storage_train = {}\n",
    "    # Only generation of samples based on the imbalanced dataset\n",
    "    for min_class in df_storage_train.keys():\n",
    "        aug_df_storage_train[min_class] = {}\n",
    "        aug_lbl_storage_train[min_class] = {}\n",
    "        for folds in df_storage_train[min_class].keys():\n",
    "            start = perf_counter()\n",
    "            data, lbls = laf.artificial_dataset_generator(df_storage_train[min_class][folds], \n",
    "                                                          labels=lbl_storage_train[min_class][folds],\n",
    "                                                    max_new_samples_per_label=512, binary=False, \n",
    "                                                    rnd=list(np.linspace(0.2,0.8,3)), \n",
    "                                                    binary_rnd_state=None, rnd_state=7519)\n",
    "            end = perf_counter()\n",
    "            print(f'Minority class: {min_class}, Folds: {folds}')\n",
    "            print(f'Simple augmentation of data done! took {(end - start):.3f} s')\n",
    "\n",
    "            data_N = transf.normalize_PQN(data, ref_sample='mean')\n",
    "            data_treated = transf.pareto_scale(transf.glog(data_N, lamb=None))\n",
    "\n",
    "            aug_df_storage_train[min_class][folds] = data_treated.copy()\n",
    "            aug_lbl_storage_train[min_class][folds] = lbls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e45d381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the generated data as to not need to make them again\n",
    "if GENERATE_LIN:\n",
    "    for lbl in aug_df_storage_train.keys():\n",
    "        for i in aug_df_storage_train[lbl].keys():\n",
    "            # Store Linear Augmented Data\n",
    "            aug_df_storage_train[lbl][i].to_csv('store_data/PCDint_imb_generated_data_'+lbl+str(i)+'.csv')\n",
    "\n",
    "            # Store labels/classes corresponding to each generated sample\n",
    "            with open('store_data/PCDint_imb_data_lbls_'+lbl+str(i)+'.txt', 'w') as a:\n",
    "                for item in aug_lbl_storage_train[lbl][i]:\n",
    "                    a.write(\"{}\\n\".format(item))\n",
    "\n",
    "# Read back the generated GAN data\n",
    "if not GENERATE_LIN:\n",
    "    aug_df_storage_train = {'Recurrence':{}, 'No Recurrence':{}}\n",
    "    aug_lbl_storage_train = {'Recurrence':{}, 'No Recurrence':{}}\n",
    "    for lbl in aug_df_storage_train:\n",
    "        for i in df_storage_train[lbl].keys():\n",
    "\n",
    "            data_treated = pd.read_csv('store_data/PCDint_imb_generated_data_'+lbl+str(i)+'.csv')\n",
    "            aug_df_storage_train[lbl][i] = data_treated.set_index(data_treated.columns[0])\n",
    "            data_treated\n",
    "            with open('store_data/PCDint_imb_data_lbls_'+lbl+str(i)+'.txt') as a:\n",
    "                aug_lbl_storage_train[lbl][i] = a.read().split('\\n')[:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770c784c",
   "metadata": {},
   "source": [
    "Set up colours for each of the classes. Generated samples will have the corresponding label with '- GAN' after."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404593ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "colours2 = sns.color_palette('tab20', 4)[:5]\n",
    "\n",
    "ordered_labels_test = ('Recurrence','Recurrence - GAN','No Recurrence','No Recurrence - GAN')\n",
    "label_colors_test = {lbl: c for lbl, c in zip(ordered_labels_test, colours2)}\n",
    "ordered_labels = ['Recurrence', 'No Recurrence']\n",
    "\n",
    "sns.palplot(label_colors_test.values())\n",
    "new_ticks_test = plt.xticks(range(len(ordered_labels_test)), ordered_labels_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0520e74a",
   "metadata": {},
   "source": [
    "## Conditional Wasserstein GAN - GP model\n",
    "\n",
    "This model construction was made by joining WGAN-GP models with Conditional GAN models. WGAN-GP models were originally made according to / originally based in https://keras.io/examples/generative/wgan_gp/#wasserstein-gan-wgan-with-gradient-penalty-gp and Conditional GAN models - https://machinelearningmastery.com/how-to-develop-a-conditional-generative-adversarial-network-from-scratch/ (generator and discriminator model) and https://keras.io/examples/generative/conditional_gan/ without using OOP (loss functions and training/training steps)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43aa5021",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_model(len_input, len_output, n_hidden_nodes, n_labels): \n",
    "    \"Make the generator model of CWGAN-GP.\"\n",
    "\n",
    "    data_input = tf.keras.Input(shape=(len_input,), name='data') # Take intensity input\n",
    "    label_input = tf.keras.Input(shape=(1,), name='label') # Take Label Input\n",
    "    \n",
    "    # Treat label input to concatenate to intensity data after\n",
    "    label_m = tf.keras.layers.Embedding(n_labels, 30, input_length=1)(label_input)\n",
    "    label_m = tf.keras.layers.Dense(256, activation='linear', use_bias=True)(label_m)\n",
    "    #label_m = tf.keras.layers.Reshape((len_input,1,))(label_m)\n",
    "    label_m2 = tf.keras.layers.Reshape((256,))(label_m)\n",
    "\n",
    "    joined_data = tf.keras.layers.Concatenate()([data_input, label_m2]) # Concatenate intensity and label data\n",
    "    # Hidden Dense Layer and Normalization\n",
    "    joined_data = tf.keras.layers.Dense(n_hidden_nodes, activation=tf.nn.leaky_relu, use_bias=True)(joined_data)\n",
    "    joined_data = tf.keras.layers.Dense(256, activation=tf.nn.leaky_relu, use_bias=True)(joined_data)\n",
    "    joined_data = tf.keras.layers.BatchNormalization()(joined_data)\n",
    "    \n",
    "    # Output - number of features of sample to make\n",
    "    output = tf.keras.layers.Dense(len_output, activation='linear', use_bias=True)(joined_data)\n",
    "    \n",
    "    generator = tf.keras.Model(inputs=[data_input, label_input], outputs=output)\n",
    "    \n",
    "    return generator\n",
    "\n",
    "\n",
    "def critic_model(len_input, n_hidden_nodes, n_labels):\n",
    "    \"Make the critic model of CWGAN-GP.\"\n",
    "    \n",
    "    label_input = tf.keras.Input(shape=(1,)) # Take intensity input\n",
    "    data_input = tf.keras.Input(shape=(len_input,)) # Take Label Input\n",
    "\n",
    "    # Treat label input to concatenate to intensity data after\n",
    "    label_m = tf.keras.layers.Embedding(n_labels, 30, input_length=1)(label_input)\n",
    "    label_m = tf.keras.layers.Dense(256, activation='linear', use_bias=True)(label_m)\n",
    "    #label_m = tf.keras.layers.Reshape((len_input,1,))(label_m)\n",
    "    label_m = tf.keras.layers.Reshape((256,))(label_m)\n",
    "\n",
    "    joined_data = tf.keras.layers.Concatenate()([data_input, label_m]) # Concatenate intensity and label data\n",
    "    # Hidden Dense Layer (Normalization worsened results here)\n",
    "    joined_data = tf.keras.layers.Dense(n_hidden_nodes, activation=tf.nn.leaky_relu, use_bias=True)(joined_data)\n",
    "    joined_data = tf.keras.layers.Dense(128, activation=tf.nn.leaky_relu, use_bias=True)(joined_data)\n",
    "    joined_data = tf.keras.layers.Dense(256, activation=tf.nn.leaky_relu, use_bias=True)(joined_data)\n",
    "    #joined_data = tf.keras.layers.BatchNormalization()(joined_data)\n",
    "\n",
    "    # Output Layer - 1 node for critic decision\n",
    "    output = tf.keras.layers.Dense(1, activation='linear', use_bias=True)(joined_data)\n",
    "    \n",
    "    critic = tf.keras.Model(inputs=[data_input, label_input], outputs=output)\n",
    "\n",
    "    return critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f1c898",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_predictions(model, num_examples_to_generate, len_input, input_dist, uni_lbls):\n",
    "    \"Generate sample predictions based on a Generator model.\"\n",
    "    \n",
    "    test_input =  tf.constant(input_dist.rvs(size=len_input*num_examples_to_generate), shape=[\n",
    "        num_examples_to_generate,len_input]) \n",
    "    \n",
    "    if len(uni_lbls) < 3:\n",
    "        test_labels = tf.constant([1.0]*(num_examples_to_generate//2) + [0.0]*(num_examples_to_generate//2), \n",
    "                                  shape=(num_examples_to_generate,1))\n",
    "    else:\n",
    "        test_labels = []\n",
    "        for i in range(len(uni_lbls)):\n",
    "            test_labels.extend([i]*(num_examples_to_generate//len(uni_lbls)))\n",
    "        test_labels = np.array(pd.get_dummies(test_labels))\n",
    "        #np.array(pd.get_dummies([i for i in range(len(uni_lbls))]*(num_examples_to_generate//len(uni_lbls))))\n",
    "    predictions = model([test_input, test_labels], training=False) # `training` is set to False.\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae73fadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_montage(train_data_o, train_lbls, test_data, test_lbls,\n",
    "                     epochs, generator, critic, generator_optimizer, critic_optimizer, input_dist,\n",
    "                    batch_size, grad_pen_weight=10, k_cov_den=50, k_crossLID=15, random_seed=145,\n",
    "                    n_generated_samples=96):\n",
    "    \"\"\"Train a generator and critic of CWGAN-GP.\n",
    "    \n",
    "       Receives training data and respective class labels (train_data_o and train_lbls) and trains a generator and a critic\n",
    "        model (generator, critic) over a number of epochs (epochs) with a set batch size (batch_size) with the respective \n",
    "        optimizers and learning rate (generator_optimizer, critic_optimizer). Gradient Penalty is calculated with\n",
    "        grad_pen_weight as the weight of the penalty.\n",
    "       The functions returns at time intervals three graphs to evaluate the progression of the models (Loss plots,\n",
    "        coverage, density, crossLID and correct first cluster plots and PCA plot with generated and test data). To this\n",
    "        end, samples need to be generated requiring the distribution to sample the initial input values from (input_dist),\n",
    "        and test data and respective labels has to be given (test_data and test_lbls). Finally the number of neighbors to\n",
    "        consider for coverage/density and crossLID calculation is also needed (k_cov_den, k_crossLID).\n",
    "    \n",
    "       train_data_o: Pandas DataFrame with training data;\n",
    "       train_lbls: List with training data class labels;\n",
    "       test_data: Pandas DataFrame with test data to evaluate the model;\n",
    "       test_lbls: List with test data class labels to evaluate the model;\n",
    "       epochs: Int value with the number of epochs to train the model;\n",
    "       generator: tensorflow keras.engine.functional.Functional model for the generator;\n",
    "       critic: tensorflow keras.engine.functional.Functional model for the critic;\n",
    "       generator_optimizer: tensorflow keras optimizer (with learning rate) for generator;\n",
    "       critic_optimizer: tensorflow keras optimizer (with learning rate) for critic;\n",
    "       input_dist: scipy.stats._continuous_distns.rv_histogram object - distribution to sample input values for generator;\n",
    "       batch_size: int value with size of batch for model training;\n",
    "       grad_pen_weight: int value (default 10) for penalty weight in gradient penalty calculation;\n",
    "       k_cov_den: int value (default 50) for number of neighbors to consider for coverage and density calculation in\n",
    "       generated samples evaluation;\n",
    "       k_crossLID: int value (default 15) for number of neighbors to consider for crossLID calculation in generated samples\n",
    "        evaluation.\n",
    "       random_seed: int value (default 145) for numpy random seeding when randomly organizing samples in the data that\n",
    "        will be split into batches.\n",
    "       n_generated_samples: int value (default 96) for number of samples generated to test the model during training.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Obtaining the train data, randomize its order and divide it be twice the standard deviation of its values\n",
    "    all_data = train_data_o.iloc[\n",
    "        np.random.RandomState(seed=random_seed).permutation(len(train_data_o))]/(2*train_data_o.values.std())\n",
    "    \n",
    "    # Same treatment for the test data\n",
    "    test_data = (test_data/(2*test_data.values.std())).values\n",
    "    training_data = all_data\n",
    "    train_data = all_data.values\n",
    "    \n",
    "    # Change class labels to numerical values while following the randomized ordered of samples\n",
    "    if len(set(train_lbls)) < 3: # 1 and 0 for when there are only two classes\n",
    "        train_labels = pd.get_dummies(\n",
    "            np.array(train_lbls)[np.random.RandomState(seed=random_seed).permutation(len(train_data))]).values[:,0]\n",
    "        test_labels = pd.get_dummies(np.array(test_lbls)).values[:,0]\n",
    "    else: # One hot encoding for when there are more than two classes\n",
    "        train_labels = pd.get_dummies(\n",
    "            np.array(train_lbls)[np.random.RandomState(seed=random_seed).permutation(len(train_data))]).values\n",
    "        test_labels = pd.get_dummies(np.array(test_lbls)).values\n",
    "    # Save the order of the labels\n",
    "    ordered_labels = pd.get_dummies(\n",
    "            np.array(train_lbls)[np.random.RandomState(seed=random_seed).permutation(len(train_data_o))]).columns\n",
    "\n",
    "    batch_divisions = int(batch_size / len(set(train_lbls))) # See how many samples of each class will be in each batch\n",
    "    n_steps = epochs * int(training_data.shape[0] / batch_size) # Number of steps: nº of batches per epoch * nº of epochs\n",
    "    n_critic = 5\n",
    "    \n",
    "    # Set up the evaluating images printed during training and the intervals they will be updated\n",
    "    f, (axl, axc, axr) = plt.subplots(1, 3, figsize = (16,5))\n",
    "    update1 = n_steps//200\n",
    "    update2 = n_steps//20\n",
    "\n",
    "    if hasattr(tqdm, '_instances'):\n",
    "        tqdm._instances.clear() # clear if it exists\n",
    "\n",
    "    i=0\n",
    "\n",
    "    for step in tqdm(range(n_steps)):\n",
    "        \n",
    "        # Critic Training\n",
    "        crit_loss_temp = []\n",
    "        \n",
    "        # Select real samples for this batch on training and order samples to put samples of the same class together\n",
    "        real_samp = train_data[i*batch_size:(i+1)*batch_size]\n",
    "        real_lbls = train_labels[i*batch_size:(i+1)*batch_size]\n",
    "\n",
    "        real_samples = np.empty(real_samp.shape)\n",
    "        real_labels = np.empty(real_lbls.shape)\n",
    "        a = 0\n",
    "        if len(set(train_lbls)) < 3:\n",
    "            for l,s in sorted(zip(real_lbls, real_samp), key=lambda pair: pair[0], reverse=True):\n",
    "                real_samples[a] = s\n",
    "                real_labels[a] = l\n",
    "                a = a+1\n",
    "        else:\n",
    "            for l,s in sorted(zip(real_lbls, real_samp), key=lambda pair: np.argmax(pair[0]), reverse=False):\n",
    "                #print(l, np.argmax(l))\n",
    "                real_samples[a] = s\n",
    "                real_labels[a] = l\n",
    "                a = a+1\n",
    "\n",
    "        for _ in range(n_critic): # For each step, train critic n_critic times\n",
    "            \n",
    "            # Generate input for generator\n",
    "            artificial_samples = tf.constant(input_dist.rvs(size=all_data.shape[1]*batch_size), shape=[\n",
    "                batch_size,all_data.shape[1]])\n",
    "            artificial_labels = real_labels.copy()\n",
    "\n",
    "            # Generate artificial samples from the latent vector\n",
    "            artificial_samples = generator([artificial_samples, artificial_labels], training=True)\n",
    "            \n",
    "            with tf.GradientTape() as crit_tape: # See the gradient for the critic\n",
    "\n",
    "                # Get the logits for the generated samples\n",
    "                X_artificial = critic([artificial_samples, artificial_labels], training=True)\n",
    "                # Get the logits for the real samples\n",
    "                X_true = critic([real_samples, real_labels], training=True)\n",
    "\n",
    "                # Calculate the critic loss using the generated and real sample results\n",
    "                c_cost = critic_loss_wgan(X_true, X_artificial)\n",
    "\n",
    "                # Calculate the gradient penalty\n",
    "                grad_pen = gradient_penalty_cwgan(batch_size, real_samples, artificial_samples,\n",
    "                                                  real_labels, artificial_labels, critic)\n",
    "                # Add the gradient penalty to the original discriminator loss\n",
    "                crit_loss = c_cost + grad_pen * grad_pen_weight\n",
    "                \n",
    "            crit_loss_temp.append(crit_loss)\n",
    "\n",
    "            # Calculate and apply the gradients obtained from the loss on the trainable variables\n",
    "            gradients_of_critic = crit_tape.gradient(crit_loss, critic.trainable_variables)\n",
    "            critic_optimizer.apply_gradients(zip(gradients_of_critic, critic.trainable_variables))\n",
    "\n",
    "        i = i + 1\n",
    "        if (step+1) % (n_steps//epochs) == 0:\n",
    "            i=0\n",
    "\n",
    "        crit_loss_all.append(np.mean(crit_loss_temp))\n",
    "        \n",
    "        # Generator Training\n",
    "        # Generate inputs for generator, values and labels\n",
    "        artificial_samples = tf.constant(input_dist.rvs(size=all_data.shape[1]*batch_size), shape=[\n",
    "                batch_size,all_data.shape[1]])\n",
    "        \n",
    "        if len(set(train_lbls)) < 3:\n",
    "            artificial_labels = tf.constant([1.0]*(batch_size//2) + [0.0]*(batch_size//2), shape=(batch_size,1))\n",
    "        else:\n",
    "            artificial_labels = np.array(pd.get_dummies([i for i in range(len(set(train_lbls)))]*batch_divisions))\n",
    "    \n",
    "        with tf.GradientTape() as gen_tape: # See the gradient for the generator\n",
    "            # Generate artificial samples\n",
    "            artificial_samples = generator([artificial_samples, artificial_labels], training=True)\n",
    "            \n",
    "            # Get the critic results for generated samples\n",
    "            X_artificial = critic([artificial_samples, artificial_labels], training=True)\n",
    "            # Calculate the generator loss\n",
    "            gen_loss = generator_loss_wgan(X_artificial)\n",
    "\n",
    "        # Calculate and apply the gradients obtained from the loss on the trainable variables\n",
    "        gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "        generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "        gen_loss_all.append(gen_loss)\n",
    "\n",
    "        # Update the progress bar and evaluation graphs every update1 steps for loss plots and update2 for the others.\n",
    "        if (step + 1) % update1 == 0:\n",
    "            \n",
    "            # Update the evaluating figures at the set intervals\n",
    "            axl.clear() # Always clear the corresponding ax before redrawing it\n",
    "            \n",
    "            # Loss Plot\n",
    "            axl.plot(gen_loss_all, color = 'blue', label='Generator Loss')\n",
    "            axl.plot(crit_loss_all,color = 'red', label='Critic Loss')\n",
    "            axl.set_xlabel('Number of Steps')\n",
    "            axl.set_ylabel('Loss')\n",
    "            axl.legend()\n",
    "            \n",
    "            ipythondisplay.clear_output(wait=True)\n",
    "            ipythondisplay.display(plt.gcf())\n",
    "\n",
    "        if (step + 1) % update2 == 0:\n",
    "\n",
    "            saved_predictions.append(generate_predictions(generator, n_generated_samples, all_data.shape[1], \n",
    "                                                          input_realdata_dist, ordered_labels))\n",
    "            # See density and coverage and crossLID (divided by 25 to be in the same order as the rest) \n",
    "            # of latest predictions\n",
    "            den, cov = gem.evaluation_coverage_density(test_data, saved_predictions[-1], k= k_cov_den, metric='euclidean')\n",
    "            clid = gem.cross_LID_estimator_byMLE(test_data, saved_predictions[-1], k=k_crossLID, metric='euclidean')/25\n",
    "            density.append(den)\n",
    "            coverage.append(cov)\n",
    "            crossLID.append(clid)\n",
    "\n",
    "            # PCA of the latest predictions and training data\n",
    "            # Divide by twice the standard deviation to be the same as the generated data\n",
    "            dfs_temp = pd.concat((train_data_o/(2*train_data_o.values.std()),pd.DataFrame(\n",
    "                saved_predictions[-1].numpy(), columns=train_data_o.columns))) \n",
    "            temp_lbls = train_lbls.copy()\n",
    "            for l in ordered_labels:\n",
    "                temp_lbls.extend([l+' - GAN']*(n_generated_samples//len(ordered_labels)))\n",
    "            principaldf = gem.pca_sample_projection(dfs_temp, temp_lbls, pca, whiten=True, \n",
    "                                                samp_number=len(train_data_o.index))\n",
    "            lcolors = label_colors_test\n",
    "\n",
    "            # Hierarchical clustering of the latest predictions and testing data, \n",
    "            # saving the correct 1st cluster fraction results\n",
    "            dfs_temp = np.concatenate((test_data, saved_predictions[-1].numpy()))\n",
    "            temp_lbls = ['real']*len(test_data) + ['gen']*len(saved_predictions[-1])\n",
    "            hca_results = gem.perform_HCA(dfs_temp, temp_lbls, metric='euclidean', method='ward')\n",
    "            corr1stcluster.append(hca_results['correct 1st clustering'])\n",
    "            \n",
    "            # Plots\n",
    "            axc.clear()\n",
    "            axc.plot(range(update2, step+2, update2), coverage, label='coverage')\n",
    "            axc.plot(range(update2, step+2, update2), density, label='density')\n",
    "            axc.plot(range(update2, step+2, update2), crossLID, color='red', label='crossLID/25')\n",
    "            axc.plot(range(update2, step+2, update2), corr1stcluster, color='purple', label='corr_cluster')\n",
    "            axc.legend()\n",
    "\n",
    "            axr.clear()\n",
    "            gem.plot_PCA(principaldf, lcolors, components=(1,2), title='', ax=axr)\n",
    "            axr.legend(loc='upper right', ncol=1, framealpha=1)\n",
    "            \n",
    "            ipythondisplay.clear_output(wait=True)\n",
    "            ipythondisplay.display(plt.gcf())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59bae48a",
   "metadata": {},
   "source": [
    "### Training the GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2450e688",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "GENERATE=True\n",
    "\n",
    "epochs = 500\n",
    "batch_size = 32\n",
    "k_cov_den = 20\n",
    "k_crossLID = 15\n",
    "random_seed = 145\n",
    "n_generated_samples = 48*len(pd.unique(aug_lbl_storage_train['Recurrence'][1]))\n",
    "\n",
    "if GENERATE:\n",
    "    generator_train = {}\n",
    "    critic_train = {}\n",
    "\n",
    "    results_train = {}\n",
    "\n",
    "    for lbl in df_storage_train:\n",
    "        generator_train[lbl] = {}\n",
    "        critic_train[lbl] = {}\n",
    "\n",
    "        results_train[lbl] = {}\n",
    "        for fold in df_storage_train[lbl]:\n",
    "            print(lbl, fold)\n",
    "            # Store results\n",
    "            gen_loss_all = []\n",
    "            crit_loss_all = []\n",
    "            saved_predictions = []\n",
    "            coverage = []\n",
    "            density = []\n",
    "            crossLID = []\n",
    "            corr1stcluster = []\n",
    "\n",
    "            # Get distribution of intensity values of the dataset\n",
    "            hist = np.histogram(real_samples[lbl][fold].values.flatten(), bins=100)\n",
    "            input_realdata_dist = stats.rv_histogram(hist)\n",
    "\n",
    "            df = real_samples[lbl][fold]\n",
    "            pca = PCA(n_components=2, svd_solver='full', whiten=True)\n",
    "            pc_coords = pca.fit_transform(df)\n",
    "\n",
    "            generator_optimizer = tf.keras.optimizers.RMSprop(5e-4)\n",
    "            critic_optimizer = tf.keras.optimizers.RMSprop(5e-4)\n",
    "\n",
    "            generator_train[lbl][fold] = generator_model(aug_df_storage_train[lbl][fold].shape[1],\n",
    "                                                 aug_df_storage_train[lbl][fold].shape[1], 128, 2)\n",
    "            critic_train[lbl][fold] = critic_model(aug_df_storage_train[lbl][fold].shape[1], 256, 2)\n",
    "\n",
    "            training_montage(aug_df_storage_train[lbl][fold], aug_lbl_storage_train[lbl][fold], \n",
    "                             real_samples[lbl][fold], lbl_storage_train[lbl][fold], \n",
    "                             epochs, generator_train[lbl][fold], critic_train[lbl][fold],\n",
    "                             generator_optimizer, critic_optimizer, input_realdata_dist, batch_size,\n",
    "                             grad_pen_weight=10, k_cov_den=k_cov_den, k_crossLID=k_crossLID,\n",
    "                             random_seed=random_seed, n_generated_samples=n_generated_samples)\n",
    "\n",
    "            results_train[lbl][fold]={'gen_loss': gen_loss_all, 'crit_loss': crit_loss_all, 'saved_pred': saved_predictions,\n",
    "                     'coverage': coverage, 'density': density, 'crossLID': crossLID, 'corr1st_cluster': corr1stcluster}\n",
    "\n",
    "            # Save the generator and critic models' weights.\n",
    "            generator_train[lbl][fold].save_weights('gan_models/PCDint_gen_imb_'+lbl+str(fold))\n",
    "            critic_train[lbl][fold].save_weights('gan_models/PCDint_crit_imb_'+lbl+str(fold))\n",
    "            # Save the results from GAN training\n",
    "            with open('gan_models/PCDint_results_imb_'+lbl+str(fold)+'.pickle', 'wb') as handle:\n",
    "                pickle.dump(results_train[lbl][fold], handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6ffa81",
   "metadata": {},
   "outputs": [],
   "source": [
    "if GENERATE:\n",
    "    for lbl in generator_train:\n",
    "        for fold in generator_train[lbl]:\n",
    "            # Save the generator and critic models' weights.\n",
    "            generator_train[lbl][fold].save_weights('gan_models/PCDint_gen_imb_'+lbl+str(fold))\n",
    "            critic_train[lbl][fold].save_weights('gan_models/PCDint_crit_imb_'+lbl+str(fold))\n",
    "    \n",
    "            # Save the results from GAN training\n",
    "            with open('gan_models/PCDint_results_imb_'+lbl+str(fold)+'.pickle', 'wb') as handle:\n",
    "                pickle.dump(results_train[lbl][fold], handle)\n",
    "else:\n",
    "    generator_train = {}\n",
    "    critic_train = {}\n",
    "\n",
    "    results_train = {}\n",
    "\n",
    "    for lbl in df_storage_train:\n",
    "        generator_train[lbl] = {}\n",
    "        critic_train[lbl] = {}\n",
    "\n",
    "        results_train[lbl] = {}\n",
    "        for fold in df_storage_train[lbl]:\n",
    "            # Read back the saved model\n",
    "            generator_optimizer = tf.keras.optimizers.RMSprop(1e-4)\n",
    "            critic_optimizer = tf.keras.optimizers.RMSprop(1e-4)\n",
    "\n",
    "            generator_train[lbl][fold] = generator_model(df_storage_train[lbl][fold].shape[1],\n",
    "                                                         df_storage_train[lbl][fold].shape[1], 128, 2)\n",
    "            critic_train[lbl][fold] = critic_model(df_storage_train[lbl][fold].shape[1],\n",
    "                                                   256, 2)\n",
    "\n",
    "            # Load previously saved models\n",
    "            generator_train[lbl][fold].load_weights('./gan_models/PCDint_gen_imb_'+lbl+str(fold))\n",
    "            critic_train[lbl][fold].load_weights('./gan_models/PCDint_crit_imb_'+lbl+str(fold))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0084f3d3",
   "metadata": {},
   "source": [
    "### Classification Accuracy Comparison\n",
    "\n",
    "Here we divided our dataset, getting 5 folds where each has 20 samples of a minority class and 80 of a majority class that act as the 'training set' and the remaining 80 samples of the minority class and 20 of the majority class as the 'test set' as well as the extra 35 'No Recurrence' and 14 'Recurrence' samples. Both classes are a minority class at one point.\n",
    "\n",
    "With the train set, we build and train a GAN model from them. Then we build models with the train set and with generated samples from the GAN models and compare the performance in discriminating the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6c8634",
   "metadata": {},
   "source": [
    "#### Generate a lot of samples and make Random Forests and PLS-DA models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd66945",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(5402)\n",
    "# Generate sample for each fold\n",
    "generated_samples = {}\n",
    "\n",
    "for i in generator_train:\n",
    "    generated_samples[i] = {}\n",
    "    for fold in generator_train[i]:\n",
    "        # Input to the generator\n",
    "        num_examples_to_generate = 2048\n",
    "        # Get distribution of intensity values of the dataset\n",
    "        hist = np.histogram(real_samples[i][fold].values.flatten(), bins=100)\n",
    "        input_realdata_dist = stats.rv_histogram(hist)\n",
    "\n",
    "        test_input = tf.constant(input_realdata_dist.rvs(\n",
    "            size=len(df_storage_train[i][fold].columns)*num_examples_to_generate),\n",
    "                                 shape=[num_examples_to_generate,len(df_storage_train[i][fold].columns)])\n",
    "\n",
    "        test_labels = tf.constant([0]*(num_examples_to_generate//2) + [1]*(num_examples_to_generate//2), shape=[\n",
    "            num_examples_to_generate,1])\n",
    "\n",
    "        # Generate GAN samples\n",
    "        predictions = generator_train[i][fold]([test_input, test_labels], training=False)\n",
    "        # Reverse the division done to the data\n",
    "        predictions = predictions * 2* aug_df_storage_train[i][fold].values.std()\n",
    "        \n",
    "        ordered_labels_fold = pd.get_dummies(\n",
    "            np.array(lbl_storage_train[i][fold])[np.random.RandomState(seed=random_seed).permutation(\n",
    "                len(lbl_storage_train[i][fold]))]).columns\n",
    "\n",
    "        generated_samples[i][fold] = [pd.DataFrame(np.array(predictions), columns=df_storage_train[i][fold].columns),\n",
    "                                [ordered_labels_fold[1],]*(num_examples_to_generate//2) + [ordered_labels_fold[0],]*(\n",
    "                                num_examples_to_generate//2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8e135e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To store for each fold\n",
    "bal_datasets = {}\n",
    "np.random.seed(325)\n",
    "rng = np.random.default_rng(7519)\n",
    "for i in generated_samples.keys():\n",
    "    bal_datasets[i] = {}\n",
    "    for fold in generator_train[i]:\n",
    "        bal_datasets[i][fold] = {}\n",
    "        df = real_samples[i][fold].loc[np.array(lbl_storage_train[i][fold]) == i]\n",
    "        # Calculate all correlations between all samples of experimental and GAN data and store them in a dataframe\n",
    "        correlations = pd.DataFrame(index=generated_samples[i][fold][0].index, columns=df.index).astype('float')\n",
    "\n",
    "        for a in df.index:\n",
    "            for j in generated_samples[i][fold][0].index:\n",
    "                correlations.loc[j,a] = stats.pearsonr(df.loc[a],\n",
    "                                                       generated_samples[i][fold][0].loc[j])[0]\n",
    "\n",
    "        correlated_samples = pd.DataFrame(columns=df.index)\n",
    "        for a in correlations:\n",
    "            correlated_samples[a] = correlations[a].sort_values(ascending=False).index\n",
    "            \n",
    "        permutated = correlated_samples.copy()\n",
    "        for l in correlated_samples.index:\n",
    "            permutated.loc[l] = rng.permutation(correlated_samples.loc[l])\n",
    "        #print(permutated)\n",
    "\n",
    "        corr_idxs = pd.unique(permutated.values.flatten())\n",
    "        \n",
    "        dataset_len = real_samples[i][fold].shape[0]\n",
    "        \n",
    "        n_min_class = (np.array(lbl_storage_train[i][fold]) == i).sum()\n",
    "        n_max_class = (dataset_len - n_min_class)//(len(pd.unique(lbl_storage_train[i][fold]))-1)\n",
    "        \n",
    "        # Slowly add samples - 3 at a time until a total of 60\n",
    "        for num in range(0,n_max_class - n_min_class+1, (n_max_class - n_min_class+1)//20):\n",
    "            idx_to_keep = corr_idxs[:num]\n",
    "\n",
    "            corr_preds = generated_samples[i][fold][0].loc[list(pd.unique(idx_to_keep))]\n",
    "            corr_lbls  = np.array(generated_samples[i][fold][1])[list(pd.unique(idx_to_keep))]\n",
    "\n",
    "            # Slowly add the GAN correlated GAN samples to the the imbalanced dataset, making it a balanced dataset\n",
    "            concat_df = pd.concat((corr_preds, real_samples[i][fold]))\n",
    "            concat_lbls = [i,]*len(set(idx_to_keep)) + lbl_storage_train[i][fold]\n",
    "            bal_datasets[i][fold][num] = [concat_df.copy(), concat_lbls.copy()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb43461",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Samples added to the minority class\n",
    "bal_datasets[i][fold].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b0fa66",
   "metadata": {},
   "source": [
    "### Fitting RF and PLS-DA models to Imbalance Datasets and Evaluating them\n",
    "\n",
    "RF and PLS-DA models are built for each minority class, each fold and each number of GAN samples added.\n",
    "\n",
    "RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb01c9c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Fitting and storing Random Forest models for each fold\n",
    "RF_models_bal = {}\n",
    "\n",
    "# Train the Models\n",
    "for min_class in bal_datasets:\n",
    "    RF_models_bal[min_class] = {}\n",
    "    for size in bal_datasets[min_class][1].keys():\n",
    "        RF_models_bal[min_class][size] = {}\n",
    "        for fold in bal_datasets[min_class]:\n",
    "            rf_mod = ma.RF_model(bal_datasets[min_class][fold][size][0], bal_datasets[min_class][fold][size][1],\n",
    "                                 return_cv=False, n_trees=200)\n",
    "            RF_models_bal[min_class][size][fold] = rf_mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bfb2985",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Testing the RF models with the test data for each fold\n",
    "RF_results_bal = {'Accuracy':{}, 'F1-Score':{}, 'Precision':{}, 'Recall':{}}\n",
    "# Evaluate the Models\n",
    "for min_class in RF_models_bal:\n",
    "    RF_results_bal['Accuracy'][min_class] = {}\n",
    "    RF_results_bal['F1-Score'][min_class] = {}\n",
    "    RF_results_bal['Precision'][min_class] = {}\n",
    "    RF_results_bal['Recall'][min_class] = {}\n",
    "    for size in RF_models_bal[min_class].keys():\n",
    "        RF_results_bal['Accuracy'][min_class][size] = {}\n",
    "        RF_results_bal['F1-Score'][min_class][size] = {}\n",
    "        RF_results_bal['Precision'][min_class][size] = {}\n",
    "        RF_results_bal['Recall'][min_class][size] = {}\n",
    "        for fold in RF_models_bal[min_class][size]:\n",
    "            RF_results_bal['Accuracy'][min_class][size][fold] = RF_models_bal[min_class][size][fold].score(\n",
    "                                                                                    df_storage_test[min_class][fold],\n",
    "                                                                                    lbl_storage_test[min_class][fold])\n",
    "            preds = RF_models_bal[min_class][size][fold].predict(df_storage_test[min_class][fold])\n",
    "            prec, rec, f1, sup = precision_recall_fscore_support(lbl_storage_test[min_class][fold], preds,\n",
    "                                                                pos_label=min_class, average='binary')\n",
    "            RF_results_bal['F1-Score'][min_class][size][fold] = f1\n",
    "            RF_results_bal['Precision'][min_class][size][fold] = prec\n",
    "            RF_results_bal['Recall'][min_class][size][fold] = rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f20fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.from_dict(RF_results_bal['Accuracy']['Recurrence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a488da",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.from_dict(RF_results_bal['Accuracy']['No Recurrence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de202b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the results for each fold\n",
    "f, (axl, axr) = plt.subplots(1,2,figsize=(10,4), constrained_layout=True)\n",
    "for min_class in RF_results_bal['Accuracy'].keys():\n",
    "    axl.plot((pd.DataFrame.from_dict(RF_results_bal['Accuracy'][min_class]).columns)/dataset_len*100,\n",
    "             pd.DataFrame.from_dict(RF_results_bal['Accuracy'][min_class]).mean(), \n",
    "             label=min_class+' Min. Accuracy')\n",
    "    axl.plot((pd.DataFrame.from_dict(RF_results_bal['F1-Score'][min_class]).columns)/dataset_len*100,\n",
    "             pd.DataFrame.from_dict(RF_results_bal['F1-Score'][min_class]).mean(), \n",
    "             label=min_class+' Min. F1-score')\n",
    "\n",
    "axl.set_ylabel('Performance', fontsize=15)\n",
    "axl.set_xlabel('% of Augmentation', fontsize=15)\n",
    "axl.set_ylim([0, 1.05])\n",
    "axl.legend(fontsize=15)\n",
    "axl.set_title('Random Forest')\n",
    "\n",
    "for min_class in RF_results_bal['Precision'].keys():\n",
    "    axr.plot((pd.DataFrame.from_dict(RF_results_bal['Precision'][min_class]).columns + n_min_class)/n_max_class*100,\n",
    "             pd.DataFrame.from_dict(RF_results_bal['Precision'][min_class]).mean(), \n",
    "             label=min_class+' Min. Precision')\n",
    "    axr.plot((pd.DataFrame.from_dict(RF_results_bal['Recall'][min_class]).columns + n_min_class)/n_max_class*100,\n",
    "             pd.DataFrame.from_dict(RF_results_bal['Recall'][min_class]).mean(), \n",
    "             label=min_class+' Min. Recall')\n",
    "\n",
    "axr.set_ylabel('Performance', fontsize=15)\n",
    "axr.set_xlabel('% of Augmentation', fontsize=15)\n",
    "axr.set_ylim([0, 1.05])\n",
    "axr.legend(fontsize=15)\n",
    "axr.set_title('Random Forest')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ff7702",
   "metadata": {},
   "source": [
    "PLS-DA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da52116",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decision_rule(y_pred, y_true, pos_label, average='binary'):\n",
    "    \"Decision rule for PLS-DA classification.\"\n",
    "    # Decision rule for classification\n",
    "    # Decision rule chosen: sample belongs to group where it has max y_pred (closer to 1)\n",
    "    # In case of 1,0 encoding for two groups, round to nearest integer to compare\n",
    "    nright = 0\n",
    "    rounded = np.round(y_pred)\n",
    "\n",
    "    for p in range(len(y_pred)):\n",
    "        if rounded[p] >= 1:\n",
    "            pred = 1\n",
    "            rounded[p] = 1\n",
    "        else:\n",
    "            pred = 0\n",
    "            rounded[p] = 0\n",
    "        if pred == y_true[p]:\n",
    "            nright += 1  # Correct prediction\n",
    "    \n",
    "    # Calculate accuracy for this iteration\n",
    "    accuracy = (nright / len(y_pred))\n",
    "    prec, rec, f1, sup = precision_recall_fscore_support(y_true, rounded, pos_label=pos_label, average=average)\n",
    "    return accuracy, f1, prec, rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c57e188",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "PLSDA_models_bal = {}\n",
    "PLSDA_results_bal = {'Accuracy':{}, 'F1-Score':{}, 'Precision':{}, 'Recall':{}}\n",
    "\n",
    "np.random.seed(325)\n",
    "\n",
    "# Train and Evaluate the models\n",
    "for min_class in bal_datasets:\n",
    "    PLSDA_models_bal[min_class] = {}\n",
    "    PLSDA_results_bal['Accuracy'][min_class] = {}\n",
    "    PLSDA_results_bal['F1-Score'][min_class] = {}\n",
    "    PLSDA_results_bal['Precision'][min_class] = {}\n",
    "    PLSDA_results_bal['Recall'][min_class] = {}\n",
    "    for size in bal_datasets[min_class][1].keys():\n",
    "        PLSDA_models_bal[min_class][size] = {}\n",
    "        PLSDA_results_bal['Accuracy'][min_class][size] = {}\n",
    "        PLSDA_results_bal['F1-Score'][min_class][size] = {}\n",
    "        PLSDA_results_bal['Precision'][min_class][size] = {}\n",
    "        PLSDA_results_bal['Recall'][min_class][size] = {}\n",
    "        for fold in bal_datasets[min_class]:\n",
    "\n",
    "            PLSDA_models_bal[min_class][size][fold] = ma.fit_PLSDA_model(bal_datasets[min_class][fold][size][0],\n",
    "                                                                   bal_datasets[min_class][fold][size][1],\n",
    "                                                              n_comp=9,\n",
    "                                                      return_scores=False, scale=False, encode2as1vector=True)\n",
    "            plsda = PLSDA_models_bal[min_class][size][fold]\n",
    "            # Obtain results with the test group\n",
    "            y_pred = plsda.predict(df_storage_test[min_class][fold])\n",
    "            y_true = ma._generate_y_PLSDA(lbl_storage_test[min_class][fold],\n",
    "                                          pd.unique(bal_datasets[min_class][fold][size][1]),\n",
    "                                          True)\n",
    "            pos_label = np.where(pd.unique(bal_datasets[min_class][fold][size][1]) != min_class)[0][0]\n",
    "            # Calculate accuracy\n",
    "            accuracy, f1, prec, rec = decision_rule(y_pred, y_true, pos_label=pos_label, average='binary')\n",
    "            PLSDA_results_bal['Accuracy'][min_class][size][fold] = accuracy\n",
    "            PLSDA_results_bal['F1-Score'][min_class][size][fold] = f1\n",
    "            PLSDA_results_bal['Precision'][min_class][size][fold] = prec\n",
    "            PLSDA_results_bal['Recall'][min_class][size][fold] = rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d150be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.from_dict(PLSDA_results_bal['Accuracy']['No Recurrence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6dddd67",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.from_dict(PLSDA_results_bal['Accuracy']['Recurrence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6eaa2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the results for each fold\n",
    "f, (axl, axr) = plt.subplots(1,2,figsize=(10,4), constrained_layout=True)\n",
    "for min_class in PLSDA_results_bal['Accuracy'].keys():\n",
    "    axl.plot((pd.DataFrame.from_dict(PLSDA_results_bal['Accuracy'][min_class]).columns)/dataset_len*100,\n",
    "             pd.DataFrame.from_dict(PLSDA_results_bal['Accuracy'][min_class]).mean(), \n",
    "             label=min_class+' Min. Accuracy')\n",
    "    axl.plot((pd.DataFrame.from_dict(PLSDA_results_bal['F1-Score'][min_class]).columns)/dataset_len*100,\n",
    "             pd.DataFrame.from_dict(PLSDA_results_bal['F1-Score'][min_class]).mean(), \n",
    "             label=min_class+' Min. F1-score')\n",
    "\n",
    "axl.set_ylabel('Performance', fontsize=15)\n",
    "axl.set_xlabel('% of Augmentation', fontsize=15)\n",
    "axl.set_ylim([0, 1.05])\n",
    "axl.legend(fontsize=15)\n",
    "axl.set_title('PLS-DA')\n",
    "\n",
    "for min_class in PLSDA_results_bal['Precision'].keys():\n",
    "    axr.plot((pd.DataFrame.from_dict(PLSDA_results_bal['Precision'][min_class]).columns + n_min_class)/n_max_class*100,\n",
    "             pd.DataFrame.from_dict(PLSDA_results_bal['Precision'][min_class]).mean(), \n",
    "             label=min_class+' Min. Precision')\n",
    "    axr.plot((pd.DataFrame.from_dict(PLSDA_results_bal['Recall'][min_class]).columns + n_min_class)/n_max_class*100,\n",
    "             pd.DataFrame.from_dict(PLSDA_results_bal['Recall'][min_class]).mean(), \n",
    "             label=min_class+' Min. Recall')\n",
    "\n",
    "axr.set_ylabel('Performance', fontsize=15)\n",
    "axr.set_xlabel('% of Augmentation', fontsize=15)\n",
    "axr.set_ylim([0, 1.05])\n",
    "axr.legend(fontsize=15)\n",
    "axr.set_title('PLS-DA')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240782e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the results for each fold\n",
    "f, axs = plt.subplots(1,3,figsize=(12,4), constrained_layout=True)\n",
    "for i, ax in zip(['F1-Score', 'Precision', 'Recall'], axs.ravel()):\n",
    "    avg_increase = (pd.DataFrame.from_dict(RF_results_bal[i]['No Recurrence']).mean()\n",
    "                    + pd.DataFrame.from_dict(RF_results_bal[i]['Recurrence']).mean())/2\n",
    "    ax.plot((avg_increase.index + n_min_class)/n_max_class*100, avg_increase, \n",
    "             label='Avg. RF')\n",
    "\n",
    "    avg_increase = (pd.DataFrame.from_dict(PLSDA_results_bal[i]['No Recurrence']).mean()\n",
    "                    + pd.DataFrame.from_dict(PLSDA_results_bal[i]['Recurrence']).mean())/2\n",
    "    ax.plot((avg_increase.index + n_min_class)/n_max_class*100, avg_increase, \n",
    "             label='Avg. PLS-DA')\n",
    "    ax.set_ylim([0, 1.01])\n",
    "    ax.set_title(i, fontsize=15)\n",
    "\n",
    "axs[0].set_ylabel('Performance', fontsize=15)\n",
    "axs[1].set_xlabel('% of Samples of the Minority Class in Comparison to the Majority Class', fontsize=15)\n",
    "axs[1].legend(fontsize=13)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc4c1d4",
   "metadata": {},
   "source": [
    "##### Extracting Important Features for RF models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e54df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting Important Features and averaging them across the 5 folds for each minority class\n",
    "rf_feats = {}\n",
    "\n",
    "for cl, models in RF_models_bal.items():\n",
    "    rf_feats[cl] = {}\n",
    "    for size, folds in models.items():\n",
    "        rf_feats[cl][size] = {}\n",
    "        for fold, mod in folds.items():\n",
    "            if fold == 1:\n",
    "                temp_df_bal = mod.feature_importances_\n",
    "            else:\n",
    "                temp_df_bal = temp_df_bal + mod.feature_importances_\n",
    "\n",
    "        temp_df_bal = temp_df_bal / len(folds)\n",
    "\n",
    "        rf_feats[cl][size] = dict(zip(range(1, len(bal_datasets[cl][1][size][0].columns)+1), temp_df_bal))\n",
    "\n",
    "# Averaging feature importance over the minority classes as well\n",
    "rf_feats_together = (pd.DataFrame.from_dict(rf_feats['No Recurrence']) + pd.DataFrame.from_dict(rf_feats['Recurrence']))/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d83bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the top 2% of important features\n",
    "rf_feats_together_abrev = pd.Series(index=rf_feats_together.columns)\n",
    "top10 = int(0.02*len(rf_feats_together.index))\n",
    "for i in rf_feats_together.columns:\n",
    "    rf_feats_together_abrev[i] = rf_feats_together[i].sort_values(ascending=False)[:top10].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1995f142",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the top 2% of important features\n",
    "rf_feats_abrev = {}\n",
    "for min_class in rf_feats:\n",
    "    temp_df = pd.DataFrame.from_dict(rf_feats[min_class])\n",
    "    rf_feats_abrev[min_class] = pd.Series(index=temp_df.columns)\n",
    "    top10 = int(0.02*len(temp_df.index))\n",
    "    for i in temp_df.columns:\n",
    "        rf_feats_abrev[min_class][i] = temp_df[i].sort_values(ascending=False)[:top10].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d08d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "top10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0293191",
   "metadata": {},
   "source": [
    "Relative importance values depending on the number of GAN samples added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002225a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the results and adjusting parameters of the plot\n",
    "with plt.style.context('seaborn-whitegrid'):\n",
    "    f, ax = plt.subplots(figsize=(6,6))\n",
    "\n",
    "    plt.plot((np.array(list(rf_feats_together_abrev.index)+n_min_class))/n_max_class*100,\n",
    "             rf_feats_together_abrev.values/rf_feats_together_abrev.values[0]*100, label='All Together',\n",
    "             color='Black', linewidth=3)\n",
    "    for i in rf_feats_abrev:\n",
    "        plt.plot((np.array(list(rf_feats_together_abrev.index)+n_min_class))/n_max_class*100,\n",
    "             rf_feats_abrev[i].values/rf_feats_abrev[i].values[0]*100,\n",
    "                 label='Minority Class: ' +i, color=label_colors_test[i])\n",
    "\n",
    "    plt.ylabel('Avg. Gini Importance of top 2% Imp. features change (%)', fontsize = 12)\n",
    "    plt.xlabel('% of Augmentation', fontsize = 15)\n",
    "    plt.ylim(80,160)\n",
    "    plt.legend(fontsize=13)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=13)\n",
    "    plt.title('Random Forest', fontsize=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1bfd86b",
   "metadata": {},
   "source": [
    "Common top 2% important features compared to the complete models depending on the number of GAN samples added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969d6747",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_line = []\n",
    "for i in rf_feats_together:\n",
    "    idxs = []\n",
    "    for l in rf_feats_together[i].sort_values(ascending=False)[:top10].index:\n",
    "        idxs.append(bal_datasets['No Recurrence'][1][0][0].columns[l-1])\n",
    "    rf_line.append(len(np.intersect1d(idxs, RF_feats_real.index[:top10])))\n",
    "    \n",
    "rf_line_uni = []\n",
    "for i in rf_feats_together:\n",
    "    idxs = []\n",
    "    for l in rf_feats_together[i].sort_values(ascending=False)[:top10].index:\n",
    "        idxs.append(bal_datasets['No Recurrence'][1][0][0].columns[l-1])\n",
    "    rf_line_uni.append(len(np.intersect1d(idxs, uni_results_filt.index)))\n",
    "    \n",
    "rf_line_unitop = []\n",
    "for i in rf_feats_together:\n",
    "    idxs = []\n",
    "    for l in rf_feats_together[i].sort_values(ascending=False)[:top10].index:\n",
    "        idxs.append(bal_datasets['No Recurrence'][1][0][0].columns[l-1])\n",
    "    rf_line_unitop.append(len(np.intersect1d(idxs, uni_results_filt.index[:top10])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffca2921",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the results and adjusting parameters of the plot\n",
    "with plt.style.context('seaborn-whitegrid'):\n",
    "    f, (axl, axc, axr) = plt.subplots(1,3,figsize=(15,5))\n",
    "\n",
    "    for min_class in rf_feats:\n",
    "        rf_line_min = []\n",
    "        for i in rf_feats[min_class]:\n",
    "            idxs = []\n",
    "            for l in pd.DataFrame(rf_feats[min_class])[i].sort_values(ascending=False)[:top10].index:\n",
    "                idxs.append(bal_datasets[min_class][1][0][0].columns[l-1])\n",
    "            rf_line_min.append(len(np.intersect1d(idxs, RF_feats_real.index[:top10])))\n",
    "\n",
    "        rf_line_uni_min = []\n",
    "        for i in rf_feats[min_class]:\n",
    "            idxs = []\n",
    "            for l in pd.DataFrame(rf_feats[min_class])[i].sort_values(ascending=False)[:top10].index:\n",
    "                idxs.append(bal_datasets[min_class][1][0][0].columns[l-1])\n",
    "            rf_line_uni_min.append(len(np.intersect1d(idxs, uni_results_filt.index)))\n",
    "            \n",
    "        rf_line_unitop_min = []\n",
    "        for i in rf_feats[min_class]:\n",
    "            idxs = []\n",
    "            for l in pd.DataFrame(rf_feats[min_class])[i].sort_values(ascending=False)[:top10].index:\n",
    "                idxs.append(bal_datasets[min_class][1][0][0].columns[l-1])\n",
    "            rf_line_unitop_min.append(len(np.intersect1d(idxs, uni_results_filt.index[:top10])))\n",
    "\n",
    "        axl.plot((np.array(list(rf_feats[min_class].keys()))+n_min_class)/n_max_class*100, np.array(rf_line_min)/top10*100,\n",
    "                 color=label_colors_test[min_class],\n",
    "                 label='Minority Class: ' +min_class)\n",
    "        axc.plot((np.array(list(rf_feats[min_class].keys()))+n_min_class)/n_max_class*100,\n",
    "                 np.array(rf_line_uni_min)/top10*100,\n",
    "                 color=label_colors_test[min_class],\n",
    "                 label='Minority Class: ' +min_class)\n",
    "        axr.plot((np.array(list(rf_feats[min_class].keys()))+n_min_class)/n_max_class*100,\n",
    "                 np.array(rf_line_unitop_min)/top10*100,\n",
    "                 color=label_colors_test[min_class],\n",
    "                 label='Minority Class: ' +min_class)\n",
    "\n",
    "axl.plot((np.array(list(rf_feats_together.keys()))+n_min_class)/n_max_class*100,\n",
    "             np.array(rf_line)/top10*100, color='black',\n",
    "            label='All Together', linewidth=3)\n",
    "axc.plot((np.array(list(rf_feats_together.keys()))+n_min_class)/n_max_class*100,\n",
    "         np.array(rf_line_uni)/top10*100, color='black',\n",
    "        label='All Together', linewidth=3)\n",
    "axr.plot((np.array(list(rf_feats_together.keys()))+n_min_class)/n_max_class*100,\n",
    "         np.array(rf_line_unitop)/top10*100, color='black',\n",
    "        label='All Together', linewidth=3)  \n",
    "\n",
    "axl.set_title('Against RF Top 2% Imp. Feats', fontsize=15)\n",
    "axc.set_title('Against Univariate Significant Feat.', fontsize=15)\n",
    "axr.set_title('Against Univariate Top 2% Significant Feat.', fontsize=14)\n",
    "axl.set_ylabel('% of Common Features with Real Model', fontsize = 15)\n",
    "axc.set_xlabel('% of Samples of the Minority Class in Comparison to the Majority Class', fontsize = 15)\n",
    "axl.set_ylim(20,102)\n",
    "axc.set_ylim(20,102)\n",
    "axr.set_ylim(20,102)\n",
    "#plt.legend(loc='upper left', fontsize=13, bbox_to_anchor=(1,1))\n",
    "axc.legend(loc='lower left', fontsize=13)\n",
    "\n",
    "plt.suptitle('Random Forest', fontsize=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75ced05",
   "metadata": {},
   "source": [
    "##### Extracting Important Features for PLS-DA models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f86058",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting Important Features and averaging them across the 5 folds for each minority class\n",
    "plsda_feats = {}\n",
    "\n",
    "for cl, models in PLSDA_models_bal.items():\n",
    "    plsda_feats[cl] = {}\n",
    "    for size, folds in models.items():\n",
    "        plsda_feats[cl][size] = {}\n",
    "        for fold, mod in folds.items():\n",
    "            if fold == 1:\n",
    "                temp_df_bal = ma._calculate_vips(mod)\n",
    "            else:\n",
    "                temp_df_bal = temp_df_bal + ma._calculate_vips(mod)\n",
    "\n",
    "        temp_df_bal = temp_df_bal / len(folds)\n",
    "\n",
    "        plsda_feats[cl][size] = dict(zip(range(1, len(bal_datasets[cl][1][size][0].columns)+1), temp_df_bal))\n",
    "\n",
    "# Averaging feature importance over the minority classes as well\n",
    "plsda_feats_together = (pd.DataFrame.from_dict(plsda_feats['Recurrence']) + pd.DataFrame.from_dict(plsda_feats[\n",
    "    'No Recurrence']))/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e952e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the top 2% of important features\n",
    "plsda_feats_together_abrev = pd.Series(index=plsda_feats_together.columns)\n",
    "top10 = int(0.02*len(plsda_feats_together.index))\n",
    "for i in plsda_feats_together.columns:\n",
    "    plsda_feats_together_abrev[i] = plsda_feats_together[i].sort_values(ascending=False)[:top10].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2bb13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the top 2% of important features\n",
    "plsda_feats_abrev = {}\n",
    "for min_class in plsda_feats:\n",
    "    temp_df = pd.DataFrame.from_dict(plsda_feats[min_class])\n",
    "    plsda_feats_abrev[min_class] = pd.Series(index=temp_df.columns)\n",
    "    top10 = int(0.02*len(temp_df.index))\n",
    "    for i in temp_df.columns:\n",
    "        plsda_feats_abrev[min_class][i] = temp_df[i].sort_values(ascending=False)[:top10].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c0fa7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "top10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4199284c",
   "metadata": {},
   "source": [
    "Relative importance values depending on the number of GAN samples added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3bb812",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the results and adjusting parameters of the plot\n",
    "with plt.style.context('seaborn-whitegrid'):\n",
    "    f, ax = plt.subplots(figsize=(6,6))\n",
    "\n",
    "    plt.plot((np.array(list(plsda_feats_together_abrev.index)+n_min_class))/n_max_class*100,\n",
    "             plsda_feats_together_abrev.values/plsda_feats_together_abrev.values[0]*100, label='All Together',\n",
    "             color='Black', linewidth=3)\n",
    "    for i in plsda_feats_abrev:\n",
    "        plt.plot((np.array(list(plsda_feats_together_abrev.index)+n_min_class))/n_max_class*100,\n",
    "             plsda_feats_abrev[i].values/plsda_feats_abrev[i].values[0]*100,\n",
    "                 label='Minority Class: ' +i, color=label_colors_test[i])\n",
    "\n",
    "    plt.ylabel('Avg. VIP Score of top 2% Imp. features change (%)', fontsize = 12)\n",
    "    plt.xlabel('% of Augmentation', fontsize = 15)\n",
    "    plt.ylim(90,120)\n",
    "    plt.legend(fontsize=13)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=13)\n",
    "    plt.title('PLS-DA', fontsize=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7961c1",
   "metadata": {},
   "source": [
    "Common top 2% important features compared to the complete models depending on the number of GAN samples added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2848a82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plsda_line = []\n",
    "for i in plsda_feats_together:\n",
    "    idxs = []\n",
    "    for l in plsda_feats_together[i].sort_values(ascending=False)[:top10].index:\n",
    "        idxs.append(bal_datasets['No Recurrence'][1][0][0].columns[l-1])\n",
    "    plsda_line.append(len(np.intersect1d(idxs, PLSDA_feats_real.index[:top10])))\n",
    "    \n",
    "plsda_line_uni = []\n",
    "for i in plsda_feats_together:\n",
    "    idxs = []\n",
    "    for l in plsda_feats_together[i].sort_values(ascending=False)[:top10].index:\n",
    "        idxs.append(bal_datasets['No Recurrence'][1][0][0].columns[l-1])\n",
    "    plsda_line_uni.append(len(np.intersect1d(idxs, uni_results_filt.index)))\n",
    "    \n",
    "plsda_line_unitop = []\n",
    "for i in plsda_feats_together:\n",
    "    idxs = []\n",
    "    for l in plsda_feats_together[i].sort_values(ascending=False)[:top10].index:\n",
    "        idxs.append(bal_datasets['No Recurrence'][1][0][0].columns[l-1])\n",
    "    plsda_line_unitop.append(len(np.intersect1d(idxs, uni_results_filt.index[:top10])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e549d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the results and adjusting parameters of the plot\n",
    "with plt.style.context('seaborn-whitegrid'):\n",
    "    f, (axl, axc, axr) = plt.subplots(1,3,figsize=(15,5))\n",
    "\n",
    "    for min_class in plsda_feats:\n",
    "        plsda_line_min = []\n",
    "        for i in plsda_feats[min_class]:\n",
    "            idxs = []\n",
    "            for l in pd.DataFrame(plsda_feats[min_class])[i].sort_values(ascending=False)[:top10].index:\n",
    "                idxs.append(bal_datasets[min_class][1][0][0].columns[l-1])\n",
    "            plsda_line_min.append(len(np.intersect1d(idxs, PLSDA_feats_real.index[:top10])))\n",
    "\n",
    "        plsda_line_uni_min = []\n",
    "        for i in plsda_feats[min_class]:\n",
    "            idxs = []\n",
    "            for l in pd.DataFrame(plsda_feats[min_class])[i].sort_values(ascending=False)[:top10].index:\n",
    "                idxs.append(bal_datasets[min_class][1][0][0].columns[l-1])\n",
    "            plsda_line_uni_min.append(len(np.intersect1d(idxs, uni_results_filt.index)))\n",
    "            \n",
    "        plsda_line_unitop_min = []\n",
    "        for i in plsda_feats[min_class]:\n",
    "            idxs = []\n",
    "            for l in pd.DataFrame(plsda_feats[min_class])[i].sort_values(ascending=False)[:top10].index:\n",
    "                idxs.append(bal_datasets[min_class][1][0][0].columns[l-1])\n",
    "            plsda_line_unitop_min.append(len(np.intersect1d(idxs, uni_results_filt.index[:top10])))\n",
    "\n",
    "        axl.plot((np.array(list(plsda_feats[min_class].keys()))+n_min_class)/n_max_class*100,\n",
    "                 np.array(plsda_line_min)/top10*100,\n",
    "                 color=label_colors_test[min_class],\n",
    "                 label='Minority Class: ' +min_class)\n",
    "        axc.plot((np.array(list(plsda_feats[min_class].keys()))+n_min_class)/n_max_class*100,\n",
    "                 np.array(plsda_line_uni_min)/top10*100,\n",
    "                 color=label_colors_test[min_class],\n",
    "                 label='Minority Class: ' +min_class)\n",
    "        axr.plot((np.array(list(plsda_feats[min_class].keys()))+n_min_class)/n_max_class*100,\n",
    "                 np.array(plsda_line_unitop_min)/top10*100,\n",
    "                 color=label_colors_test[min_class],\n",
    "                 label='Minority Class: ' +min_class)\n",
    "\n",
    "axl.plot((np.array(list(plsda_feats_together.keys()))+n_min_class)/n_max_class*100,\n",
    "             np.array(plsda_line)/top10*100, color='black',\n",
    "            label='All Together', linewidth=3)\n",
    "axc.plot((np.array(list(plsda_feats_together.keys()))+n_min_class)/n_max_class*100,\n",
    "         np.array(plsda_line_uni)/top10*100, color='black',\n",
    "        label='All Together', linewidth=3)\n",
    "axr.plot((np.array(list(plsda_feats_together.keys()))+n_min_class)/n_max_class*100,\n",
    "         np.array(plsda_line_unitop)/top10*100, color='black',\n",
    "        label='All Together', linewidth=3)  \n",
    "\n",
    "axl.set_title('Against PLS-DA Top 2% Imp. Feats', fontsize=15)\n",
    "axc.set_title('Against Univariate Significant Feat.', fontsize=15)\n",
    "axr.set_title('Against Univariate Top 2% Significant Feat.', fontsize=14)\n",
    "axl.set_ylabel('% of Common Features with Real Model', fontsize = 15)\n",
    "axc.set_xlabel('% of Samples of the Minority Class in Comparison to the Majority Class', fontsize = 15)\n",
    "axl.set_ylim(20,102)\n",
    "axc.set_ylim(20,102)\n",
    "axr.set_ylim(20,102)\n",
    "axc.legend(loc='lower left', fontsize=13)\n",
    "\n",
    "plt.suptitle('PLS-DA', fontsize=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c998f07",
   "metadata": {},
   "source": [
    "#### Summary of Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bcad818",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the results for each fold\n",
    "with plt.style.context('seaborn-whitegrid'):\n",
    "    f, (axl,axr) = plt.subplots(1,2,figsize=(8,3), constrained_layout=True)\n",
    "    for i in ['F1-Score', 'Precision', 'Recall']:\n",
    "        avg_increase = (pd.DataFrame.from_dict(RF_results_bal[i]['No Recurrence']).mean()\n",
    "                        + pd.DataFrame.from_dict(RF_results_bal[i]['Recurrence']).mean())/2\n",
    "        axl.plot((avg_increase.index + n_min_class)/n_max_class*100, avg_increase, \n",
    "                 label='Avg. ' + i)\n",
    "\n",
    "        avg_increase = (pd.DataFrame.from_dict(PLSDA_results_bal[i]['No Recurrence']).mean()\n",
    "                        + pd.DataFrame.from_dict(PLSDA_results_bal[i]['Recurrence']).mean())/2\n",
    "        axr.plot((avg_increase.index + n_min_class)/n_max_class*100, avg_increase, \n",
    "                 label='Avg. ' + i)\n",
    "        axl.set_ylim([0, 1.05])\n",
    "        axr.set_ylim([0, 1.05])\n",
    "        axl.set_title('RF', fontsize=13)\n",
    "        axr.set_title('PLS-DA', fontsize=13)\n",
    "\n",
    "    axl.set_ylabel('Performance', fontsize=13)\n",
    "    axr.set_ylabel('Performance', fontsize=13)\n",
    "    axl.set_xlabel('Class Balance (%)', fontsize=13)\n",
    "    axr.set_xlabel('Class Balance (%)', fontsize=13)\n",
    "    #f.supxlabel('Class Balance (%)', fontsize=15)\n",
    "    axl.legend(fontsize=13)\n",
    "    axl.tick_params(axis='both', which='major', labelsize=11)\n",
    "    axr.tick_params(axis='both', which='major', labelsize=11)\n",
    "\n",
    "    plt.show()\n",
    "    f.savefig('images/PCDint_Imbalanced_AugPerformance_plot.png', dpi=400)\n",
    "    f.savefig('images/PCDint_Imbalanced_AugPerformance_plot.pdf', dpi=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01158e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the results and adjusting parameters of the plot\n",
    "with plt.style.context('seaborn-whitegrid'):\n",
    "    f, (axl, axr) = plt.subplots(1,2, figsize=(8,4), constrained_layout=True)\n",
    "\n",
    "    axl.plot((np.array(list(rf_feats_together_abrev.index)+n_min_class))/n_max_class*100,\n",
    "             rf_feats_together_abrev.values/rf_feats_together_abrev.values[0]*100, label='All Together',\n",
    "             color='Black', linewidth=3)\n",
    "    for i in rf_feats_abrev:\n",
    "        axl.plot((np.array(list(rf_feats_together_abrev.index)+n_min_class))/n_max_class*100,\n",
    "             rf_feats_abrev[i].values/rf_feats_abrev[i].values[0]*100,\n",
    "                 label='Minority Class: ' +i, color=label_colors_test[i])\n",
    "\n",
    "    axl.set_ylabel('Avg. Gini Imp. of top 2% Imp. Feat. change (%)', fontsize = 10)\n",
    "    axl.set_ylim(75,150)\n",
    "    axl.legend(fontsize=12)\n",
    "    axl.tick_params(axis='both', which='major', labelsize=11)\n",
    "    axl.set_title('RF', fontsize=15)\n",
    "    \n",
    "    axr.plot((np.array(list(plsda_feats_together_abrev.index)+n_min_class))/n_max_class*100,\n",
    "             plsda_feats_together_abrev.values/plsda_feats_together_abrev.values[0]*100, label='All Together',\n",
    "             color='Black', linewidth=3)\n",
    "    for i in plsda_feats_abrev:\n",
    "        axr.plot((np.array(list(plsda_feats_together_abrev.index)+n_min_class))/n_max_class*100,\n",
    "             plsda_feats_abrev[i].values/plsda_feats_abrev[i].values[0]*100,\n",
    "                 label='Minority Class: ' +i, color=label_colors_test[i])\n",
    "\n",
    "    axr.set_ylabel('Avg. VIP Score of top 2% Imp. Feat. change (%)', fontsize = 10)\n",
    "    f.supxlabel('% of Samples of the Minority Class in Comparison to the Majority Class', fontsize = 15)\n",
    "    axr.set_ylim(85,115)\n",
    "    axr.tick_params(axis='both', which='major', labelsize=11)\n",
    "    axr.set_title('PLS-DA', fontsize=15)\n",
    "    f.savefig('images/PCDint_Imbalanced_ImpFeatChange_plot.png', dpi=400)\n",
    "    f.savefig('images/PCDint_Imbalanced_ImpFeatChange_plot.pdf', dpi=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9123a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the results and adjusting parameters of the plot\n",
    "with plt.style.context('seaborn-whitegrid'):\n",
    "    f, (axu, axd) = plt.subplots(2,3,figsize=(15,10), constrained_layout=True)\n",
    "    axul, axuc, axur = axu\n",
    "    axdl, axdc, axdr = axd\n",
    "    \n",
    "    for min_class in rf_feats:\n",
    "        rf_line_min = []\n",
    "        for i in rf_feats[min_class]:\n",
    "            idxs = []\n",
    "            for l in pd.DataFrame(rf_feats[min_class])[i].sort_values(ascending=False)[:top10].index:\n",
    "                idxs.append(bal_datasets[min_class][1][0][0].columns[l-1])\n",
    "            rf_line_min.append(len(np.intersect1d(idxs, RF_feats_real.index[:top10])))\n",
    "\n",
    "        rf_line_uni_min = []\n",
    "        for i in rf_feats[min_class]:\n",
    "            idxs = []\n",
    "            for l in pd.DataFrame(rf_feats[min_class])[i].sort_values(ascending=False)[:top10].index:\n",
    "                idxs.append(bal_datasets[min_class][1][0][0].columns[l-1])\n",
    "            rf_line_uni_min.append(len(np.intersect1d(idxs, uni_results_filt.index)))\n",
    "            \n",
    "        rf_line_unitop_min = []\n",
    "        for i in rf_feats[min_class]:\n",
    "            idxs = []\n",
    "            for l in pd.DataFrame(rf_feats[min_class])[i].sort_values(ascending=False)[:top10].index:\n",
    "                idxs.append(bal_datasets[min_class][1][0][0].columns[l-1])\n",
    "            rf_line_unitop_min.append(len(np.intersect1d(idxs, uni_results_filt.index[:top10])))\n",
    "\n",
    "        axul.plot((np.array(list(rf_feats[min_class].keys()))+n_min_class)/n_max_class*100, np.array(rf_line_min)/top10*100,\n",
    "                 color=label_colors_test[min_class],\n",
    "                 label='Minority Class: ' +min_class)\n",
    "        axuc.plot((np.array(list(rf_feats[min_class].keys()))+n_min_class)/n_max_class*100,\n",
    "                 np.array(rf_line_uni_min)/top10*100,\n",
    "                 color=label_colors_test[min_class],\n",
    "                 label='Minority Class: ' +min_class)\n",
    "        axur.plot((np.array(list(rf_feats[min_class].keys()))+n_min_class)/n_max_class*100,\n",
    "                 np.array(rf_line_unitop_min)/top10*100,\n",
    "                 color=label_colors_test[min_class],\n",
    "                 label='Minority Class: ' +min_class)\n",
    "\n",
    "    axul.plot((np.array(list(rf_feats_together.keys()))+n_min_class)/n_max_class*100,\n",
    "                 np.array(rf_line)/top10*100, color='black',\n",
    "                label='All Together', linewidth=3)\n",
    "    axuc.plot((np.array(list(rf_feats_together.keys()))+n_min_class)/n_max_class*100,\n",
    "             np.array(rf_line_uni)/top10*100, color='black',\n",
    "            label='All Together', linewidth=3)\n",
    "    axur.plot((np.array(list(rf_feats_together.keys()))+n_min_class)/n_max_class*100,\n",
    "             np.array(rf_line_unitop)/top10*100, color='black',\n",
    "            label='All Together', linewidth=3)  \n",
    "\n",
    "    axul.set_title('Against RF Top 2% Imp. Feats', fontsize=15)\n",
    "    axuc.set_title('Against Univariate Significant Feat.', fontsize=15)\n",
    "    axur.set_title('Against Univariate Top 2% Significant Feat.', fontsize=15)\n",
    "    axul.set_ylabel('% of Common Features with RF Real Model', fontsize = 14)\n",
    "    axuc.set_xlabel('% of Samples of the Minority Class in Comparison to the Majority Class', fontsize = 15)\n",
    "    axul.set_ylim(0,102)\n",
    "    axuc.set_ylim(0,102)\n",
    "    axur.set_ylim(0,102)\n",
    "    #plt.legend(loc='upper left', fontsize=13, bbox_to_anchor=(1,1))\n",
    "    axur.legend(loc='upper left', fontsize=15)\n",
    "    axul.tick_params(axis='both', which='major', labelsize=14)\n",
    "    axuc.tick_params(axis='both', which='major', labelsize=14)\n",
    "    axur.tick_params(axis='both', which='major', labelsize=14)\n",
    "    \n",
    "    for min_class in plsda_feats:\n",
    "        plsda_line_min = []\n",
    "        for i in plsda_feats[min_class]:\n",
    "            idxs = []\n",
    "            for l in pd.DataFrame(plsda_feats[min_class])[i].sort_values(ascending=False)[:top10].index:\n",
    "                idxs.append(bal_datasets[min_class][1][0][0].columns[l-1])\n",
    "            plsda_line_min.append(len(np.intersect1d(idxs, PLSDA_feats_real.index[:top10])))\n",
    "\n",
    "        plsda_line_uni_min = []\n",
    "        for i in plsda_feats[min_class]:\n",
    "            idxs = []\n",
    "            for l in pd.DataFrame(plsda_feats[min_class])[i].sort_values(ascending=False)[:top10].index:\n",
    "                idxs.append(bal_datasets[min_class][1][0][0].columns[l-1])\n",
    "            plsda_line_uni_min.append(len(np.intersect1d(idxs, uni_results_filt.index)))\n",
    "            \n",
    "        plsda_line_unitop_min = []\n",
    "        for i in plsda_feats[min_class]:\n",
    "            idxs = []\n",
    "            for l in pd.DataFrame(plsda_feats[min_class])[i].sort_values(ascending=False)[:top10].index:\n",
    "                idxs.append(bal_datasets[min_class][1][0][0].columns[l-1])\n",
    "            plsda_line_unitop_min.append(len(np.intersect1d(idxs, uni_results_filt.index[:top10])))\n",
    "\n",
    "        axdl.plot((np.array(list(plsda_feats[min_class].keys()))+n_min_class)/n_max_class*100,\n",
    "                 np.array(plsda_line_min)/top10*100,\n",
    "                 color=label_colors_test[min_class],\n",
    "                 label='Minority Class: ' +min_class)\n",
    "        axdc.plot((np.array(list(plsda_feats[min_class].keys()))+n_min_class)/n_max_class*100,\n",
    "                 np.array(plsda_line_uni_min)/top10*100,\n",
    "                 color=label_colors_test[min_class],\n",
    "                 label='Minority Class: ' +min_class)\n",
    "        axdr.plot((np.array(list(plsda_feats[min_class].keys()))+n_min_class)/n_max_class*100,\n",
    "                 np.array(plsda_line_unitop_min)/top10*100,\n",
    "                 color=label_colors_test[min_class],\n",
    "                 label='Minority Class: ' +min_class)\n",
    "\n",
    "    axdl.plot((np.array(list(plsda_feats_together.keys()))+n_min_class)/n_max_class*100,\n",
    "                 np.array(plsda_line)/top10*100, color='black',\n",
    "                label='All Together', linewidth=3)\n",
    "    axdc.plot((np.array(list(plsda_feats_together.keys()))+n_min_class)/n_max_class*100,\n",
    "             np.array(plsda_line_uni)/top10*100, color='black',\n",
    "            label='All Together', linewidth=3)\n",
    "    axdr.plot((np.array(list(plsda_feats_together.keys()))+n_min_class)/n_max_class*100,\n",
    "             np.array(plsda_line_unitop)/top10*100, color='black',\n",
    "            label='All Together', linewidth=3)         \n",
    "\n",
    "    axdl.set_title('Against PLS-DA Top 2% Imp. Feats', fontsize=15)\n",
    "    axdc.set_title('Against Univariate Significant Feat.', fontsize=15)\n",
    "    axdr.set_title('Against Univariate Top 2% Significant Feat.', fontsize=15)\n",
    "    axdl.set_ylabel('% of Common Features with Real PLS-DA Model', fontsize = 14)\n",
    "    axdc.set_xlabel('% of Samples of the Minority Class in Comparison to the Majority Class', fontsize = 15)\n",
    "    axdl.set_ylim(0,102)\n",
    "    axdc.set_ylim(0,102)\n",
    "    axdr.set_ylim(0,102)\n",
    "    #plt.legend(loc='upper left', fontsize=13, bbox_to_anchor=(1,1))\n",
    "    axdl.tick_params(axis='both', which='major', labelsize=14)\n",
    "    axdc.tick_params(axis='both', which='major', labelsize=14)\n",
    "    axdr.tick_params(axis='both', which='major', labelsize=14)\n",
    "    #plt.suptitle('PCD', fontsize=18)\n",
    "    f.savefig('images/PCDint_Imbalanced_ImpFeatCommon_plot.png', dpi=400)\n",
    "    f.savefig('images/PCDint_Imbalanced_ImpFeatCommon_plot.pdf', dpi=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d98884",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the results and adjusting parameters of the plot\n",
    "with plt.style.context('seaborn-whitegrid'):\n",
    "    f, (axul, axur) = plt.subplots(1,2,figsize=(8,3), constrained_layout=True)\n",
    "    #plt.subplots_adjust(wspace=0.4)\n",
    "    \n",
    "    for min_class in rf_feats:\n",
    "        rf_line_min = []\n",
    "        for i in rf_feats[min_class]:\n",
    "            idxs = []\n",
    "            for l in pd.DataFrame(rf_feats[min_class])[i].sort_values(ascending=False)[:top10].index:\n",
    "                idxs.append(bal_datasets[min_class][1][0][0].columns[l-1])\n",
    "            rf_line_min.append(len(np.intersect1d(idxs, RF_feats_real.index[:top10])))\n",
    "\n",
    "        rf_line_uni_min = []\n",
    "        for i in rf_feats[min_class]:\n",
    "            idxs = []\n",
    "            for l in pd.DataFrame(rf_feats[min_class])[i].sort_values(ascending=False)[:top10].index:\n",
    "                idxs.append(bal_datasets[min_class][1][0][0].columns[l-1])\n",
    "            rf_line_uni_min.append(len(np.intersect1d(idxs, uni_results_filt.index)))\n",
    "\n",
    "        axul.plot((np.array(list(rf_feats[min_class].keys()))+n_min_class)/n_max_class*100, np.array(rf_line_min)/top10*100,\n",
    "                 color=label_colors_test[min_class],\n",
    "                 label='Minority Class: ' +min_class)\n",
    "        axur.plot((np.array(list(rf_feats[min_class].keys()))+n_min_class)/n_max_class*100,\n",
    "                 np.array(rf_line_uni_min)/top10*100,\n",
    "                 color=label_colors_test[min_class],\n",
    "                 label='Minority Class: ' +min_class)\n",
    "\n",
    "    axul.plot((np.array(list(rf_feats_together.keys()))+n_min_class)/n_max_class*100,\n",
    "                 np.array(rf_line)/top10*100, color='black',\n",
    "                label='All Together', linewidth=3)\n",
    "    axur.plot((np.array(list(rf_feats_together.keys()))+n_min_class)/n_max_class*100,\n",
    "             np.array(rf_line_uni)/top10*100, color='black',\n",
    "            label='All Together', linewidth=3)\n",
    "\n",
    "    axul.set_title('RF Vs RF Complete Model', fontsize=13)\n",
    "    axur.set_title('RF Vs Ref. Significant Features', fontsize=13)\n",
    "    axul.set_ylabel('% of Common Features', fontsize = 13)\n",
    "    f.supxlabel('Class Balance (%)', fontsize = 13)\n",
    "    axul.set_ylim(0,105)\n",
    "    axur.set_ylim(0,105)\n",
    "    #plt.legend(loc='upper left', fontsize=13, bbox_to_anchor=(1,1))\n",
    "    axul.legend(loc='upper left', fontsize=12)\n",
    "    axul.tick_params(axis='both', which='major', labelsize=11)\n",
    "    #axuc.tick_params(axis='both', which='major', labelsize=14)\n",
    "    axur.tick_params(axis='both', which='major', labelsize=11)\n",
    "    \n",
    "    f.savefig('images/PCDint_Imbalanced_RF_ImpFeatCommon_plot.png', dpi=400)\n",
    "    f.savefig('images/PCDint_Imbalanced_RF_ImpFeatCommon_plot.pdf', dpi=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfbae0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the results and adjusting parameters of the plot\n",
    "with plt.style.context('seaborn-whitegrid'):\n",
    "    f, (axdl, axdr) = plt.subplots(1,2,figsize=(8,3), constrained_layout=True)\n",
    "    \n",
    "    for min_class in plsda_feats:\n",
    "        plsda_line_min = []\n",
    "        for i in plsda_feats[min_class]:\n",
    "            idxs = []\n",
    "            for l in pd.DataFrame(plsda_feats[min_class])[i].sort_values(ascending=False)[:top10].index:\n",
    "                idxs.append(bal_datasets[min_class][1][0][0].columns[l-1])\n",
    "            plsda_line_min.append(len(np.intersect1d(idxs, PLSDA_feats_real.index[:top10])))\n",
    "\n",
    "        plsda_line_uni_min = []\n",
    "        for i in plsda_feats[min_class]:\n",
    "            idxs = []\n",
    "            for l in pd.DataFrame(plsda_feats[min_class])[i].sort_values(ascending=False)[:top10].index:\n",
    "                idxs.append(bal_datasets[min_class][1][0][0].columns[l-1])\n",
    "            plsda_line_uni_min.append(len(np.intersect1d(idxs, uni_results_filt.index)))\n",
    "\n",
    "        axdl.plot((np.array(list(plsda_feats[min_class].keys()))+n_min_class)/n_max_class*100,\n",
    "                 np.array(plsda_line_min)/top10*100,\n",
    "                 color=label_colors_test[min_class],\n",
    "                 label='Minority Class: ' +min_class)\n",
    "        axdr.plot((np.array(list(plsda_feats[min_class].keys()))+n_min_class)/n_max_class*100,\n",
    "                 np.array(plsda_line_uni_min)/top10*100,\n",
    "                 color=label_colors_test[min_class],\n",
    "                 label='Minority Class: ' +min_class)\n",
    "\n",
    "    axdl.plot((np.array(list(plsda_feats_together.keys()))+n_min_class)/n_max_class*100,\n",
    "                 np.array(plsda_line)/top10*100, color='black',\n",
    "                label='All Together', linewidth=3)\n",
    "    axdr.plot((np.array(list(plsda_feats_together.keys()))+n_min_class)/n_max_class*100,\n",
    "             np.array(plsda_line_uni)/top10*100, color='black',\n",
    "            label='All Together', linewidth=3)       \n",
    "\n",
    "    axdl.set_title('PLS-DA Vs PLS-DA Complete Model', fontsize=13)\n",
    "    axdr.set_title('PLS-DA Vs Ref. Significant Features', fontsize=13)\n",
    "    axdl.set_ylabel('% of Common Features', fontsize = 13)\n",
    "    f.supxlabel('Class Balance (%)', fontsize = 13)\n",
    "    axdl.set_ylim(0,105)\n",
    "    axdr.set_ylim(0,105)\n",
    "    axdl.tick_params(axis='both', which='major', labelsize=11)\n",
    "    axdr.tick_params(axis='both', which='major', labelsize=11)\n",
    "\n",
    "    f.savefig('images/PCDint_Imbalanced_PLSDA_ImpFeatCommon_plot.png', dpi=400)\n",
    "    f.savefig('images/PCDint_Imbalanced_PLSDA_ImpFeatCommon_plot.pdf', dpi=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ff3156",
   "metadata": {},
   "source": [
    "### Comparing Imbalanced Datasets, Imb + Min. Cl. GAN Samples and only GAN samples\n",
    "\n",
    "Creating the models and evaluating them for GAN data and compiling results for the imbalanced datasets (with no augmentation) and the imbalanced datasets made balanced with minority class GAN samples.\n",
    "\n",
    "##### RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4426b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting and storing Random Forest models for each fold\n",
    "RF_models_GAN = {}\n",
    "RF_models_real = {}\n",
    "RF_models_GAN_bal = {}\n",
    "\n",
    "# Train the Models\n",
    "for i in generated_samples:\n",
    "    RF_models_GAN[i] = {}\n",
    "    RF_models_real[i] = {}\n",
    "    RF_models_GAN_bal[i] = {}\n",
    "    for fold in generated_samples[i]:\n",
    "        rf_mod = ma.RF_model(generated_samples[i][fold][0], generated_samples[i][fold][1], return_cv=False, n_trees=200)\n",
    "        RF_models_GAN[i][fold] = rf_mod\n",
    "\n",
    "        RF_models_real[i][fold] = RF_models_bal[i][0][fold]\n",
    "\n",
    "        RF_models_GAN_bal[i][fold] = RF_models_bal[i][n_max_class-n_min_class][fold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952fdfb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the RF models with the test data for each fold for real, GAN and CorrGAN Data\n",
    "RF_results_GAN = {'Accuracy':{'Recurrence':{}, 'No Recurrence':{}}, 'F1-Score':{'Recurrence':{}, 'No Recurrence':{}},\n",
    "                  'Precision':{'Recurrence':{}, 'No Recurrence':{}}, 'Recall':{'Recurrence':{}, 'No Recurrence':{}}}\n",
    "RF_results_real = {'Accuracy':{'Recurrence':{}, 'No Recurrence':{}}, 'F1-Score':{'Recurrence':{}, 'No Recurrence':{}},\n",
    "                   'Precision':{'Recurrence':{}, 'No Recurrence':{}}, 'Recall':{'Recurrence':{}, 'No Recurrence':{}}}\n",
    "RF_results_GAN_bal = {'Accuracy':{'Recurrence':{}, 'No Recurrence':{}}, 'F1-Score':{'Recurrence':{}, 'No Recurrence':{}},\n",
    "                      'Precision':{'Recurrence':{}, 'No Recurrence':{}}, 'Recall':{'Recurrence':{}, 'No Recurrence':{}}}\n",
    "\n",
    "for min_class in RF_models_bal:\n",
    "    for fold in generated_samples[min_class]:\n",
    "        RF_results_GAN['Accuracy'][min_class][fold] = RF_models_GAN[min_class][fold].score(\n",
    "                                                                                df_storage_test[min_class][fold],\n",
    "                                                                                lbl_storage_test[min_class][fold])\n",
    "        preds = RF_models_GAN[min_class][fold].predict(df_storage_test[min_class][fold])\n",
    "        prec, rec, f1, sup = precision_recall_fscore_support(lbl_storage_test[min_class][fold], preds,\n",
    "                                                            pos_label=min_class, average='binary')\n",
    "        RF_results_GAN['F1-Score'][min_class][fold] = f1\n",
    "        RF_results_GAN['Precision'][min_class][fold] = prec\n",
    "        RF_results_GAN['Recall'][min_class][fold] = rec\n",
    "\n",
    "        RF_results_real['Accuracy'][min_class][fold] = RF_results_bal['Accuracy'][min_class][0][fold]\n",
    "        RF_results_real['F1-Score'][min_class][fold] = RF_results_bal['F1-Score'][min_class][0][fold]\n",
    "        RF_results_real['Precision'][min_class][fold] = RF_results_bal['Precision'][min_class][0][fold]\n",
    "        RF_results_real['Recall'][min_class][fold] = RF_results_bal['Recall'][min_class][0][fold]\n",
    "        \n",
    "        RF_results_GAN_bal['Accuracy'][min_class][fold] = RF_results_bal[\n",
    "            'Accuracy'][min_class][n_max_class-n_min_class][fold]\n",
    "        RF_results_GAN_bal['F1-Score'][min_class][fold] = RF_results_bal[\n",
    "            'F1-Score'][min_class][n_max_class-n_min_class][fold]\n",
    "        RF_results_GAN_bal['Precision'][min_class][fold] = RF_results_bal[\n",
    "            'Precision'][min_class][n_max_class-n_min_class][fold]\n",
    "        RF_results_GAN_bal['Recall'][min_class][fold] = RF_results_bal['Recall'][min_class][n_max_class-n_min_class][fold]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59a5bb9",
   "metadata": {},
   "source": [
    "##### PLS-DA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804555d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "PLSDA_models_GAN = {}\n",
    "PLSDA_results_GAN = {'Accuracy':{'Recurrence':{}, 'No Recurrence':{}}, 'F1-Score':{'Recurrence':{}, 'No Recurrence':{}},\n",
    "                  'Precision':{'Recurrence':{}, 'No Recurrence':{}}, 'Recall':{'Recurrence':{}, 'No Recurrence':{}}}\n",
    "\n",
    "PLSDA_models_real = {}\n",
    "PLSDA_results_real = {'Accuracy':{'Recurrence':{}, 'No Recurrence':{}}, 'F1-Score':{'Recurrence':{}, 'No Recurrence':{}},\n",
    "                  'Precision':{'Recurrence':{}, 'No Recurrence':{}}, 'Recall':{'Recurrence':{}, 'No Recurrence':{}}}\n",
    "\n",
    "PLSDA_models_GAN_bal = {}\n",
    "PLSDA_results_GAN_bal = {'Accuracy':{'Recurrence':{}, 'No Recurrence':{}}, 'F1-Score':{'Recurrence':{}, 'No Recurrence':{}},\n",
    "                  'Precision':{'Recurrence':{}, 'No Recurrence':{}}, 'Recall':{'Recurrence':{}, 'No Recurrence':{}}}\n",
    "\n",
    "# Train the Models\n",
    "for min_class in bal_datasets:\n",
    "    PLSDA_models_GAN[min_class] = {}\n",
    "    PLSDA_models_real[min_class] = {}\n",
    "    PLSDA_models_GAN_bal[min_class] = {}\n",
    "\n",
    "    for fold in bal_datasets[min_class]:\n",
    "\n",
    "        PLSDA_models_GAN[min_class][fold] = ma.fit_PLSDA_model(generated_samples[min_class][fold][0],\n",
    "                                                               generated_samples[min_class][fold][1],\n",
    "                                                          n_comp=9,\n",
    "                                                  return_scores=False, scale=False, encode2as1vector=True)\n",
    "        plsda = PLSDA_models_GAN[min_class][fold]\n",
    "        # Obtain results with the test group\n",
    "        y_pred = plsda.predict(df_storage_test[min_class][fold])\n",
    "        y_true = ma._generate_y_PLSDA(lbl_storage_test[min_class][fold],\n",
    "                                      pd.unique(generated_samples[min_class][fold][1]),\n",
    "                                      True)\n",
    "        pos_label = np.where(pd.unique(generated_samples[min_class][fold][1]) != min_class)[0][0]\n",
    "        # Calculate accuracy\n",
    "        accuracy, f1, prec, rec = decision_rule(y_pred, y_true, pos_label=pos_label, average='binary')\n",
    "        PLSDA_results_GAN['Accuracy'][min_class][fold] = accuracy\n",
    "        PLSDA_results_GAN['F1-Score'][min_class][fold] = f1\n",
    "        PLSDA_results_GAN['Precision'][min_class][fold] = prec\n",
    "        PLSDA_results_GAN['Recall'][min_class][fold] = rec\n",
    "        \n",
    "        PLSDA_models_real[min_class][fold] = PLSDA_models_bal[min_class][0][fold]\n",
    "        \n",
    "        PLSDA_results_real['Accuracy'][min_class][fold] = PLSDA_results_bal['Accuracy'][min_class][0][fold]\n",
    "        PLSDA_results_real['F1-Score'][min_class][fold] = PLSDA_results_bal['F1-Score'][min_class][0][fold]\n",
    "        PLSDA_results_real['Precision'][min_class][fold] = PLSDA_results_bal['Precision'][min_class][0][fold]\n",
    "        PLSDA_results_real['Recall'][min_class][fold] = PLSDA_results_bal['Recall'][min_class][0][fold]\n",
    "        \n",
    "        PLSDA_models_GAN_bal[min_class][fold] = PLSDA_models_bal[min_class][n_max_class-n_min_class][fold]\n",
    "        \n",
    "        PLSDA_results_GAN_bal['Accuracy'][min_class][fold] = PLSDA_results_bal[\n",
    "            'Accuracy'][min_class][n_max_class-n_min_class][fold]\n",
    "        PLSDA_results_GAN_bal['F1-Score'][min_class][fold] = PLSDA_results_bal[\n",
    "            'F1-Score'][min_class][n_max_class-n_min_class][fold]\n",
    "        PLSDA_results_GAN_bal['Precision'][min_class][fold] = PLSDA_results_bal[\n",
    "            'Precision'][min_class][n_max_class-n_min_class][fold]\n",
    "        PLSDA_results_GAN_bal['Recall'][min_class][fold] = PLSDA_results_bal[\n",
    "            'Recall'][min_class][n_max_class-n_min_class][fold]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f990fc79",
   "metadata": {},
   "source": [
    "##### Summarising Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d75ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results\n",
    "with sns.axes_style(\"whitegrid\"):\n",
    "    with sns.plotting_context(\"notebook\", font_scale=1.5):\n",
    "        f, axs = plt.subplots(1, 2, figsize=(4, 2.5), constrained_layout=True)\n",
    "        for metric, axu in zip(['F1-Score', 'Recall'], axs.ravel()):\n",
    "            x = np.arange(2)  # the label locations\n",
    "            l = ['RF', 'PLS-DA']\n",
    "            width = 0.23  # the width of the bars\n",
    "\n",
    "            offset = - 0.25 + 0 * 0.25\n",
    "            accuracy_stats = pd.DataFrame({metric: [pd.DataFrame(RF_results_real[metric]).values.mean(), \n",
    "                                                                pd.DataFrame(PLSDA_results_real[metric]).values.mean()],\n",
    "                                         'STD': [pd.DataFrame(RF_results_real[metric]).values.std(), \n",
    "                                                 pd.DataFrame(PLSDA_results_real[metric]).values.std()]})\n",
    "            rects = axu.bar(x + offset, accuracy_stats[metric], width, label='Exp. Imb. Data', color='green')\n",
    "            axu.errorbar(x + offset, y=accuracy_stats[metric], yerr=accuracy_stats['STD'],\n",
    "                                ls='none', ecolor='0.2', capsize=3)\n",
    "\n",
    "            offset = - 0.25 + 1 * 0.25\n",
    "            accuracy_stats = pd.DataFrame({metric: [pd.DataFrame(RF_results_GAN_bal[metric]).values.mean(), \n",
    "                                                            pd.DataFrame(PLSDA_results_GAN_bal[metric]).values.mean()],\n",
    "                                         'STD': [pd.DataFrame(RF_results_GAN_bal[metric]).values.std(), \n",
    "                                                 pd.DataFrame(PLSDA_results_GAN_bal[metric]).values.std()]})\n",
    "            rects = axu.bar(x + offset, accuracy_stats[metric],\n",
    "                            width, label='Imb. + GAN Data', color='red')\n",
    "            axu.errorbar(x + offset, y=accuracy_stats[metric], yerr=accuracy_stats['STD'],\n",
    "                                ls='none', ecolor='0.2', capsize=3)\n",
    "\n",
    "            offset = - 0.25 + 2 * 0.25\n",
    "            accuracy_stats = pd.DataFrame({metric: [pd.DataFrame(RF_results_GAN[metric]).values.mean(), \n",
    "                                                                pd.DataFrame(PLSDA_results_GAN[metric]).values.mean()],\n",
    "                                         'STD': [pd.DataFrame(RF_results_GAN[metric]).values.std(), \n",
    "                                                 pd.DataFrame(PLSDA_results_GAN[metric]).values.std()]})\n",
    "            rects = axu.bar(x + offset, accuracy_stats[metric], width, label='GAN Data', color='blue')\n",
    "            axu.errorbar(x + offset, y=accuracy_stats[metric], yerr=accuracy_stats['STD'],\n",
    "                                ls='none', ecolor='0.2', capsize=3)\n",
    "\n",
    "            for spine in axu.spines.values():\n",
    "                spine.set_edgecolor('0.1')\n",
    "            axu.set_xticks(x)\n",
    "            axu.set_xticklabels(l, fontsize=13)\n",
    "            axu.set_title(metric, fontsize=13)\n",
    "        axs[0].set(ylabel='Performance', ylim=(0,1.05))\n",
    "        axs[0].set_ylabel('Performance', fontsize=13)\n",
    "        axs[1].tick_params(labelleft=False)\n",
    "        axs[0].tick_params(axis='y', which='major', labelsize=11)\n",
    "        #axs[1].set_yticks([])\n",
    "        #axs[0].legend(loc='center', fontsize=11, bbox_to_anchor=(1.1,1.25), ncol=2)\n",
    "        lgd = f.legend(*axs[0].get_legend_handles_labels(), loc='center', fontsize=11.2, bbox_to_anchor=(0.572,1.12),\n",
    "                       ncol=2)\n",
    "\n",
    "        plt.show()\n",
    "        f.savefig('Images/PCDint_Imbalanced_Accuracy_plot.png', dpi=400, bbox_extra_artists=(lgd,), bbox_inches='tight')\n",
    "        f.savefig('images/PCDint_Imbalanced_Accuracy_plot.pdf', dpi=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292c8f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Results_abridged = {'RF':{}, 'PLS-DA':{}}\n",
    "Results_abridged_std = {'RF':{}, 'PLS-DA':{}}\n",
    "for i in RF_results_GAN:\n",
    "    Results_abridged['RF'][i] = {'Real Imbalanced': pd.DataFrame(RF_results_real[i]).values.mean(),\n",
    "                                 'GAN Augmented': pd.DataFrame(RF_results_GAN_bal[i]).values.mean(),\n",
    "                                 'GAN': pd.DataFrame(RF_results_GAN[i]).values.mean()}\n",
    "    Results_abridged['PLS-DA'][i] = {'Real Imbalanced': pd.DataFrame(PLSDA_results_real[i]).values.mean(),\n",
    "                                     'GAN Augmented': pd.DataFrame(PLSDA_results_GAN_bal[i]).values.mean(),\n",
    "                                     'GAN': pd.DataFrame(PLSDA_results_GAN[i]).values.mean()}\n",
    "    \n",
    "    Results_abridged_std['RF'][i] = {'Real Imbalanced': pd.DataFrame(RF_results_real[i]).values.std(),\n",
    "                                 'GAN Augmented': pd.DataFrame(RF_results_GAN_bal[i]).values.std(),\n",
    "                                 'GAN': pd.DataFrame(RF_results_GAN[i]).values.std()}\n",
    "    Results_abridged_std['PLS-DA'][i] = {'Real Imbalanced': pd.DataFrame(PLSDA_results_real[i]).values.std(),\n",
    "                                     'GAN Augmented': pd.DataFrame(PLSDA_results_GAN_bal[i]).values.std(),\n",
    "                                     'GAN': pd.DataFrame(PLSDA_results_GAN[i]).values.std()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9fb80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(Results_abridged['RF'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0fecf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(Results_abridged_std['RF'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc07de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(Results_abridged['PLS-DA'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03367174",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(Results_abridged_std['PLS-DA'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29e6112",
   "metadata": {},
   "source": [
    "### Comparing Important features of models built from Imbalanced Datasets, Imb + Min. Cl. GAN Samples and only GAN samples\n",
    "\n",
    "#### Comparing Against Important Features of the Complete models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c619c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 2nd complete model but with only 1 iteration to compare the important features\n",
    "RF_model_real1 = ma.RF_model_CV(hd_datamatrix_treated, hd_labels, iter_num=1, n_fold=n_fold, n_trees=200) \n",
    "RF_feats_real1 = pd.DataFrame(RF_model_real1['important_features']).set_index(0).sort_values(by=1, ascending=False)\n",
    "RF_feats_real1.index = [hd_datamatrix_treated.columns[i] for i in RF_feats_real1.index]\n",
    "RF_feats_real1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1486dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_feats_GAN = {'Recurrence':{}, 'No Recurrence':{}}\n",
    "RF_feats_imb = {'Recurrence':{}, 'No Recurrence':{}}\n",
    "RF_feats_GAN_bal = {'Recurrence':{}, 'No Recurrence':{}}\n",
    "\n",
    "for i in RF_models_real.keys():\n",
    "    for fold in RF_models_real[i]:\n",
    "        temp_df = pd.DataFrame(zip(range(len(generated_samples[i][fold][0].columns)),\n",
    "                                   RF_models_real[i][fold].feature_importances_))\n",
    "        temp_df = temp_df.set_index(0).sort_values(by=1, ascending=False)\n",
    "        temp_df.index = [generated_samples[i][fold][0].columns[a] for a in temp_df.index]\n",
    "        RF_feats_imb[i][fold] = temp_df.copy()\n",
    "\n",
    "        temp_df = pd.DataFrame(zip(range(len(generated_samples[i][fold][0].columns)),\n",
    "                                   RF_models_GAN[i][fold].feature_importances_))\n",
    "        temp_df = temp_df.set_index(0).sort_values(by=1, ascending=False)\n",
    "        temp_df.index = [generated_samples[i][fold][0].columns[a] for a in temp_df.index]\n",
    "        RF_feats_GAN[i][fold] = temp_df.copy()\n",
    "\n",
    "        temp_df = pd.DataFrame(zip(range(len(generated_samples[i][fold][0].columns)),\n",
    "                                   RF_models_GAN_bal[i][fold].feature_importances_))\n",
    "        temp_df = temp_df.set_index(0).sort_values(by=1, ascending=False)\n",
    "        temp_df.index = [generated_samples[i][fold][0].columns[a] for a in temp_df.index]\n",
    "        RF_feats_GAN_bal[i][fold] = temp_df.copy()\n",
    "    \n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49b4b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_feats_GAN_mean = {'Recurrence':[], 'No Recurrence':[]}\n",
    "RF_feats_imb_mean = {'Recurrence':[], 'No Recurrence':[]}\n",
    "RF_feats_GAN_bal_mean = {'Recurrence':[], 'No Recurrence':[]}\n",
    "\n",
    "for i in RF_models_real.keys():\n",
    "    for fold in RF_models_real[i]:\n",
    "        if fold == 1:\n",
    "            temp_df_imb = RF_models_real[i][fold].feature_importances_\n",
    "            temp_df_bal = RF_models_GAN_bal[i][fold].feature_importances_\n",
    "            temp_df_GAN = RF_models_GAN[i][fold].feature_importances_\n",
    "        else:\n",
    "            temp_df_imb = temp_df_imb + RF_models_real[i][fold].feature_importances_\n",
    "            temp_df_bal = temp_df_bal + RF_models_GAN_bal[i][fold].feature_importances_\n",
    "            temp_df_GAN = temp_df_GAN + RF_models_GAN[i][fold].feature_importances_\n",
    "\n",
    "    temp_df_imb = temp_df_imb / len(RF_models_real[i])\n",
    "    temp_df_imb = pd.DataFrame(zip(range(len(generated_samples[i][fold][0].columns)), temp_df_imb))\n",
    "    temp_df_imb = temp_df_imb.set_index(0).sort_values(by=1, ascending=False)\n",
    "    temp_df_imb.index = [generated_samples[i][fold][0].columns[a] for a in temp_df_imb.index]\n",
    "    RF_feats_imb_mean[i] = temp_df_imb.copy()\n",
    "\n",
    "    temp_df_bal = temp_df_bal / len(RF_models_GAN_bal[i])\n",
    "    temp_df_bal = pd.DataFrame(zip(range(len(generated_samples[i][fold][0].columns)), temp_df_bal))\n",
    "    temp_df_bal = temp_df_bal.set_index(0).sort_values(by=1, ascending=False)\n",
    "    temp_df_bal.index = [generated_samples[i][fold][0].columns[a] for a in temp_df_bal.index]\n",
    "    RF_feats_GAN_bal_mean[i] = temp_df_bal.copy()\n",
    "\n",
    "    temp_df_GAN = temp_df_GAN / len(RF_models_GAN[i])\n",
    "    temp_df_GAN = pd.DataFrame(zip(range(len(generated_samples[i][fold][0].columns)), temp_df_GAN))\n",
    "    temp_df_GAN = temp_df_GAN.set_index(0).sort_values(by=1, ascending=False)\n",
    "    temp_df_GAN.index = [generated_samples[i][fold][0].columns[a] for a in temp_df_GAN.index]\n",
    "    RF_feats_GAN_mean[i] = temp_df_GAN.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd734b8",
   "metadata": {},
   "source": [
    "Calculate intersection of important features from top 1 to top (number of features in the dataset) between the complete dataset (averaged over 20 iterations) and a iteration of the complete dataset, the imbalanced dataset, the balanced dataset and the GAN dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3dce16",
   "metadata": {},
   "outputs": [],
   "source": [
    "intersections_RF = []\n",
    "for i in range(1, len(RF_feats_real1)):\n",
    "    intersections_RF.append(len(np.intersect1d(RF_feats_real1.index[:i], RF_feats_real.index[:i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac96ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "intersections_RF_bal = {}\n",
    "for cl in RF_feats_GAN_bal_mean:\n",
    "    int_bal = []\n",
    "    for i in range(1, len(RF_feats_real)):\n",
    "        int_bal.append(len(np.intersect1d(RF_feats_real.index[:i], RF_feats_GAN_bal_mean[cl].index[:i])))\n",
    "    intersections_RF_bal[cl] = int_bal\n",
    "\n",
    "print('Intersections - Dataset Balanced - Finished')\n",
    "\n",
    "intersections_RF_GAN = {}\n",
    "for cl in RF_feats_GAN_mean:\n",
    "    int_GAN = []\n",
    "    for i in range(1, len(RF_feats_real)):\n",
    "        int_GAN.append(len(np.intersect1d(RF_feats_real.index[:i], RF_feats_GAN_mean[cl].index[:i])))\n",
    "    intersections_RF_GAN[cl] = int_GAN\n",
    "\n",
    "print('Intersections - Dataset GAN - Finished')\n",
    "\n",
    "intersections_RF_imb = {}\n",
    "for cl in RF_feats_imb_mean:\n",
    "    int_imb = []\n",
    "    for i in range(1, len(RF_feats_real)):\n",
    "        int_imb.append(len(np.intersect1d(RF_feats_real.index[:i], RF_feats_imb_mean[cl].index[:i])))\n",
    "    intersections_RF_imb[cl] = int_imb\n",
    "\n",
    "print('Intersections - Dataset Imbalanced - Finished')\n",
    "\n",
    "# See intersections if features were randomly shuffled\n",
    "random_intersections_RF = []\n",
    "copy_shuffle = list(RF_feats_real.index).copy()\n",
    "np.random.shuffle(copy_shuffle)\n",
    "for i in range(1, len(RF_feats_real)):\n",
    "    random_intersections_RF.append(len(np.intersect1d(RF_feats_real.index[:i], copy_shuffle[:i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f59237",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, (axl, axr) = plt.subplots(1,2,figsize=(12,6))\n",
    "# Graph depicting intersection of important features\n",
    "axl.scatter(range(1,len(intersections_RF)+1), np.array(intersections_RF) / np.array(range(1,len(intersections_RF)+1)), \n",
    "            label = 'Reference (expected var.)', color='Black', s=5)\n",
    "axr.scatter(range(1,len(intersections_RF)+1), np.array(intersections_RF) / np.array(range(1,len(intersections_RF)+1)),\n",
    "            label = 'Reference (expected var.)', color='Black', s=5)\n",
    "\n",
    "for i,ax in zip(intersections_RF_imb, (axl,axr)):\n",
    "    ax.scatter(range(1,len(intersections_RF)+1),\n",
    "               np.array(intersections_RF_imb[i]) / np.array(range(1,len(intersections_RF)+1)),\n",
    "                label = 'Imbalanced Exp. Dataset', color='Green', s=5)\n",
    "    ax.scatter(range(1,len(intersections_RF)+1),\n",
    "               np.array(intersections_RF_bal[i]) / np.array(range(1,len(intersections_RF)+1)),\n",
    "                label = 'GAN Augmented Exp. Dataset', color='Blue', s=5)\n",
    "    ax.scatter(range(1,len(intersections_RF)+1),\n",
    "               np.array(intersections_RF_GAN[i]) / np.array(range(1,len(intersections_RF)+1)),\n",
    "                label = 'GAN Samples Dataset', color='Red', s=5)\n",
    "\n",
    "    ax.scatter(range(1,len(intersections_RF)+1),\n",
    "               np.array(random_intersections_RF) / np.array(range(1,len(intersections_RF)+1)),\n",
    "            label = 'Random', color='Orange', s=5)\n",
    "    \n",
    "    ax.set_title('Minority Class: '+i, fontsize=15)\n",
    "\n",
    "#axl.legend(loc='center left', fontsize=11, bbox_to_anchor=(-0.2,-0.15), ncol=5)\n",
    "axl.set_ylabel('Fraction of Common Compounds', fontsize=15)\n",
    "axl.set_xlim([0,len(intersections_RF)//8])\n",
    "axl.set_ylim([0,1.01])\n",
    "\n",
    "f.supxlabel('Nº of Top Important (Gini Importance) Compounds', fontsize=15)\n",
    "\n",
    "axr.set_xlim([0,len(intersections_RF)//8])\n",
    "axr.set_ylim([0,1.01])\n",
    "axr.legend(loc='upper left', fontsize=11, bbox_to_anchor=(1,1), ncol=1, markerscale=3)\n",
    "plt.suptitle('Random Forest', fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39a5ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axl = plt.subplots(1,1,figsize=(8,4))\n",
    "# Graph depicting intersection of important features\n",
    "axl.scatter(range(1,len(intersections_RF)+1), np.array(intersections_RF) / np.array(range(1,len(intersections_RF)+1)), \n",
    "            label = 'Reference (expected var.)', color='Black', s=5)\n",
    "\n",
    "axl.scatter(range(1,len(intersections_RF)+1),\n",
    "            np.array(pd.DataFrame(intersections_RF_imb).mean(axis=1)) / np.array(range(1,len(intersections_RF)+1)),\n",
    "            label = 'Imbalanced Exp. Dataset', color='Green', s=5)\n",
    "axl.scatter(range(1,len(intersections_RF)+1),\n",
    "            np.array(pd.DataFrame(intersections_RF_bal).mean(axis=1)) / np.array(range(1,len(intersections_RF)+1)),\n",
    "            label = 'GAN Augmented Exp. Dataset', color='Red', s=5)\n",
    "axl.scatter(range(1,len(intersections_RF)+1),\n",
    "            np.array(pd.DataFrame(intersections_RF_GAN).mean(axis=1)) / np.array(range(1,len(intersections_RF)+1)),\n",
    "            label = 'GAN Samples Dataset', color='Blue', s=5)\n",
    "\n",
    "axl.scatter(range(1,len(intersections_RF)+1),\n",
    "            np.array(random_intersections_RF) / np.array(range(1,len(intersections_RF)+1)),\n",
    "            label = 'Random', color='Orange', s=5)\n",
    "\n",
    "axl.legend(loc='upper left', fontsize=11, ncol=1, bbox_to_anchor=(1,1), markerscale=3)\n",
    "axl.set_xlabel('Nº of Top Important (Gini Importance) Compounds', fontsize=14)\n",
    "axl.set_ylabel('Fraction of Common Compounds', fontsize=14)\n",
    "axl.set_xlim([0,len(intersections_RF)//8])\n",
    "axl.set_ylim([0,1.01])\n",
    "axl.set_title('Random Forest', fontsize=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea284a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 2nd complete model but with only 1 iteration to compare the important features\n",
    "np.random.seed(485)\n",
    "n_fold = 5\n",
    "PLSDA_model_real1 = ma.PLSDA_model_CV(hd_datamatrix_treated, hd_labels, n_comp=9, iter_num=1, n_fold=n_fold,\n",
    "                                      feat_type='VIP')\n",
    "PLSDA_feats_real1 = pd.DataFrame(PLSDA_model_real1['important_features']).set_index(0).sort_values(by=1, ascending=False)\n",
    "PLSDA_feats_real1.index = [hd_datamatrix_treated.columns[i] for i in PLSDA_feats_real1.index]\n",
    "PLSDA_feats_real1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb917c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "PLSDA_feats_GAN = {'Recurrence':{}, 'No Recurrence':{}}\n",
    "PLSDA_feats_imb = {'Recurrence':{}, 'No Recurrence':{}}\n",
    "PLSDA_feats_GAN_bal = {'Recurrence':{}, 'No Recurrence':{}}\n",
    "\n",
    "for i in PLSDA_models_real.keys():\n",
    "    for fold in PLSDA_models_real[i]:\n",
    "        vips = ma._calculate_vips(PLSDA_models_real[i][fold])\n",
    "        temp_df = pd.DataFrame(zip(range(len(generated_samples[i][fold][0].columns)), vips))\n",
    "        temp_df = temp_df.set_index(0).sort_values(by=1, ascending=False)\n",
    "        temp_df.index = [generated_samples[i][fold][0].columns[a] for a in temp_df.index]\n",
    "        PLSDA_feats_imb[i][fold] = temp_df.copy()\n",
    "\n",
    "        vips = ma._calculate_vips(PLSDA_models_GAN[i][fold])\n",
    "        temp_df = pd.DataFrame(zip(range(len(generated_samples[i][fold][0].columns)), vips))\n",
    "        temp_df = temp_df.set_index(0).sort_values(by=1, ascending=False)\n",
    "        temp_df.index = [generated_samples[i][fold][0].columns[a] for a in temp_df.index]\n",
    "        PLSDA_feats_GAN[i][fold] = temp_df.copy()\n",
    "\n",
    "        vips = ma._calculate_vips(PLSDA_models_GAN_bal[i][fold])\n",
    "        temp_df = pd.DataFrame(zip(range(len(generated_samples[i][fold][0].columns)), vips))\n",
    "        temp_df = temp_df.set_index(0).sort_values(by=1, ascending=False)\n",
    "        temp_df.index = [generated_samples[i][fold][0].columns[a] for a in temp_df.index]\n",
    "        PLSDA_feats_GAN_bal[i][fold] = temp_df.copy()\n",
    "    \n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8753531d",
   "metadata": {},
   "outputs": [],
   "source": [
    "PLSDA_feats_GAN_mean = {'Recurrence':[], 'No Recurrence':[]}\n",
    "PLSDA_feats_imb_mean = {'Recurrence':[], 'No Recurrence':[]}\n",
    "PLSDA_feats_GAN_bal_mean = {'Recurrence':[], 'No Recurrence':[]}\n",
    "\n",
    "for i in PLSDA_models_real.keys():\n",
    "    for fold in PLSDA_models_real[i]:\n",
    "        if fold == 1:\n",
    "            vips = ma._calculate_vips(PLSDA_models_real[i][fold])\n",
    "            temp_df_imb = vips\n",
    "            vips = ma._calculate_vips(PLSDA_models_GAN_bal[i][fold])\n",
    "            temp_df_bal = vips\n",
    "            vips = ma._calculate_vips(PLSDA_models_GAN[i][fold])\n",
    "            temp_df_GAN = vips\n",
    "        else:\n",
    "            vips = ma._calculate_vips(PLSDA_models_real[i][fold])\n",
    "            temp_df_imb = temp_df_imb + vips\n",
    "            vips = ma._calculate_vips(PLSDA_models_GAN_bal[i][fold])\n",
    "            temp_df_bal = temp_df_bal + vips\n",
    "            vips = ma._calculate_vips(PLSDA_models_GAN[i][fold])\n",
    "            temp_df_GAN = temp_df_GAN + vips\n",
    "\n",
    "    temp_df_imb = temp_df_imb / len(PLSDA_models_real[i])\n",
    "    temp_df_imb = pd.DataFrame(zip(range(len(generated_samples[i][fold][0].columns)), temp_df_imb))\n",
    "    temp_df_imb = temp_df_imb.set_index(0).sort_values(by=1, ascending=False)\n",
    "    temp_df_imb.index = [generated_samples[i][fold][0].columns[a] for a in temp_df_imb.index]\n",
    "    PLSDA_feats_imb_mean[i] = temp_df_imb.copy()\n",
    "\n",
    "    temp_df_bal = temp_df_bal / len(PLSDA_models_GAN_bal[i])\n",
    "    temp_df_bal = pd.DataFrame(zip(range(len(generated_samples[i][fold][0].columns)), temp_df_bal))\n",
    "    temp_df_bal = temp_df_bal.set_index(0).sort_values(by=1, ascending=False)\n",
    "    temp_df_bal.index = [generated_samples[i][fold][0].columns[a] for a in temp_df_bal.index]\n",
    "    PLSDA_feats_GAN_bal_mean[i] = temp_df_bal.copy()\n",
    "\n",
    "    temp_df_GAN = temp_df_GAN / len(PLSDA_models_GAN[i])\n",
    "    temp_df_GAN = pd.DataFrame(zip(range(len(generated_samples[i][fold][0].columns)), temp_df_GAN))\n",
    "    temp_df_GAN = temp_df_GAN.set_index(0).sort_values(by=1, ascending=False)\n",
    "    temp_df_GAN.index = [generated_samples[i][fold][0].columns[a] for a in temp_df_GAN.index]\n",
    "    PLSDA_feats_GAN_mean[i] = temp_df_GAN.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da435f2e",
   "metadata": {},
   "source": [
    "Calculate intersection of important features from top 1 to top (number of features in the dataset) between the complete dataset (averaged over 20 iterations) and a iteration of the complete dataset, the imbalanced dataset, the balanced dataset and the GAN dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8dedcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "intersections_PLSDA = []\n",
    "for i in range(1, len(PLSDA_feats_real1)):\n",
    "    intersections_PLSDA.append(len(np.intersect1d(PLSDA_feats_real1.index[:i], PLSDA_feats_real.index[:i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd22122e",
   "metadata": {},
   "outputs": [],
   "source": [
    "intersections_PLSDA_bal = {}\n",
    "for cl in PLSDA_feats_GAN_bal_mean:\n",
    "    int_bal = []\n",
    "    for i in range(1, len(PLSDA_feats_real)):\n",
    "        int_bal.append(len(np.intersect1d(PLSDA_feats_real.index[:i], PLSDA_feats_GAN_bal_mean[cl].index[:i])))\n",
    "    intersections_PLSDA_bal[cl] = int_bal\n",
    "\n",
    "print('Intersections - Dataset Balanced - Finished')\n",
    "\n",
    "intersections_PLSDA_GAN = {}\n",
    "for cl in PLSDA_feats_GAN_mean:\n",
    "    int_GAN = []\n",
    "    for i in range(1, len(PLSDA_feats_real)):\n",
    "        int_GAN.append(len(np.intersect1d(PLSDA_feats_real.index[:i], PLSDA_feats_GAN_mean[cl].index[:i])))\n",
    "    intersections_PLSDA_GAN[cl] = int_GAN\n",
    "\n",
    "print('Intersections - Dataset GAN - Finished')\n",
    "\n",
    "intersections_PLSDA_imb = {}\n",
    "for cl in PLSDA_feats_imb_mean:\n",
    "    int_imb = []\n",
    "    for i in range(1, len(PLSDA_feats_real)):\n",
    "        int_imb.append(len(np.intersect1d(PLSDA_feats_real.index[:i], PLSDA_feats_imb_mean[cl].index[:i])))\n",
    "    intersections_PLSDA_imb[cl] = int_imb\n",
    "\n",
    "print('Intersections - Dataset Imbalanced - Finished')\n",
    "\n",
    "# See intersections if features were randomly shuffled\n",
    "random_intersections_PLSDA = []\n",
    "copy_shuffle = list(PLSDA_feats_real.index).copy()\n",
    "np.random.shuffle(copy_shuffle)\n",
    "for i in range(1, len(PLSDA_feats_real)):\n",
    "    random_intersections_PLSDA.append(len(np.intersect1d(PLSDA_feats_real.index[:i], copy_shuffle[:i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc75f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, (axl, axr) = plt.subplots(1,2,figsize=(12,6))\n",
    "# Graph depicting intersection of important features\n",
    "axl.scatter(range(1,len(intersections_PLSDA)+1),\n",
    "            np.array(intersections_PLSDA) / np.array(range(1,len(intersections_PLSDA)+1)), \n",
    "            label = 'Reference (expected var.)', color='Black', s=5)\n",
    "axr.scatter(range(1,len(intersections_PLSDA)+1),\n",
    "            np.array(intersections_PLSDA) / np.array(range(1,len(intersections_PLSDA)+1)),\n",
    "            label = 'Reference (expected var.)', color='Black', s=5)\n",
    "\n",
    "for i,ax in zip(intersections_PLSDA_imb, (axl, axr)):\n",
    "    ax.scatter(range(1,len(intersections_PLSDA)+1),\n",
    "               np.array(intersections_PLSDA_imb[i]) / np.array(range(1,len(intersections_PLSDA)+1)),\n",
    "                label = 'Imbalanced Exp. Dataset', color='Green', s=5)\n",
    "    ax.scatter(range(1,len(intersections_PLSDA)+1),\n",
    "               np.array(intersections_PLSDA_bal[i]) / np.array(range(1,len(intersections_PLSDA)+1)),\n",
    "                label = 'GAN Augmented Exp. Dataset', color='Blue', s=5)\n",
    "    ax.scatter(range(1,len(intersections_PLSDA)+1),\n",
    "               np.array(intersections_PLSDA_GAN[i]) / np.array(range(1,len(intersections_PLSDA)+1)),\n",
    "                label = 'GAN Samples Dataset', color='Red', s=5)\n",
    "    \n",
    "    ax.scatter(range(1,len(intersections_PLSDA)+1),\n",
    "               np.array(random_intersections_PLSDA) / np.array(range(1,len(intersections_PLSDA)+1)),\n",
    "            label = 'Random', color='Orange', s=5)\n",
    "    \n",
    "    ax.set_title('Minority Class: '+i, fontsize=15)\n",
    "\n",
    "axl.set_ylabel('Fraction of Common Compounds', fontsize=15)\n",
    "axl.set_xlim([0,len(intersections_PLSDA)//8])\n",
    "axl.set_ylim([0,1.01])\n",
    "\n",
    "f.supxlabel('Nº of Top Important (VIP Score) Compounds', fontsize=15)\n",
    "\n",
    "axr.set_xlim([0,len(intersections_PLSDA)//8])\n",
    "axr.set_ylim([0,1.01])\n",
    "axr.legend(loc='upper left', fontsize=11, bbox_to_anchor=(1,1), ncol=1, markerscale=3)\n",
    "plt.suptitle('PLS-DA', fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1ef23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axl = plt.subplots(1,1,figsize=(8,4))\n",
    "# Graph depicting intersection of important features\n",
    "axl.scatter(range(1,len(intersections_PLSDA)+1),\n",
    "            np.array(intersections_PLSDA) / np.array(range(1,len(intersections_PLSDA)+1)), \n",
    "            label = 'Reference (expected var.)', color='Black', s=5)\n",
    "\n",
    "axl.scatter(range(1,len(intersections_PLSDA)+1),\n",
    "            np.array(pd.DataFrame(intersections_PLSDA_imb).mean(axis=1)) / np.array(range(1,len(intersections_PLSDA)+1)),\n",
    "            label = 'Imbalanced Exp. Dataset', color='Green', s=5)\n",
    "axl.scatter(range(1,len(intersections_PLSDA)+1),\n",
    "            np.array(pd.DataFrame(intersections_PLSDA_bal).mean(axis=1)) / np.array(range(1,len(intersections_PLSDA)+1)),\n",
    "            label = 'GAN Augmented Exp. Dataset', color='Red', s=5)\n",
    "axl.scatter(range(1,len(intersections_PLSDA)+1),\n",
    "            np.array(pd.DataFrame(intersections_PLSDA_GAN).mean(axis=1)) / np.array(range(1,len(intersections_PLSDA)+1)),\n",
    "            label = 'GAN Samples Dataset', color='Blue', s=5)\n",
    "\n",
    "axl.scatter(range(1,len(intersections_PLSDA)+1),\n",
    "            np.array(random_intersections_PLSDA) / np.array(range(1,len(intersections_PLSDA)+1)),\n",
    "            label = 'Random', color='Orange', s=5)\n",
    "\n",
    "axl.legend(loc='upper left', fontsize=11, ncol=1, bbox_to_anchor=(1,1), markerscale=3)\n",
    "axl.set_xlabel('Nº of Top Important (VIP Score) Compounds', fontsize=14)\n",
    "axl.set_ylabel('Fraction of Common Compounds', fontsize=14)\n",
    "axl.set_xlim([0,len(intersections_PLSDA)//8])\n",
    "axl.set_ylim([0,1.01])\n",
    "axl.set_title('PLS-DA', fontsize=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e5ff9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with plt.style.context('seaborn-whitegrid'):\n",
    "    f, (axl, axr) = plt.subplots(1,2,figsize=(8,3), constrained_layout=True)\n",
    "\n",
    "    # Graph depicting intersection of important features\n",
    "    axl.scatter(range(1,len(intersections_RF)+1), np.array(intersections_RF) / np.array(range(1,len(intersections_RF)+1)), \n",
    "                label = 'Reference (expected var.)', color='Black', s=2)\n",
    "\n",
    "    axl.scatter(range(1,len(intersections_RF)+1),\n",
    "                np.array(pd.DataFrame(intersections_RF_imb).mean(axis=1)) / np.array(range(1,len(intersections_RF)+1)),\n",
    "                label = 'Exp. Imbalanced', color='Green', s=2)\n",
    "    axl.scatter(range(1,len(intersections_RF)+1),\n",
    "                np.array(pd.DataFrame(intersections_RF_bal).mean(axis=1)) / np.array(range(1,len(intersections_RF)+1)),\n",
    "                label = 'Imb. + Min. Cl. GAN Data', color='Red', s=2)\n",
    "    axl.scatter(range(1,len(intersections_RF)+1),\n",
    "                np.array(pd.DataFrame(intersections_RF_GAN).mean(axis=1)) / np.array(range(1,len(intersections_RF)+1)),\n",
    "                label = 'Only GAN data', color='Blue', s=2)\n",
    "\n",
    "    axl.scatter(range(1,len(intersections_RF)+1),\n",
    "                np.array(random_intersections_RF) / np.array(range(1,len(intersections_RF)+1)),\n",
    "                label = 'Random', color='Orange', s=2)\n",
    "\n",
    "    axl.set_xlabel('Nº of Top Imp. (Gini Importance) Feat.', fontsize=13)\n",
    "    axl.set_ylabel('Common Imp. Feat. Frac.', fontsize=13)\n",
    "    axl.set_xlim([0,len(intersections_RF)//8])\n",
    "    axl.set_ylim([0,1.01])\n",
    "    axl.set_title('RF', fontsize=13)\n",
    "\n",
    "    # Graph depicting intersection of important features\n",
    "    axr.scatter(range(1,len(intersections_PLSDA)+1),\n",
    "                np.array(intersections_PLSDA) / np.array(range(1,len(intersections_PLSDA)+1)), \n",
    "                label = 'Reference (expected var.)', color='Black', s=2)\n",
    "\n",
    "    axr.scatter(range(1,len(intersections_PLSDA)+1),\n",
    "                np.array(pd.DataFrame(intersections_PLSDA_imb).mean(axis=1)) / np.array(range(1,len(intersections_PLSDA)+1)),\n",
    "                label = 'Exp. Imbalanced', color='Green', s=2)\n",
    "    axr.scatter(range(1,len(intersections_PLSDA)+1),\n",
    "                np.array(pd.DataFrame(intersections_PLSDA_bal).mean(axis=1)) / np.array(range(1,len(intersections_PLSDA)+1)),\n",
    "                label = 'Imb. + Min. Cl. GAN Data', color='Red', s=2)\n",
    "    axr.scatter(range(1,len(intersections_PLSDA)+1),\n",
    "                np.array(pd.DataFrame(intersections_PLSDA_GAN).mean(axis=1)) / np.array(range(1,len(intersections_PLSDA)+1)),\n",
    "                label = 'Only GAN data', color='Blue', s=2)\n",
    "\n",
    "    axr.scatter(range(1,len(intersections_PLSDA)+1),\n",
    "                np.array(random_intersections_PLSDA) / np.array(range(1,len(intersections_PLSDA)+1)),\n",
    "                label = 'Random', color='Orange', s=2)\n",
    "\n",
    "    #axr.legend(loc='upper left', fontsize=11, ncol=1, bbox_to_anchor=(1,1), markerscale=3, handletextpad=0)\n",
    "    axr.legend(loc='center right', fontsize=9, ncol=1, markerscale=3, handletextpad=0, frameon=True,\n",
    "               bbox_to_anchor=(0.9,0.35))\n",
    "    axr.set_xlabel('Nº of Top Imp. (VIP Score) Feat.', fontsize=13)\n",
    "    #axr.set_ylabel('Common Imp. Feat. Fraction', fontsize=11.5)\n",
    "    axr.set_xlim([0,len(intersections_PLSDA)//8])\n",
    "    axr.set_ylim([0,1.01])\n",
    "    axr.set_title('PLS-DA', fontsize=13)\n",
    "    def fmt_two_digits(x, pos):\n",
    "        return f'{x:.2f}'\n",
    "    axl.yaxis.set_major_formatter(ticker.FuncFormatter(fmt_two_digits))\n",
    "    axr.yaxis.set_major_formatter(ticker.FuncFormatter(fmt_two_digits))\n",
    "    \n",
    "    axl.tick_params(axis='both', which='major', labelsize=11)\n",
    "    axr.tick_params(axis='both', which='major', labelsize=11)\n",
    "\n",
    "f.savefig('images/PCDint_Imbalanced_ImpFeat_plot.png', dpi=400)\n",
    "f.savefig('images/PCDint_Imbalanced_ImpFeat_plot.pdf', dpi=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d4a06e",
   "metadata": {},
   "source": [
    "#### Comparing Against Significant Feature (by Univariate Analysis) of the Complete models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28368e71",
   "metadata": {},
   "source": [
    "Calculate intersection of important features from top 1 to top (number of features in the dataset) between the complete dataset (averaged over 20 iterations) and a iteration of the complete dataset, the imbalanced dataset, the balanced dataset and the GAN dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ceee60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "intersections_RF_uni = []\n",
    "for i in range(1, len(uni_results)):\n",
    "    intersections_RF_uni.append(len(np.intersect1d(RF_feats_real1.index[:i], uni_results.index[:i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93e7ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "intersections_RF_bal_uni = {}\n",
    "for cl in RF_feats_GAN_bal_mean:\n",
    "    int_bal = []\n",
    "    for i in range(1, len(uni_results)):\n",
    "        int_bal.append(len(np.intersect1d(uni_results.index[:i], RF_feats_GAN_bal_mean[cl].index[:i])))\n",
    "    intersections_RF_bal_uni[cl] = int_bal\n",
    "\n",
    "print('Intersections - Dataset Balanced - Finished')\n",
    "\n",
    "intersections_RF_GAN_uni = {}\n",
    "for cl in RF_feats_GAN_mean:\n",
    "    int_GAN = []\n",
    "    for i in range(1, len(uni_results)):\n",
    "        int_GAN.append(len(np.intersect1d(uni_results.index[:i], RF_feats_GAN_mean[cl].index[:i])))\n",
    "    intersections_RF_GAN_uni[cl] = int_GAN\n",
    "\n",
    "print('Intersections - Dataset GAN - Finished')\n",
    "\n",
    "intersections_RF_imb_uni = {}\n",
    "for cl in RF_feats_imb_mean:\n",
    "    int_imb = []\n",
    "    for i in range(1, len(uni_results)):\n",
    "        int_imb.append(len(np.intersect1d(uni_results.index[:i], RF_feats_imb_mean[cl].index[:i])))\n",
    "    intersections_RF_imb_uni[cl] = int_imb\n",
    "\n",
    "print('Intersections - Dataset Imbalanced - Finished')\n",
    "\n",
    "# See intersections if features were randomly shuffled\n",
    "random_intersections_RF_uni = []\n",
    "copy_shuffle = list(uni_results.index).copy()\n",
    "np.random.shuffle(copy_shuffle)\n",
    "for i in range(1, len(uni_results)):\n",
    "    random_intersections_RF_uni.append(len(np.intersect1d(uni_results.index[:i], copy_shuffle[:i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac1c626",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, (axl, axr) = plt.subplots(1,2,figsize=(12,6))\n",
    "# Graph depicting intersection of important features\n",
    "axl.scatter(range(1,len(intersections_RF_uni)+1),\n",
    "            np.array(intersections_RF_uni) / np.array(range(1,len(intersections_RF_uni)+1)), \n",
    "            label = 'Real-Univariate Intersections', color='Black', s=5)\n",
    "axr.scatter(range(1,len(intersections_RF_uni)+1),\n",
    "            np.array(intersections_RF_uni) / np.array(range(1,len(intersections_RF_uni)+1)),\n",
    "            label = 'Real-Univariate Intersections', color='Black', s=5)\n",
    "\n",
    "for i,ax in zip(intersections_RF_imb_uni, (axl,axr)):\n",
    "    ax.scatter(range(1,len(intersections_RF_uni)+1),\n",
    "               np.array(intersections_RF_imb_uni[i]) / np.array(range(1,len(intersections_RF_uni)+1)),\n",
    "                label = 'Imbalanced Exp. Dataset', color='Green', s=5)\n",
    "    ax.scatter(range(1,len(intersections_RF_uni)+1),\n",
    "               np.array(intersections_RF_bal_uni[i]) / np.array(range(1,len(intersections_RF_uni)+1)),\n",
    "                label = 'GAN Augmented Exp. Dataset', color='Blue', s=5)\n",
    "    ax.scatter(range(1,len(intersections_RF_uni)+1),\n",
    "               np.array(intersections_RF_GAN_uni[i]) / np.array(range(1,len(intersections_RF_uni)+1)),\n",
    "                label = 'GAN Samples Dataset', color='Red', s=5)\n",
    "    \n",
    "    ax.scatter(range(1,len(intersections_RF_uni)+1),\n",
    "               np.array(random_intersections_RF_uni) / np.array(range(1,len(intersections_RF_uni)+1)),\n",
    "            label = 'Random', color='Orange', s=5)\n",
    "    \n",
    "    ax.set_title('Minority Class: '+i, fontsize=15)\n",
    "\n",
    "#axl.legend(loc='center left', fontsize=11, bbox_to_anchor=(-0.2,-0.15), ncol=5)\n",
    "axl.set_ylabel('Fraction of Common Compounds', fontsize=15)\n",
    "axl.set_xlim([0,len(intersections_RF_uni)//8])\n",
    "axl.set_ylim([0,1.01])\n",
    "\n",
    "f.supxlabel('Nº of Top Important (Gini Importance) Compounds', fontsize=15)\n",
    "\n",
    "axr.set_xlim([0,len(intersections_RF_uni)//8])\n",
    "axr.set_ylim([0,1.01])\n",
    "axr.legend(loc='upper left', fontsize=11, bbox_to_anchor=(1,1), ncol=1, markerscale=3)\n",
    "plt.suptitle('Random Forest', fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c89b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axl = plt.subplots(1,1,figsize=(8,4))\n",
    "# Graph depicting intersection of important features\n",
    "axl.scatter(range(1,len(intersections_RF_uni)+1),\n",
    "            np.array(intersections_RF_uni) / np.array(range(1,len(intersections_RF_uni)+1)), \n",
    "            label = 'Ref.-Univariate Intersections', color='Black', s=5)\n",
    "\n",
    "axl.scatter(range(1,len(intersections_RF_uni)+1),\n",
    "            np.array(pd.DataFrame(intersections_RF_imb_uni).mean(axis=1)) / np.array(range(1,len(intersections_RF_uni)+1)),\n",
    "            label = 'Imbalanced Exp. Dataset', color='Green', s=5)\n",
    "axl.scatter(range(1,len(intersections_RF_uni)+1),\n",
    "            np.array(pd.DataFrame(intersections_RF_bal_uni).mean(axis=1)) / np.array(range(1,len(intersections_RF_uni)+1)),\n",
    "            label = 'GAN Augmented Exp. Dataset', color='Red', s=5)\n",
    "axl.scatter(range(1,len(intersections_RF_uni)+1),\n",
    "            np.array(pd.DataFrame(intersections_RF_GAN_uni).mean(axis=1)) / np.array(range(1,len(intersections_RF_uni)+1)),\n",
    "            label = 'GAN Samples Dataset', color='Blue', s=5)\n",
    "\n",
    "axl.scatter(range(1,len(intersections_RF_uni)+1),\n",
    "            np.array(random_intersections_RF_uni) / np.array(range(1,len(intersections_RF_uni)+1)),\n",
    "            label = 'Random', color='Orange', s=5)\n",
    "\n",
    "axl.legend(loc='upper left', fontsize=11, ncol=1, bbox_to_anchor=(1,1), markerscale=3)\n",
    "axl.set_xlabel('Nº of Top Important (Gini Importance) Compounds', fontsize=14)\n",
    "axl.set_ylabel('Fraction of Common Compounds', fontsize=14)\n",
    "axl.set_xlim([0,len(intersections_RF_uni)//8])\n",
    "axl.set_ylim([0,1.01])\n",
    "axl.set_title('Random Forest', fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431dcab1",
   "metadata": {},
   "source": [
    "Calculate intersection of important features from top 1 to top (number of features in the dataset) between the complete dataset (averaged over 20 iterations) and a iteration of the complete dataset, the imbalanced dataset, the balanced dataset and the GAN dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8606d6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "intersections_PLSDA_uni = []\n",
    "for i in range(1, len(uni_results)):\n",
    "    intersections_PLSDA_uni.append(len(np.intersect1d(PLSDA_feats_real1.index[:i], uni_results.index[:i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d37a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "intersections_PLSDA_bal_uni = {}\n",
    "for cl in PLSDA_feats_GAN_bal_mean:\n",
    "    int_bal = []\n",
    "    for i in range(1, len(uni_results)):\n",
    "        int_bal.append(len(np.intersect1d(uni_results.index[:i], PLSDA_feats_GAN_bal_mean[cl].index[:i])))\n",
    "    intersections_PLSDA_bal_uni[cl] = int_bal\n",
    "\n",
    "print('Intersections - Dataset Balanced - Finished')\n",
    "\n",
    "intersections_PLSDA_GAN_uni = {}\n",
    "for cl in PLSDA_feats_GAN_mean:\n",
    "    int_GAN = []\n",
    "    for i in range(1, len(uni_results)):\n",
    "        int_GAN.append(len(np.intersect1d(uni_results.index[:i], PLSDA_feats_GAN_mean[cl].index[:i])))\n",
    "    intersections_PLSDA_GAN_uni[cl] = int_GAN\n",
    "\n",
    "print('Intersections - Dataset GAN - Finished')\n",
    "\n",
    "intersections_PLSDA_imb_uni = {}\n",
    "for cl in PLSDA_feats_imb_mean:\n",
    "    int_imb = []\n",
    "    for i in range(1, len(uni_results)):\n",
    "        int_imb.append(len(np.intersect1d(uni_results.index[:i], PLSDA_feats_imb_mean[cl].index[:i])))\n",
    "    intersections_PLSDA_imb_uni[cl] = int_imb\n",
    "\n",
    "print('Intersections - Dataset Imbalanced - Finished')\n",
    "\n",
    "# See intersections if features were randomly shuffled\n",
    "random_intersections_PLSDA_uni = []\n",
    "copy_shuffle = list(uni_results.index).copy()\n",
    "np.random.shuffle(copy_shuffle)\n",
    "for i in range(1, len(uni_results)):\n",
    "    random_intersections_PLSDA_uni.append(len(np.intersect1d(uni_results.index[:i], copy_shuffle[:i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d08075",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, (axl, axr) = plt.subplots(1,2,figsize=(12,6))\n",
    "# Graph depicting intersection of important features\n",
    "axl.scatter(range(1,len(intersections_PLSDA_uni)+1),\n",
    "            np.array(intersections_PLSDA_uni) / np.array(range(1,len(intersections_PLSDA_uni)+1)), \n",
    "            label = 'Ref.-Univariate Intersections', color='Black', s=5)\n",
    "axr.scatter(range(1,len(intersections_PLSDA_uni)+1),\n",
    "            np.array(intersections_PLSDA_uni) / np.array(range(1,len(intersections_PLSDA_uni)+1)),\n",
    "            label = 'Ref.-Univariate Intersections', color='Black', s=5)\n",
    "\n",
    "\n",
    "for i,ax in zip(intersections_PLSDA_imb_uni, (axl, axr)):\n",
    "    ax.scatter(range(1,len(intersections_PLSDA_uni)+1),\n",
    "               np.array(intersections_PLSDA_imb_uni[i]) / np.array(range(1,len(intersections_PLSDA_uni)+1)),\n",
    "                label = 'Imbalanced Exp. Dataset', color='Green', s=5)\n",
    "    ax.scatter(range(1,len(intersections_PLSDA_uni)+1),\n",
    "               np.array(intersections_PLSDA_bal_uni[i]) / np.array(range(1,len(intersections_PLSDA_uni)+1)),\n",
    "                label = 'GAN Augmented Exp. Dataset', color='Blue', s=5)\n",
    "    ax.scatter(range(1,len(intersections_PLSDA_uni)+1),\n",
    "               np.array(intersections_PLSDA_GAN_uni[i]) / np.array(range(1,len(intersections_PLSDA_uni)+1)),\n",
    "                label = 'GAN Samples Dataset', color='Red', s=5)\n",
    "    \n",
    "    ax.scatter(range(1,len(intersections_PLSDA_uni)+1),\n",
    "               np.array(random_intersections_PLSDA_uni) / np.array(range(1,len(intersections_PLSDA_uni)+1)),\n",
    "                label = 'Random', color='Orange', s=5)\n",
    "    \n",
    "    ax.set_title('Minority Class: '+i, fontsize=15)\n",
    "\n",
    "axl.set_ylabel('Fraction of Common Compounds', fontsize=15)\n",
    "axl.set_xlim([0,len(intersections_PLSDA_uni)//8])\n",
    "axl.set_ylim([0,1.01])\n",
    "\n",
    "f.supxlabel('Nº of Top Important (VIP Score) Compounds', fontsize=15)\n",
    "\n",
    "axr.set_xlim([0,len(intersections_PLSDA_uni)//8])\n",
    "axr.set_ylim([0,1.01])\n",
    "axr.legend(loc='upper left', fontsize=11, bbox_to_anchor=(1,1), ncol=1, markerscale=3)\n",
    "plt.suptitle('PLS-DA', fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34881885",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axl = plt.subplots(1,1,figsize=(8,4))\n",
    "# Graph depicting intersection of important features\n",
    "axl.scatter(range(1,len(intersections_PLSDA_uni)+1),\n",
    "            np.array(intersections_PLSDA_uni) / np.array(range(1,len(intersections_PLSDA_uni)+1)), \n",
    "            label = 'Ref.-Univariate Intersections', color='Black', s=5)\n",
    "\n",
    "axl.scatter(range(1,len(intersections_PLSDA_uni)+1),\n",
    "            np.array(pd.DataFrame(intersections_PLSDA_imb_uni).mean(axis=1)) / np.array(\n",
    "                range(1,len(intersections_PLSDA_uni)+1)),\n",
    "            label = 'Imbalanced Exp. Dataset', color='Green', s=5)\n",
    "axl.scatter(range(1,len(intersections_PLSDA_uni)+1),\n",
    "            np.array(pd.DataFrame(intersections_PLSDA_bal_uni).mean(axis=1)) / np.array(\n",
    "                range(1,len(intersections_PLSDA_uni)+1)),\n",
    "            label = 'GAN Augmented Exp. Dataset', color='Red', s=5)\n",
    "axl.scatter(range(1,len(intersections_PLSDA_uni)+1),\n",
    "            np.array(pd.DataFrame(intersections_PLSDA_GAN_uni).mean(axis=1)) / np.array(\n",
    "                range(1,len(intersections_PLSDA_uni)+1)),\n",
    "            label = 'GAN Samples Dataset', color='Blue', s=5)\n",
    "\n",
    "axl.scatter(range(1,len(intersections_PLSDA_uni)+1),\n",
    "            np.array(random_intersections_PLSDA_uni) / np.array(range(1,len(intersections_PLSDA_uni)+1)),\n",
    "            label = 'Random', color='Orange', s=5)\n",
    "\n",
    "axl.legend(loc='upper left', fontsize=11, ncol=1, bbox_to_anchor=(1,1), markerscale=3)\n",
    "axl.set_xlabel('Nº of Top Important (VIP Score) Compounds', fontsize=14)\n",
    "axl.set_ylabel('Fraction of Common Compounds', fontsize=14)\n",
    "axl.set_xlim([0,len(intersections_PLSDA_uni)//8])\n",
    "axl.set_ylim([0,1.01])\n",
    "axl.set_title('PLS-DA', fontsize=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f331580",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, (axl, axr) = plt.subplots(1,2,figsize=(12,4),constrained_layout=True)\n",
    "\n",
    "# Graph depicting intersection of important features\n",
    "axl.scatter(range(1,len(intersections_RF_uni)+1), \n",
    "            np.array(intersections_RF_uni) / np.array(range(1,len(intersections_RF_uni)+1)), \n",
    "            label = 'Ref.-Univariate Inter.', color='Black', s=5)\n",
    "\n",
    "axl.scatter(range(1,len(intersections_RF_uni)+1),\n",
    "            np.array(pd.DataFrame(intersections_RF_imb_uni).mean(axis=1)) / np.array(range(1,len(intersections_RF_uni)+1)),\n",
    "            label = 'Exp. Imbalanced', color='Green', s=5)\n",
    "axl.scatter(range(1,len(intersections_RF_uni)+1),\n",
    "            np.array(pd.DataFrame(intersections_RF_bal_uni).mean(axis=1)) / np.array(range(1,len(intersections_RF_uni)+1)),\n",
    "            label = 'Imb. + Min. Cl. GAN Data', color='Red', s=5)\n",
    "axl.scatter(range(1,len(intersections_RF_uni)+1),\n",
    "            np.array(pd.DataFrame(intersections_RF_GAN_uni).mean(axis=1)) / np.array(range(1,len(intersections_RF_uni)+1)),\n",
    "            label = 'Only GAN data', color='Blue', s=5)\n",
    "\n",
    "axl.scatter(range(1,len(intersections_RF_uni)+1),\n",
    "            np.array(random_intersections_RF_uni) / np.array(range(1,len(intersections_RF_uni)+1)),\n",
    "            label = 'Random', color='Orange', s=5)\n",
    "\n",
    "axl.set_xlabel('Nº of Top Important (Gini Importance) Compounds', fontsize=13)\n",
    "axl.set_ylabel('Fraction of Common Compounds', fontsize=13)\n",
    "axl.set_xlim([0,len(intersections_RF_uni)//8])\n",
    "axl.set_ylim([0,1.01])\n",
    "axl.set_title('Random Forest', fontsize=18)\n",
    "\n",
    "# Graph depicting intersection of important features\n",
    "axr.scatter(range(1,len(intersections_PLSDA_uni)+1),\n",
    "            np.array(intersections_PLSDA_uni) / np.array(range(1,len(intersections_PLSDA_uni)+1)), \n",
    "            label = 'Ref.-Univariate Inter.', color='Black', s=5)\n",
    "\n",
    "axr.scatter(range(1,len(intersections_PLSDA_uni)+1),\n",
    "            np.array(pd.DataFrame(intersections_PLSDA_imb_uni).mean(axis=1)) / np.array(\n",
    "                range(1,len(intersections_PLSDA_uni)+1)),\n",
    "            label = 'Exp. Imbalanced', color='Green', s=5)\n",
    "axr.scatter(range(1,len(intersections_PLSDA_uni)+1),\n",
    "            np.array(pd.DataFrame(intersections_PLSDA_bal_uni).mean(axis=1)) / np.array(\n",
    "                range(1,len(intersections_PLSDA_uni)+1)),\n",
    "            label = 'Imb. + Min. Cl. GAN Data', color='Red', s=5)\n",
    "axr.scatter(range(1,len(intersections_PLSDA_uni)+1),\n",
    "            np.array(pd.DataFrame(intersections_PLSDA_GAN_uni).mean(axis=1)) / np.array(\n",
    "                range(1,len(intersections_PLSDA_uni)+1)),\n",
    "            label = 'Only GAN data', color='Blue', s=5)\n",
    "\n",
    "axr.scatter(range(1,len(intersections_PLSDA_uni)+1),\n",
    "            np.array(random_intersections_PLSDA_uni) / np.array(range(1,len(intersections_PLSDA_uni)+1)),\n",
    "            label = 'Random', color='Orange', s=5)\n",
    "\n",
    "axr.legend(loc='upper left', fontsize=11, ncol=1, bbox_to_anchor=(1,1), markerscale=3)\n",
    "#axl.legend(loc='lower left', fontsize=11, ncol=3, bbox_to_anchor=(0,-0.4), markerscale=3)\n",
    "axr.set_xlabel('Nº of Top Important (VIP Score) Compounds', fontsize=13)\n",
    "axr.set_ylabel('Fraction of Common Compounds', fontsize=13)\n",
    "axr.set_xlim([0,len(intersections_PLSDA_uni)//8])\n",
    "axr.set_ylim([0,1.01])\n",
    "axr.set_title('PLS-DA', fontsize=18)\n",
    "f.savefig('images/PCDint_Imbalanced_ImpFeat_UNI_plot.png', dpi=400)\n",
    "f.savefig('images/PCDint_Imbalanced_ImpFeat_UNI_plot.pdf', dpi=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115e6551",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e03b612d84ba21ce95ed447e81b3062e1eb99b56c6d885cdab4aaa12f1b8e240"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
