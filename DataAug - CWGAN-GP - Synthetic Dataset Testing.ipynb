{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c7c5b4b",
   "metadata": {},
   "source": [
    "# Data Augmentation - Conditional Wasserstein GANs - GP\n",
    "\n",
    "# Synthetic Dataset Testing\n",
    "\n",
    "This notebook presents the testing made on the effect of supplementing imbalanced datasets with samples of the minority class generated by trained CWGAN-GP models to balance datasets on supervised analysis. This testing is made on six synthetic datasets made whose difference is on the separation between the 2 classes of the dataset.\n",
    "\n",
    "Notebook Organization:\n",
    "- Create and treat 6 synthetic datasets.\n",
    "- Unsupervised and Supervised statistical analysis and univariate analysis of the synthetic datasets.\n",
    "- Creation of the imbalanced datasets for each synthetic datasets, creating 5 folds for each one.\n",
    "- Setup the CWGAN-GP model and train all 30 models (6 datasets times 5 folds), with the corresponding training data.\n",
    "- Generate GAN samples and add them to the corresponding imbalanced training sets in small increments.\n",
    "- Build and evaluate performance of RF and PLS-DA models from the imbalanced datasets, the imbalanced datasets supplemented with minority class samples and purely GAN samples dataset for each synthetic dataset and each fold.\n",
    "- Compare important features in the RF and PLS-DA models against important features of models built with the complete dataset.\n",
    "\n",
    "#### Due to stochasticity, re-running the notebook will get slightly different results. Thus, figures in the paper can be slightly different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9818aa4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# json for persistence\n",
    "from time import perf_counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import scipy.spatial.distance as dist\n",
    "import scipy.cluster.hierarchy as hier\n",
    "import scipy.stats as stats\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib import ticker\n",
    "\n",
    "import seaborn as sns\n",
    "from collections import namedtuple, Counter\n",
    "\n",
    "from tqdm import tqdm\n",
    "from IPython import display as ipythondisplay\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "import sklearn.cluster as skclust\n",
    "from sklearn.metrics import (adjusted_rand_score, precision_recall_fscore_support, r2_score, roc_auc_score,\n",
    "                             roc_curve, auc, f1_score, precision_score, recall_score)\n",
    "from sklearn.datasets import make_classification\n",
    "import sklearn.ensemble as skensemble\n",
    "import sklearn.model_selection\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, cross_validate\n",
    "\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "from keras import backend\n",
    "\n",
    "# Metabolinks package\n",
    "import metabolinks as mtl\n",
    "import metabolinks.transformations as transf\n",
    "\n",
    "# Python files in the repository\n",
    "import multianalysis as ma\n",
    "from elips import plot_confidence_ellipse\n",
    "import gan_evaluation_metrics as gem\n",
    "import linear_augmentation_functions as laf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b2eba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import needed functions from GAN_functions\n",
    "from GAN_functions import gradient_penalty_cwgan\n",
    "from GAN_functions import critic_loss_wgan\n",
    "from GAN_functions import generator_loss_wgan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68af2240",
   "metadata": {},
   "source": [
    "### Functions for unsupervised analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67384d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to plot PCA\n",
    "def plot_PCA(principaldf, label_colors, components=(1,2), title=\"PCA\", ax=None):\n",
    "    \"Plot the projection of samples in the 2 main components of a PCA model.\"\n",
    "    \n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    \n",
    "    loc_c1, loc_c2 = [c - 1 for c in components]\n",
    "    col_c1_name, col_c2_name = principaldf.columns[[loc_c1, loc_c2]]\n",
    "    \n",
    "    #ax.axis('equal')\n",
    "    ax.set_xlabel(f'{col_c1_name}')\n",
    "    ax.set_ylabel(f'{col_c2_name}')\n",
    "\n",
    "    unique_labels = principaldf['Label'].unique()\n",
    "\n",
    "    for lbl in unique_labels:\n",
    "        subset = principaldf[principaldf['Label']==lbl]\n",
    "        ax.scatter(subset[col_c1_name],\n",
    "                   subset[col_c2_name],\n",
    "                   s=50, color=label_colors[lbl], label=lbl)\n",
    "\n",
    "    #ax.legend(framealpha=1)\n",
    "    ax.set_title(title, fontsize=15)\n",
    "\n",
    "def plot_ellipses_PCA(principaldf, label_colors, components=(1,2),ax=None, q=None, nstd=2):\n",
    "    \"Plot confidence ellipses of a class' samples based on their projection in the 2 main components of a PCA model.\"\n",
    "    \n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    \n",
    "    loc_c1, loc_c2 = [c - 1 for c in components]\n",
    "    points = principaldf.iloc[:, [loc_c1, loc_c2]]\n",
    "    \n",
    "    #ax.axis('equal')\n",
    "\n",
    "    unique_labels = principaldf['Label'].unique()\n",
    "\n",
    "    for lbl in unique_labels:\n",
    "        subset_points = points[principaldf['Label']==lbl]\n",
    "        plot_confidence_ellipse(subset_points, q, nstd, ax=ax, ec=label_colors[lbl], fc='none')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9defd88d",
   "metadata": {},
   "source": [
    "#### Hierarchical Clustering Analysis (HCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b962aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_HCA(df, metric='euclidean', method='average'):\n",
    "    \"Performs Hierarchical Clustering Analysis of a data set with chosen linkage method and distance metric.\"\n",
    "    \n",
    "    distances = dist.pdist(df, metric=metric)\n",
    "    \n",
    "    # method is one of\n",
    "    # ward, average, centroid, single, complete, weighted, median\n",
    "    Z = hier.linkage(distances, method=method)\n",
    "\n",
    "    # Cophenetic Correlation Coefficient\n",
    "    # (see how the clustering - from hier.linkage - preserves the original distances)\n",
    "    coph = hier.cophenet(Z, distances)\n",
    "    # Baker's gamma\n",
    "    mr = ma.mergerank(Z)\n",
    "    bg = mr[mr!=0]\n",
    "\n",
    "    return {'Z': Z, 'distances': distances, 'coph': coph, 'merge_rank': mr, \"Baker's Gamma\": bg}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585aae24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_clustering_metrics(res_dict, labels):\n",
    "    \"\"\"Fill dict with clustering performance metrics.\"\"\"\n",
    "    \n",
    "    discrim = ma.dist_discrim(res_dict['Z'], labels, # all samples have the same order\n",
    "                              method = 'average')\n",
    "    res_dict['Average discrim dist'] = discrim[0]\n",
    "    correct = np.array(list(discrim[1].values()))\n",
    "    \n",
    "    classes = pd.unique(labels)\n",
    "    res_dict['% correct clustering'] = (100/len(classes)) * len(correct[correct>0])\n",
    "\n",
    "    # Correct First Cluster Percentage\n",
    "    res_dict['% correct 1st clustering'] = 100 * ma.correct_1stcluster_fraction(res_dict['Z'],labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12577109",
   "metadata": {},
   "source": [
    "#### K-Means Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b7c04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_KMeans(dataset, target, iter_num=150, best_fraction=0.1):\n",
    "    \"Perform K-means Clustering Analysis and calculate discrimination evaluation metrics.\"\n",
    "    \n",
    "    sample_labels = target\n",
    "    n_classes = len(pd.unique(sample_labels))\n",
    "    \n",
    "    df = dataset\n",
    "    \n",
    "    discrim = ma.Kmeans_discrim(df, sample_labels,\n",
    "                                method='average', \n",
    "                                iter_num=iter_num,\n",
    "                                best_fraction=best_fraction)\n",
    "\n",
    "    \n",
    "    # Lists for the results of the best k-means clustering\n",
    "    average = []\n",
    "    correct = []\n",
    "    rand = []\n",
    "    \n",
    "    for j in discrim:\n",
    "        global_disc_dist, disc_dists, rand_index, SSE = discrim[j]\n",
    "        \n",
    "        # Average of discrimination distances\n",
    "        average.append(global_disc_dist) \n",
    "        \n",
    "        # Correct Clustering Percentages\n",
    "        all_correct = np.array(list(disc_dists.values()))\n",
    "        correct.append(len(all_correct[all_correct>0]))\n",
    "        \n",
    "        # Adjusted Rand Index\n",
    "        rand.append(rand_index) \n",
    "    \n",
    "    return{'dataset': dataset,\n",
    "           'Discrimination Distance': np.median(average),\n",
    "           '% correct clusters':np.median(correct)*100/n_classes,\n",
    "           'Rand Index': np.median(rand)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4444e414",
   "metadata": {},
   "source": [
    "### Synthetic Dataset Creation\n",
    "\n",
    "**Characteristics:**\n",
    "\n",
    "- 2 classes\n",
    "- 200 samples (100 for each class)\n",
    "- 600 features (20 informative, 100 redundant - linear combination of informative ones - and 480 noisy features)\n",
    "- 2 clusters per class\n",
    "- No random flipping of class labels (flip_y)\n",
    "\n",
    "Syntethic datasets are Pareto scaled so their values are mainly between -1 and 1.\n",
    "\n",
    "**class_sep** (separation between classes) - varying from 0.6 to 1.6 in 0.2 intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022b41c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = {}\n",
    "lbls = {}\n",
    "dfs_no_t = {}\n",
    "\n",
    "sep = [0.6, 1.0, 1.2, 1.4, 1.6, 2.0]\n",
    "for i in sep:\n",
    "    print(i)\n",
    "    df_f, lbl_f = make_classification(n_samples=200, n_features=600, n_informative=20, n_redundant=100,\n",
    "                                       n_classes=2, n_clusters_per_class=2, weights=None,\n",
    "                                       flip_y=0, class_sep=i, random_state=52683)\n",
    "    dfs_no_t[i] = pd.DataFrame(df_f) # Save non-treated synthetic datasets\n",
    "    dfs[i] = transf.pareto_scale(pd.DataFrame(df_f)) # Save treated synthetic datasets\n",
    "    lbls[i] = [str(i) for i in lbl_f]\n",
    "lbls_orig = lbls # Save original labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa87a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "colours = sns.color_palette('Set1', 3)\n",
    "\n",
    "ordered_labels = pd.unique(lbls[0.6])\n",
    "\n",
    "label_colors = {lbl: c for lbl, c in zip(ordered_labels, colours)}\n",
    "sample_colors = [label_colors[lbl] for lbl in lbls[0.6]]\n",
    "\n",
    "sns.palplot(label_colors.values())\n",
    "new_ticks = plt.xticks(range(len(ordered_labels)), ordered_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de575e9",
   "metadata": {},
   "source": [
    "#### PCA Projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24305361",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axs = plt.subplots(2, 3, figsize=(16,10))\n",
    "\n",
    "# Perform PCA after autoscaling of the original data\n",
    "for i, ax in zip(dfs, axs.ravel()):\n",
    "    df = dfs[i]\n",
    "\n",
    "    principaldf, var = ma.compute_df_with_PCs(df, n_components=2, whiten=True, labels=lbls[i], return_var_ratios=True)\n",
    "\n",
    "    # Plot PCA\n",
    "    ax.axis('equal')\n",
    "    lcolors = label_colors\n",
    "\n",
    "    gem.plot_PCA(principaldf, lcolors, components=(1,2), title='', ax=ax)\n",
    "    plot_ellipses_PCA(principaldf, lcolors, components=(1,2),ax=ax, q=0.95)\n",
    "\n",
    "    ax.set_xlabel(f'PC 1 ({var[0] * 100:.1f} %)')\n",
    "    ax.set_ylabel(f'PC 2 ({var[1] * 100:.1f} %)')\n",
    "\n",
    "    ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43418301",
   "metadata": {},
   "source": [
    "#### Hierarchical Clustering Analysis (HCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4a0ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "HCA_all = {}\n",
    "for treat in dfs:\n",
    "    print(f'Performing HCA with treatment {treat}', end=' ...')\n",
    "    metric = 'euclidean'\n",
    "    HCA_all[treat] = perform_HCA(dfs[treat], metric=metric, method='ward')\n",
    "    print('done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e16115",
   "metadata": {},
   "outputs": [],
   "source": [
    "with sns.axes_style(\"white\"):\n",
    "    f, axs = plt.subplots(2, 3, figsize=(12, 10), constrained_layout=True)\n",
    "    \n",
    "    for i, ax in zip(dfs, axs.ravel()):\n",
    "        gem.plot_dendogram(HCA_all[i]['Z'], \n",
    "                       lbls[i], ax=ax,\n",
    "                       label_colors=label_colors,\n",
    "                       title='', color_threshold=0)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4d4352",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, res_dict in HCA_all.items():\n",
    "    compute_clustering_metrics(res_dict, lbls[name])\n",
    "\n",
    "# Build table - summary of results\n",
    "clust_performance = {}\n",
    "\n",
    "for metric in ('Average discrim dist', '% correct clustering', '% correct 1st clustering'):\n",
    "    clust_performance[metric] = {d: HCA_all[d][metric] for d in HCA_all}\n",
    "clust_performance = pd.DataFrame(clust_performance, index=HCA_all)\n",
    "clust_performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb152c16",
   "metadata": {},
   "source": [
    "#### K-Means Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8736065",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%capture --no-stdout\n",
    "np.random.seed(63780)\n",
    "iter_num=15\n",
    "\n",
    "KMeans_all = []\n",
    "\n",
    "for treatment in dfs:\n",
    "    print(f'performing KMeans with treatment {treatment}' , end=' ...')\n",
    "    KMeans_all.append(perform_KMeans(dfs[treatment], lbls[treatment], iter_num=iter_num))\n",
    "    print('done!')      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed35db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "KMeans_all = pd.DataFrame(KMeans_all).iloc[:,1:]\n",
    "KMeans_all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d658b22",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452feb3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RF_model_CV - RF application and result extraction.\n",
    "def RF_model_CV(df, y, iter_num=1, n_fold=5, n_trees=200):\n",
    "    nfeats = df.shape[1]\n",
    "\n",
    "    # Setting up variables for result storing\n",
    "    imp_feat = np.zeros((iter_num * n_fold, nfeats))\n",
    "    accuracy_scores = []\n",
    "    f1_scores = []\n",
    "    prec_scores = []\n",
    "    rec_scores = []\n",
    "    f = 0\n",
    "\n",
    "    # Number of times Random Forest cross-validation is made\n",
    "    # with `n_fold` randomly generated folds.\n",
    "    for _ in range(iter_num):\n",
    "        # Use stratified n_fold cross validation\n",
    "        kf = StratifiedKFold(n_fold, shuffle=True)\n",
    "        CV_accuracy_scores = []\n",
    "        CV_f1_scores = []\n",
    "        CV_prec_scores = []\n",
    "        CV_rec_scores = []\n",
    "        # Fit and evaluate a Random Forest model for each fold\n",
    "        for train_index, test_index in kf.split(df, y):\n",
    "            # Random Forest setup and fit\n",
    "            rf = skensemble.RandomForestClassifier(n_estimators=n_trees)\n",
    "            X_train, X_test = df.iloc[train_index, :], df.iloc[test_index, :]\n",
    "            y_train, y_test = [y[i] for i in train_index], [y[i] for i in test_index]\n",
    "            rf.fit(X_train, y_train)\n",
    "\n",
    "            # Compute performance and important features\n",
    "            CV_accuracy_scores.append(rf.score(X_test, y_test)) # Predictive Accuracy\n",
    "            preds = rf.predict(X_test)\n",
    "            prec, rec, f1, sup = precision_recall_fscore_support(y_test, preds,\n",
    "                                                                pos_label='1', average='binary',\n",
    "                                                                zero_division=1)\n",
    "            CV_f1_scores.append(f1)\n",
    "            CV_prec_scores.append(prec)\n",
    "            CV_rec_scores.append(rec)\n",
    "            imp_feat[f, :] = rf.feature_importances_ # Importance of each feature\n",
    "            f = f + 1\n",
    "\n",
    "        # Average Predictive Accuracy in this iteration\n",
    "        accuracy_scores.append(np.mean(CV_accuracy_scores))\n",
    "        f1_scores.append(np.mean(CV_f1_scores))\n",
    "        prec_scores.append(np.mean(CV_prec_scores))\n",
    "        rec_scores.append(np.mean(CV_rec_scores))\n",
    "\n",
    "    # Collect and order all important features values from each Random Forest\n",
    "    imp_feat_sum = imp_feat.sum(axis=0) / (iter_num * n_fold)\n",
    "    sorted_imp_feat = sorted(enumerate(imp_feat_sum), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # locs are sufficient as a reference to features\n",
    "    #imp_feat_tuples = [(loc, importance) for loc, importance in sorted_imp_feat]\n",
    "    \n",
    "    if iter_num == 1:\n",
    "        return {'accuracy': accuracy_scores[0], 'F1-Score':f1_scores[0], 'Precision':prec_scores[0],\n",
    "                'Recall':rec_scores[0], 'important_features': sorted_imp_feat}\n",
    "    else:\n",
    "        return {'accuracy': accuracy_scores, 'F1-Score':f1_scores, 'Precision':prec_scores,\n",
    "                'Recall':rec_scores, 'important_features': sorted_imp_feat}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192d9e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_num=20\n",
    "\n",
    "RF_all = {}\n",
    "\n",
    "# Application of the Random Forests for each differently-treated dataset\n",
    "for treatment in dfs:\n",
    "    print(f'Fitting random forest with treatment {treatment}', end=' ...')\n",
    "    rfname = treatment\n",
    "    RF_all[rfname] = {'treatment':treatment}\n",
    "    n_fold = 5\n",
    "\n",
    "    fit = RF_model_CV(dfs[treatment], lbls_orig[treatment], iter_num=iter_num, n_fold=n_fold, n_trees=200)\n",
    "    RF_all[rfname].update(fit)\n",
    "\n",
    "    print(f'done')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a309e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy across the iterations\n",
    "accuracies = pd.DataFrame({name: RF_all[name]['accuracy'] for name in RF_all})\n",
    "\n",
    "accuracy_stats_RF = pd.DataFrame({'Average accuracy': accuracies.mean(axis=0),\n",
    "                               'STD': accuracies.std(axis=0)})\n",
    "accuracy_stats_RF = accuracy_stats_RF.assign(treatment=[RF_all[name]['treatment'] for name in RF_all])\n",
    "accuracy_stats_RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc1b4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# F1-scores across the iterations\n",
    "F1s = pd.DataFrame({name: RF_all[name]['F1-Score'] for name in RF_all})\n",
    "\n",
    "F1s_stats_RF = pd.DataFrame({'Average accuracy': F1s.mean(axis=0),\n",
    "                               'STD': F1s.std(axis=0)})\n",
    "F1s_stats_RF = F1s_stats_RF.assign(treatment=[RF_all[name]['treatment'] for name in RF_all])\n",
    "F1s_stats_RF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5074bde5",
   "metadata": {},
   "source": [
    "#### PLS-DA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30b7625",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stdout\n",
    "# above is to supress PLS warnings\n",
    "\n",
    "max_comp=10\n",
    "\n",
    "# Store Results\n",
    "PLS_optim = {}\n",
    "\n",
    "# Build and extract metrics from models build with different number of components by using the optim_PLS function.\n",
    "for treatment in dfs:\n",
    "    print(f'Fitting PLS-DA model with treatment {treatment}', end=' ...')\n",
    "    plsdaname = treatment\n",
    "    PLS_optim[plsdaname] = {'treatment':treatment}\n",
    "    n_fold = 5\n",
    "    optim = ma.optim_PLSDA_n_components(dfs[treatment], lbls[treatment],\n",
    "                                        max_comp=max_comp, n_fold=n_fold).CVscores\n",
    "    PLS_optim[plsdaname]['CV_scores'] = optim\n",
    "    print(f'done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f50dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the results and adjusting plot parameters\n",
    "with sns.axes_style(\"whitegrid\"):\n",
    "    with sns.plotting_context(\"notebook\", font_scale=1.2):\n",
    "        f, ax = plt.subplots(1, 1, figsize = (5,5))\n",
    "        for name, data in PLS_optim.items():\n",
    "\n",
    "            # Negative Grapevine Dataset\n",
    "            ax.plot(range(1, len(data['CV_scores']) + 1), data['CV_scores'],\n",
    "                     label=data['treatment'])\n",
    "            ax.set(xlabel='Number of Components',\n",
    "                    ylabel='PLS Score (1 - PRESS/SS)',\n",
    "                    title='Negative Mode Grapevine Dataset')\n",
    "            ax.legend()\n",
    "            ax.set_ylim([0, 1])\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd4b9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PLSDA_model_CV(df, labels, n_comp=10,\n",
    "                   kf = None, n_fold=5,\n",
    "                   iter_num=1,\n",
    "                   encode2as1vector=True,\n",
    "                   scale=False,\n",
    "                   feat_type='VIP'):\n",
    "    \n",
    "    \"\"\"Perform PLS-DA with n-fold cross-validation.\n",
    "\n",
    "       df: pandas DataFrame; includes X equivalent in PLS-DA (training vectors).\n",
    "       labels: target labels.\n",
    "       n_comp: integer; number of components to use in PLS-DA.\n",
    "       kf: default None; pass a specific cross validation method from \n",
    "        https://scikit-learn.org/stable/modules/cross_validation.html#cross-validation-iterators (3.1.2)\n",
    "       n_fold: int (default: 5); number of groups to divide dataset in for cross-validation\n",
    "        (NOTE: max n_fold can not exceed minimum number of samples per class).\n",
    "       iter_num: int (default: 1); number of iterations that cross validation is repeated.\n",
    "       scale: bool (default: False); if data is scaled when inputted to PLS model (only true if scaling was not done earlier)\n",
    "       feat_type: string (default: 'VIP'); types of feature importance metrics to use; accepted: {'VIP', 'Coef', 'Weights'}.\n",
    "\n",
    "    Returns: (accuracy, F1-score, precision, recall, Q2, import_features);\n",
    "        accuracy: list of accuracy values in group selection\n",
    "        F1-score: list of F1-scores (weighted) in group selection\n",
    "        precision: list of precision (weighted) in group selection\n",
    "        recall: list of recall (weighted) in group selection\n",
    "        Q2: list of average Q2 scores of the models\n",
    "        imp_features: list of tuples (index number of feature, feature importance)\n",
    "            ordered by decreasing feature importance.\n",
    "    \"\"\"\n",
    "    # Setting up lists and matrices to store results\n",
    "    CVR2 = []\n",
    "    accuracies = []\n",
    "    f1_scores = []\n",
    "    precision = []\n",
    "    recall = []\n",
    "    Imp_Feat = np.zeros((iter_num * n_fold, df.shape[1]))\n",
    "    f = 0\n",
    "\n",
    "    unique_labels = list(pd.unique(labels))\n",
    "\n",
    "    is1vector = len(unique_labels) == 2 and encode2as1vector\n",
    "\n",
    "    matrix = ma._generate_y_PLSDA(labels, unique_labels, is1vector)\n",
    "\n",
    "    if is1vector:\n",
    "        # keep a copy to use later\n",
    "        target1D = matrix.copy()\n",
    "\n",
    "    # Number of iterations equal to iter_num\n",
    "    for i in range(iter_num):\n",
    "        if kf is None:\n",
    "            kf = sklearn.model_selection.StratifiedKFold(n_fold, shuffle=True)\n",
    "        \n",
    "        # Setting up storing variables for cross-validation\n",
    "        nright = 0 # For accuracy\n",
    "        cvr2 = [] # For R2 score\n",
    "        # To store real and predicted classes to calculate F1-score, precision and recall\n",
    "        if not is1vector:\n",
    "            all_preds = pd.DataFrame(columns=matrix.columns, index=matrix.index)\n",
    "            all_tests = pd.DataFrame(columns=matrix.columns, index=matrix.index)\n",
    "            a = 0\n",
    "        else:\n",
    "            all_preds = []\n",
    "            all_tests = []\n",
    "\n",
    "        # Iterate through cross-validation procedure\n",
    "        for train_index, test_index in kf.split(df, labels):\n",
    "            plsda = PLSRegression(n_components=n_comp, scale=scale)\n",
    "            X_train, X_test = df.iloc[train_index, :], df.iloc[test_index, :]\n",
    "            if not is1vector:\n",
    "                y_train = matrix.iloc[train_index, :].copy()\n",
    "                y_test = matrix.iloc[test_index, :].copy()\n",
    "\n",
    "            else:\n",
    "                y_train, y_test = target1D[train_index], target1D[test_index]\n",
    "                correct = target1D[test_index]\n",
    "\n",
    "            # Fit PLS model\n",
    "            plsda.fit(X=X_train, Y=y_train)\n",
    "\n",
    "            # Obtain results with the test group\n",
    "            y_pred = plsda.predict(X_test)\n",
    "            cvr2.append(r2_score(y_test, y_pred))\n",
    "\n",
    "            # Decision rule for classification\n",
    "            # Decision rule chosen: sample belongs to group where it has max y_pred (closer to 1)\n",
    "            # In case of 1,0 encoding for two groups, round to nearest integer to compare\n",
    "            if not is1vector:\n",
    "                rounded_pred = y_pred.copy()\n",
    "                for i in range(len(y_pred)):\n",
    "                    if list(y_test.iloc[i, :]).index(max(y_test.iloc[i, :])) == np.argmax(\n",
    "                        y_pred[i]\n",
    "                    ):\n",
    "                        nright += 1  # Correct prediction\n",
    "                    \n",
    "                    for l in range(len(y_pred[i])):\n",
    "                        if l == np.argmax(y_pred[i]):\n",
    "                            rounded_pred[i, l] = 1\n",
    "                        else:\n",
    "                            rounded_pred[i, l] = 0\n",
    "            \n",
    "                # Save y-test and predictions to calculate F1-score, precision and recall\n",
    "                all_tests.iloc[a:a+len(y_test)] = y_test\n",
    "                all_preds.iloc[a:a+len(y_test)] = rounded_pred\n",
    "                a = a + len(y_test)\n",
    "\n",
    "            else:\n",
    "                rounded = np.round(y_pred)\n",
    "                for p in range(len(y_pred)):\n",
    "                    if rounded[p] >= 1:\n",
    "                        rounded[p] = 1\n",
    "                    else:\n",
    "                        rounded[p] = 0\n",
    "                    if rounded[p] == correct[p]:\n",
    "                        nright += 1  # Correct prediction\n",
    "                \n",
    "                # Save y-test and predictions to calculate F1-score, precision and recall\n",
    "                all_preds.extend(list(rounded[:,0]))\n",
    "                all_tests.extend(y_test)\n",
    "            \n",
    "            # Calculate important features (3 different methods to choose from)\n",
    "            if feat_type == 'VIP':\n",
    "                Imp_Feat[f, :] = ma._calculate_vips(plsda)\n",
    "            elif feat_type == 'Coef':\n",
    "                Imp_Feat[f, :] = abs(plsda.coef_).sum(axis=1)\n",
    "            elif feat_type == 'Weights':\n",
    "                Imp_Feat[f, :] = abs(plsda.x_weights_).sum(axis=1)\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    'Type not Recognized. Types accepted: \"VIP\", \"Coef\", \"Weights\"'\n",
    "                )\n",
    "\n",
    "            f += 1\n",
    "\n",
    "        # Calculate the accuracy of the group predicted and storing score results\n",
    "        accuracies.append(nright / len(labels))\n",
    "        CVR2.append(np.mean(cvr2))\n",
    "        # Calculate F1-score, precision and recall for the fold and storing results\n",
    "        if not is1vector:\n",
    "            pos_label = np.where(unique_labels != '1')[0][0]\n",
    "            #print(unique_labels, pos_label)\n",
    "            f1_scores.append(f1_score(all_tests.astype(int), all_preds.astype(int), average='binary', pos_label=pos_label))\n",
    "            precision.append(precision_score(all_tests.astype(int), all_preds.astype(int), average='binary', pos_label=pos_label))\n",
    "            recall.append(recall_score(all_tests.astype(int), all_preds.astype(int), average='binary', pos_label=pos_label))\n",
    "        else:\n",
    "            pos_label = np.where(unique_labels != '1')[0][0]\n",
    "            #print(unique_labels, pos_label)\n",
    "            f1_scores.append(f1_score(all_tests, all_preds, average='binary', pos_label=pos_label))\n",
    "            precision.append(precision_score(all_tests, all_preds, average='binary', pos_label=pos_label))\n",
    "            recall.append(recall_score(all_tests, all_preds, average='binary', pos_label=pos_label))\n",
    "\n",
    "\n",
    "    # Join and sort all important features values from each cross validation group and iteration.\n",
    "    Imp_sum = Imp_Feat.sum(axis=0) / (iter_num * n_fold)\n",
    "    imp_features = sorted(enumerate(Imp_sum), key=lambda x: x[1], reverse=True)\n",
    "    if iter_num == 1:\n",
    "        return {'accuracy': accuracies[0], 'F1-scores':f1_scores[0], 'precision': precision[0], 'recall':recall[0],\n",
    "                'Q2': CVR2[0], 'imp_feat': imp_features}\n",
    "    else:\n",
    "        return {'accuracy': accuracies, 'F1-scores':f1_scores, 'precision': precision, 'recall':recall,\n",
    "                'Q2': CVR2, 'imp_feat': imp_features}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3979645",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stdout\n",
    "\n",
    "PLSDA_all = {}\n",
    "\n",
    "iter_num=20\n",
    "\n",
    "for treatment in dfs:\n",
    "    print(f'Fitting a PLS-DA model with treatment {treatment}', end=' ...')\n",
    "    plsdaname = treatment\n",
    "    PLSDA_all[plsdaname] = {'treatment':treatment}\n",
    "    n_comp = 4\n",
    "    n_fold = 5\n",
    "    fit = PLSDA_model_CV(dfs[treatment], lbls_orig[treatment],\n",
    "                            n_comp=n_comp, n_fold=n_fold,\n",
    "                            iter_num=iter_num,\n",
    "                            feat_type='VIP')\n",
    "    PLSDA_all[plsdaname].update(fit)\n",
    "    print(f'done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6b9755",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy across the iterations\n",
    "accuracies = pd.DataFrame({name: PLSDA_all[name]['accuracy'] for name in PLSDA_all})\n",
    "#accuracies\n",
    "\n",
    "accuracy_stats_PLSDA = pd.DataFrame({'Average accuracy': accuracies.mean(axis=0),\n",
    "                               'STD': accuracies.std(axis=0)})\n",
    "accuracy_stats_PLSDA = accuracy_stats_PLSDA.assign(treatment=[PLSDA_all[name]['treatment'] for name in PLSDA_all])\n",
    "accuracy_stats_PLSDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3d9d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# F1-scores across the iterations\n",
    "F1s = pd.DataFrame({name: PLSDA_all[name]['F1-scores'] for name in PLSDA_all})\n",
    "\n",
    "F1s_stats_PLSDA = pd.DataFrame({'Average accuracy': F1s.mean(axis=0),\n",
    "                               'STD': F1s.std(axis=0)})\n",
    "F1s_stats_PLSDA = F1s_stats_PLSDA.assign(treatment=[PLSDA_all[name]['treatment'] for name in PLSDA_all])\n",
    "F1s_stats_PLSDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7969cfac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot RF results\n",
    "p4 = sns.color_palette('tab20', 9)\n",
    "with sns.axes_style(\"whitegrid\"):\n",
    "    with sns.plotting_context(\"notebook\", font_scale=1.3):\n",
    "        f, (axl, axr) = plt.subplots(1, 2, figsize=(16, 4))\n",
    "        x = 1  # the label locations\n",
    "        width = 0.17  # the width of the bars\n",
    "        for i, treatment in enumerate(dfs.keys()):\n",
    "            acc_treatment = accuracy_stats_RF[accuracy_stats_RF['treatment']==treatment]\n",
    "            offset = - 0.5 + i * 0.2\n",
    "            rects = axl.bar(x + offset, acc_treatment['Average accuracy'], width, label=treatment, color = p4[i])\n",
    "            axl.errorbar(x + offset, y=acc_treatment['Average accuracy'], yerr=acc_treatment['STD'],\n",
    "                        ls='none', ecolor='0.2', capsize=3)   \n",
    "        axl.set_xticks([1-0.5 +i*0.2 for i in range(len(dfs.keys()))])\n",
    "        axl.set_xticklabels(dfs.keys())\n",
    "        axl.set(ylabel='Average accuracy', title='', ylim=(0.4,1.02))\n",
    "        axl.legend(loc='upper left', bbox_to_anchor=(0.75, 0.85))\n",
    "        axl.set_title('Random Forest')\n",
    "        \n",
    "        for i, treatment in enumerate(dfs.keys()):\n",
    "            acc_treatment = accuracy_stats_PLSDA[accuracy_stats_PLSDA['treatment']==treatment]\n",
    "            offset = - 0.5 + i * 0.2\n",
    "            rects = axr.bar(x + offset, acc_treatment['Average accuracy'], width, label=treatment, color = p4[i])\n",
    "            axr.errorbar(x + offset, y=acc_treatment['Average accuracy'], yerr=acc_treatment['STD'],\n",
    "                        ls='none', ecolor='0.2', capsize=3)   \n",
    "        axr.set_xticks([1-0.5 +i*0.2 for i in range(len(dfs.keys()))])\n",
    "        axr.set_xticklabels(dfs.keys())\n",
    "        axr.set(ylabel='Average accuracy', title='', ylim=(0.4,1.02))\n",
    "        axr.legend(loc='upper left', bbox_to_anchor=(0.75, 0.85))\n",
    "        axr.set_title('PLS-DA')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f085fd4a",
   "metadata": {},
   "source": [
    "### Univariate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e174335",
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_results = {}\n",
    "uni_results_filt = {}\n",
    "for sep in dfs:\n",
    "    uni_results[sep] = ma.compute_FC_pvalues_2groups(dfs[sep], dfs[sep],\n",
    "                               labels=lbls_orig[sep],\n",
    "                               equal_var=True,\n",
    "                               alpha=0.05, useMW=False)\n",
    "    uni_results_filt[sep] = uni_results[sep].iloc[:40]\n",
    "    #print(uni_results_filt[sep].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d6e165",
   "metadata": {},
   "source": [
    "# Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8c7da8",
   "metadata": {},
   "source": [
    "## Creating Imbalanced Datasets\n",
    "\n",
    "The six synthetic datasets have two balanced datasets. Since both classes are similar in terms of heterogeneity, we will only use one of them as the minority class - the class '1'.\n",
    "\n",
    "Then, for each synthetic dataset:\n",
    "\n",
    "We split it in 5 different ways where each had 80 samples of the majority class in that case and 20 samples of the minority class in the training set. Thus, this left 20 samples of the majority and 80 of the minority class to be the test sets. This was made by putting the set of 100 samples of a class into 5 folds of 20, combining 4 for the majority class for the training set. Training set was Pareto scaled and on the test set we performed a 'faux' Pareto scaling using the features standard deviation and mean of the training set since the training and test sets have a vastly different balance of class samples. Thus, feature averages and standard deviations can be quite different between them, especially in key features for discrimination. To compensate for this, the ‘faux’ Pareto scaling was applied. \n",
    "\n",
    "The untreated training sets were linearly augmented, which was then treated (using a normal Pareto scaling, in this case), to generate samples to train the CWGAN-GP models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e19bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(7519)\n",
    "\n",
    "# This is useless but the rng seed was set before this so this has to be ran so the others remain the same\n",
    "permutations = {}\n",
    "for cl in ordered_labels:\n",
    "    permutations[cl] = list(rng.permutation(np.where(np.array(lbls[0.6]) == cl)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74ab610",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_storage_train = {}\n",
    "df_storage_test = {}\n",
    "lbl_storage_train = {}\n",
    "lbl_storage_test = {}\n",
    "real_samples = {}\n",
    "permutations = {}\n",
    "\n",
    "for sep in dfs:\n",
    "    fold_len = 100//5\n",
    "    df_storage_train[sep] = {}\n",
    "    df_storage_test[sep] = {}\n",
    "    lbl_storage_train[sep] = {}\n",
    "    lbl_storage_test[sep] = {}\n",
    "    real_samples[sep] = {}\n",
    "    permutations[sep] = {}\n",
    "    \n",
    "    # Select the samples which will be in the imbalanced and in the test set\n",
    "    for cl in ordered_labels:\n",
    "        permutations[sep][cl] = list(rng.permutation(np.where(np.array(lbls[sep]) == cl)[0]))\n",
    "    \n",
    "    for i in range(5):\n",
    "        train_idxs = {'1':[], '0':[]}\n",
    "        test_idxs = {'1':[], '0':[]}\n",
    "\n",
    "        for cl in ordered_labels:\n",
    "            if cl == '1':\n",
    "                train_idxs[cl] = list(np.array(permutations[sep][cl])[i*fold_len: (i+1)*fold_len])\n",
    "                test_idxs[cl] = list(np.array(permutations[sep][cl])[: i*fold_len]) + list(\n",
    "                    np.array(permutations[sep][cl])[(i+1)*fold_len:])\n",
    "            else:\n",
    "                train_idxs[cl] = list(np.array(permutations[sep][cl])[: i*fold_len]) + list(\n",
    "                    np.array(permutations[sep][cl])[(i+1)*fold_len:])\n",
    "                test_idxs[cl] = list(np.array(permutations[sep][cl])[i*fold_len: (i+1)*fold_len])\n",
    "\n",
    "        print('Synthetic Dataset:', sep, 'Fold nº:', i+1)\n",
    "        print('Train 1/0:', len(train_idxs['1']), len(train_idxs['0']))\n",
    "        print('Test 1/0: ', len(test_idxs['1']), len(test_idxs['0']))\n",
    "        train_idxs = train_idxs['1'] + train_idxs['0']\n",
    "        test_idxs = test_idxs['1'] + test_idxs['0']\n",
    "        \n",
    "        # Create the imbalanced and test set\n",
    "        df_storage_train[sep][i+1] = dfs_no_t[sep].iloc[train_idxs]\n",
    "        lbl_storage_train[sep][i+1] = list(np.array(lbls[sep])[train_idxs])\n",
    "\n",
    "        df_storage_test[sep][i+1] = dfs_no_t[sep].iloc[test_idxs]\n",
    "        lbl_storage_test[sep][i+1] = list(np.array(lbls[sep])[test_idxs])\n",
    "\n",
    "        # Data pretreatment of the imbalanced and test dataset\n",
    "        real_samples[sep][i+1] = transf.pareto_scale(df_storage_train[sep][i+1])\n",
    "\n",
    "        df_storage_test[sep][i+1] = (df_storage_test[sep][i+1] - df_storage_train[\n",
    "            sep][i+1].mean())/np.sqrt(df_storage_train[sep][i+1].std()) # 'Faux' Pareto Scale"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975b9299",
   "metadata": {},
   "source": [
    "Linear Augmentation of the Training Sets and Pareto Scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce8f617",
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_df_storage_train = {}\n",
    "aug_lbl_storage_train = {}\n",
    "# Only generation of samples based on the imbalanced dataset\n",
    "for sep in dfs:\n",
    "    aug_df_storage_train[sep] = {}\n",
    "    aug_lbl_storage_train[sep] = {}\n",
    "    for i in range(1,6):#df_storage_train.keys():\n",
    "        start = perf_counter()\n",
    "        data, lbls = laf.artificial_dataset_generator(df_storage_train[sep][i], labels=lbl_storage_train[sep][i],\n",
    "                                            max_new_samples_per_label=512, binary=False, \n",
    "                                            rnd=list(np.linspace(0.2,0.8,3)), \n",
    "                                            binary_rnd_state=None, rnd_state=42345)\n",
    "\n",
    "        data_treated = transf.pareto_scale(data)\n",
    "\n",
    "        aug_df_storage_train[sep][i] = data_treated.copy()\n",
    "        aug_lbl_storage_train[sep][i] = lbls\n",
    "\n",
    "        end = perf_counter()\n",
    "        print(f'Simple augmentation of data done! Dataset: {sep}, Fold: {i}. Took {(end - start):.3f} s')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770c784c",
   "metadata": {},
   "source": [
    "Set up colours for each of the classes. Generated samples will have the corresponding label with '- GAN' after."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f3b2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colors to use in plots\n",
    "colours2 = sns.color_palette('tab20', 4)#[:6]\n",
    "\n",
    "ordered_labels_test = []\n",
    "for i in ['1', '0']:\n",
    "    ordered_labels_test.extend([i, i + ' - GAN'])\n",
    "label_colors_test = {lbl: c for lbl, c in zip(ordered_labels_test, colours2)}\n",
    "\n",
    "sns.palplot(label_colors_test.values())\n",
    "new_ticks_test = plt.xticks(range(len(ordered_labels_test)), ordered_labels_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0520e74a",
   "metadata": {},
   "source": [
    "## Conditional Wasserstein GAN - GP model\n",
    "\n",
    "This model construction was made by joining WGAN-GP models with Conditional GAN models. WGAN-GP models were originally made according to / originally based in https://keras.io/examples/generative/wgan_gp/#wasserstein-gan-wgan-with-gradient-penalty-gp and Conditional GAN models - https://machinelearningmastery.com/how-to-develop-a-conditional-generative-adversarial-network-from-scratch/ (generator and discriminator model) and https://keras.io/examples/generative/conditional_gan/ without using OOP (loss functions and training/training steps)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f5a8bf",
   "metadata": {},
   "source": [
    "Functions for the generator and critic (discriminator) models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43aa5021",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_model(len_input, len_output, n_hidden_nodes, n_labels): \n",
    "    \"Make the generator model of CWGAN-GP.\"\n",
    "\n",
    "    data_input = tf.keras.Input(shape=(len_input,), name='data') # Take intensity input\n",
    "    label_input = tf.keras.Input(shape=(1,), name='label') # Take Label Input\n",
    "    \n",
    "    # Treat label input to concatenate to intensity data after\n",
    "    label_m = tf.keras.layers.Embedding(n_labels, 30, input_length=1)(label_input)\n",
    "    label_m = tf.keras.layers.Dense(256, activation='linear', use_bias=True)(label_m)\n",
    "    #label_m = tf.keras.layers.Reshape((len_input,1,))(label_m)\n",
    "    label_m2 = tf.keras.layers.Reshape((256,))(label_m)\n",
    "\n",
    "    joined_data = tf.keras.layers.Concatenate()([data_input, label_m2]) # Concatenate intensity and label data\n",
    "    # Hidden Dense Layer and Normalization\n",
    "    joined_data = tf.keras.layers.Dense(n_hidden_nodes, activation=tf.nn.leaky_relu, use_bias=True)(joined_data)\n",
    "    joined_data = tf.keras.layers.Dense(256, activation=tf.nn.leaky_relu, use_bias=True)(joined_data)\n",
    "    joined_data = tf.keras.layers.BatchNormalization()(joined_data)\n",
    "    \n",
    "    # Output - number of features of sample to make\n",
    "    output = tf.keras.layers.Dense(len_output, activation='linear', use_bias=True)(joined_data)\n",
    "    \n",
    "    generator = tf.keras.Model(inputs=[data_input, label_input], outputs=output)\n",
    "    \n",
    "    return generator\n",
    "\n",
    "\n",
    "def critic_model(len_input, n_hidden_nodes, n_labels):\n",
    "    \"Make the critic model of CWGAN-GP.\"\n",
    "    \n",
    "    label_input = tf.keras.Input(shape=(1,)) # Take intensity input\n",
    "    data_input = tf.keras.Input(shape=(len_input,)) # Take Label Input\n",
    "\n",
    "    # Treat label input to concatenate to intensity data after\n",
    "    label_m = tf.keras.layers.Embedding(n_labels, 30, input_length=1)(label_input)\n",
    "    label_m = tf.keras.layers.Dense(256, activation='linear', use_bias=True)(label_m)\n",
    "    #label_m = tf.keras.layers.Reshape((len_input,1,))(label_m)\n",
    "    label_m = tf.keras.layers.Reshape((256,))(label_m)\n",
    "\n",
    "    joined_data = tf.keras.layers.Concatenate()([data_input, label_m]) # Concatenate intensity and label data\n",
    "    # Hidden Dense Layer (Normalization worsened results here)\n",
    "    joined_data = tf.keras.layers.Dense(n_hidden_nodes, activation=tf.nn.leaky_relu, use_bias=True)(joined_data)\n",
    "    joined_data = tf.keras.layers.Dense(128, activation=tf.nn.leaky_relu, use_bias=True)(joined_data)\n",
    "    joined_data = tf.keras.layers.Dense(256, activation=tf.nn.leaky_relu, use_bias=True)(joined_data)\n",
    "    #joined_data = tf.keras.layers.BatchNormalization()(joined_data)\n",
    "\n",
    "    # Output Layer - 1 node for critic decision\n",
    "    output = tf.keras.layers.Dense(1, activation='linear', use_bias=True)(joined_data)\n",
    "    \n",
    "    critic = tf.keras.Model(inputs=[data_input, label_input], outputs=output)\n",
    "\n",
    "    return critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f1c898",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_predictions(model, num_examples_to_generate, len_input, input_dist, uni_lbls):\n",
    "    \"Generate sample predictions based on a Generator model.\"\n",
    "    \n",
    "    test_input =  tf.constant(input_dist.rvs(size=len_input*num_examples_to_generate), shape=[\n",
    "        num_examples_to_generate,len_input]) \n",
    "    \n",
    "    if len(uni_lbls) < 3:\n",
    "        test_labels = tf.constant([1.0]*(num_examples_to_generate//2) + [0.0]*(num_examples_to_generate//2), \n",
    "                                  shape=(num_examples_to_generate,1))\n",
    "    else:\n",
    "        test_labels = []\n",
    "        for i in range(len(uni_lbls)):\n",
    "            test_labels.extend([i]*(num_examples_to_generate//len(uni_lbls)))\n",
    "        test_labels = np.array(pd.get_dummies(test_labels))\n",
    "        #np.array(pd.get_dummies([i for i in range(len(uni_lbls))]*(num_examples_to_generate//len(uni_lbls))))\n",
    "    predictions = model([test_input, test_labels], training=False) # `training` is set to False.\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d29dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_montage(train_data_o, train_lbls, test_data, test_lbls,\n",
    "                     epochs, generator, critic, generator_optimizer, critic_optimizer, input_dist,\n",
    "                    batch_size, grad_pen_weight=10, k_cov_den=50, k_crossLID=15, random_seed=145,\n",
    "                    n_generated_samples=96):\n",
    "    \"\"\"Train a generator and critic of CWGAN-GP.\n",
    "    \n",
    "       Receives training data and respective class labels (train_data_o and train_lbls) and trains a generator and a critic\n",
    "        model (generator, critic) over a number of epochs (epochs) with a set batch size (batch_size) with the respective \n",
    "        optimizers and learning rate (generator_optimizer, critic_optimizer). Gradient Penalty is calculated with\n",
    "        grad_pen_weight as the weight of the penalty.\n",
    "       The functions returns at time intervals three graphs to evaluate the progression of the models (Loss plots,\n",
    "        coverage, density, crossLID and correct first cluster plots and PCA plot with generated and test data). To this\n",
    "        end, samples need to be generated requiring the distribution to sample the initial input values from (input_dist),\n",
    "        and test data and respective labels has to be given (test_data and test_lbls). Finally the number of neighbors to\n",
    "        consider for coverage/density and crossLID calculation is also needed (k_cov_den, k_crossLID).\n",
    "    \n",
    "       train_data_o: Pandas DataFrame with training data;\n",
    "       train_lbls: List with training data class labels;\n",
    "       test_data: Pandas DataFrame with test data to evaluate the model;\n",
    "       test_lbls: List with test data class labels to evaluate the model;\n",
    "       epochs: Int value with the number of epochs to train the model;\n",
    "       generator: tensorflow keras.engine.functional.Functional model for the generator;\n",
    "       critic: tensorflow keras.engine.functional.Functional model for the critic;\n",
    "       generator_optimizer: tensorflow keras optimizer (with learning rate) for generator;\n",
    "       critic_optimizer: tensorflow keras optimizer (with learning rate) for critic;\n",
    "       input_dist: scipy.stats._continuous_distns.rv_histogram object - distribution to sample input values for generator;\n",
    "       batch_size: int value with size of batch for model training;\n",
    "       grad_pen_weight: int value (default 10) for penalty weight in gradient penalty calculation;\n",
    "       k_cov_den: int value (default 50) for number of neighbors to consider for coverage and density calculation in\n",
    "       generated samples evaluation;\n",
    "       k_crossLID: int value (default 15) for number of neighbors to consider for crossLID calculation in generated samples\n",
    "        evaluation.\n",
    "       random_seed: int value (default 145) for numpy random seeding when randomly organizing samples in the data that\n",
    "        will be split into batches.\n",
    "       n_generated_samples: int value (default 96) for number of samples generated to test the model during training.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Obtaining the train data, randomize its order and divide it be twice the standard deviation of its values\n",
    "    all_data = train_data_o.iloc[\n",
    "        np.random.RandomState(seed=random_seed).permutation(len(train_data_o))]/(2*train_data_o.values.std())\n",
    "    \n",
    "    # Same treatment for the test data\n",
    "    test_data = (test_data/(2*test_data.values.std())).values\n",
    "    training_data = all_data\n",
    "    train_data = all_data.values\n",
    "    \n",
    "    # Change class labels to numerical values while following the randomized ordered of samples\n",
    "    if len(set(train_lbls)) < 3: # 1 and 0 for when there are only two classes\n",
    "        train_labels = pd.get_dummies(\n",
    "            np.array(train_lbls)[np.random.RandomState(seed=random_seed).permutation(len(train_data))]).values[:,0]\n",
    "        test_labels = pd.get_dummies(np.array(test_lbls)).values[:,0]\n",
    "    else: # One hot encoding for when there are more than two classes\n",
    "        train_labels = pd.get_dummies(\n",
    "            np.array(train_lbls)[np.random.RandomState(seed=random_seed).permutation(len(train_data))]).values\n",
    "        test_labels = pd.get_dummies(np.array(test_lbls)).values\n",
    "    # Save the order of the labels\n",
    "    ordered_labels = pd.get_dummies(\n",
    "            np.array(train_lbls)[np.random.RandomState(seed=random_seed).permutation(len(train_data_o))]).columns\n",
    "\n",
    "    batch_divisions = int(batch_size / len(set(train_lbls))) # See how many samples of each class will be in each batch\n",
    "    n_steps = epochs * int(training_data.shape[0] / batch_size) # Number of steps: nº of batches per epoch * nº of epochs\n",
    "    n_critic = 5\n",
    "    \n",
    "    # Set up the evaluating images printed during training and the intervals they will be updated\n",
    "    f, (axl, axc, axr) = plt.subplots(1, 3, figsize = (16,5))\n",
    "    update1 = n_steps//200\n",
    "    update2 = n_steps//20\n",
    "\n",
    "    if hasattr(tqdm, '_instances'):\n",
    "        tqdm._instances.clear() # clear if it exists\n",
    "\n",
    "    i=0\n",
    "\n",
    "    for step in tqdm(range(n_steps)):\n",
    "        \n",
    "        # Critic Training\n",
    "        crit_loss_temp = []\n",
    "        \n",
    "        # Select real samples for this batch on training and order samples to put samples of the same class together\n",
    "        real_samp = train_data[i*batch_size:(i+1)*batch_size]\n",
    "        real_lbls = train_labels[i*batch_size:(i+1)*batch_size]\n",
    "\n",
    "        real_samples = np.empty(real_samp.shape)\n",
    "        real_labels = np.empty(real_lbls.shape)\n",
    "        a = 0\n",
    "        if len(set(train_lbls)) < 3:\n",
    "            for l,s in sorted(zip(real_lbls, real_samp), key=lambda pair: pair[0], reverse=True):\n",
    "                real_samples[a] = s\n",
    "                real_labels[a] = l\n",
    "                a = a+1\n",
    "        else:\n",
    "            for l,s in sorted(zip(real_lbls, real_samp), key=lambda pair: np.argmax(pair[0]), reverse=False):\n",
    "                #print(l, np.argmax(l))\n",
    "                real_samples[a] = s\n",
    "                real_labels[a] = l\n",
    "                a = a+1\n",
    "\n",
    "        for _ in range(n_critic): # For each step, train critic n_critic times\n",
    "            \n",
    "            # Generate input for generator\n",
    "            artificial_samples = tf.constant(input_dist.rvs(size=all_data.shape[1]*batch_size), shape=[\n",
    "                batch_size,all_data.shape[1]])\n",
    "            artificial_labels = real_labels.copy()\n",
    "\n",
    "            # Generate artificial samples from the latent vector\n",
    "            artificial_samples = generator([artificial_samples, artificial_labels], training=True)\n",
    "            \n",
    "            with tf.GradientTape() as crit_tape: # See the gradient for the critic\n",
    "\n",
    "                # Get the logits for the generated samples\n",
    "                X_artificial = critic([artificial_samples, artificial_labels], training=True)\n",
    "                # Get the logits for the real samples\n",
    "                X_true = critic([real_samples, real_labels], training=True)\n",
    "\n",
    "                # Calculate the critic loss using the generated and real sample results\n",
    "                c_cost = critic_loss_wgan(X_true, X_artificial)\n",
    "\n",
    "                # Calculate the gradient penalty\n",
    "                grad_pen = gradient_penalty_cwgan(batch_size, real_samples, artificial_samples,\n",
    "                                                  real_labels, artificial_labels, critic)\n",
    "                # Add the gradient penalty to the original discriminator loss\n",
    "                crit_loss = c_cost + grad_pen * grad_pen_weight\n",
    "                \n",
    "            crit_loss_temp.append(crit_loss)\n",
    "\n",
    "            # Calculate and apply the gradients obtained from the loss on the trainable variables\n",
    "            gradients_of_critic = crit_tape.gradient(crit_loss, critic.trainable_variables)\n",
    "            critic_optimizer.apply_gradients(zip(gradients_of_critic, critic.trainable_variables))\n",
    "\n",
    "        i = i + 1\n",
    "        if (step+1) % (n_steps//epochs) == 0:\n",
    "            i=0\n",
    "\n",
    "        crit_loss_all.append(np.mean(crit_loss_temp))\n",
    "        \n",
    "        # Generator Training\n",
    "        # Generate inputs for generator, values and labels\n",
    "        artificial_samples = tf.constant(input_dist.rvs(size=all_data.shape[1]*batch_size), shape=[\n",
    "                batch_size,all_data.shape[1]])\n",
    "        \n",
    "        if len(set(train_lbls)) < 3:\n",
    "            artificial_labels = tf.constant([1.0]*(batch_size//2) + [0.0]*(batch_size//2), shape=(batch_size,1))\n",
    "        else:\n",
    "            artificial_labels = np.array(pd.get_dummies([i for i in range(len(set(train_lbls)))]*batch_divisions))\n",
    "    \n",
    "        with tf.GradientTape() as gen_tape: # See the gradient for the generator\n",
    "            # Generate artificial samples\n",
    "            artificial_samples = generator([artificial_samples, artificial_labels], training=True)\n",
    "            \n",
    "            # Get the critic results for generated samples\n",
    "            X_artificial = critic([artificial_samples, artificial_labels], training=True)\n",
    "            # Calculate the generator loss\n",
    "            gen_loss = generator_loss_wgan(X_artificial)\n",
    "\n",
    "        # Calculate and apply the gradients obtained from the loss on the trainable variables\n",
    "        gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "        generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "        gen_loss_all.append(gen_loss)\n",
    "\n",
    "        # Update the progress bar and evaluation graphs every update1 steps for loss plots and update2 for the others.\n",
    "        if (step + 1) % update1 == 0:\n",
    "            \n",
    "            # Update the evaluating figures at the set intervals\n",
    "            axl.clear() # Always clear the corresponding ax before redrawing it\n",
    "            \n",
    "            # Loss Plot\n",
    "            axl.plot(gen_loss_all, color = 'blue', label='Generator Loss')\n",
    "            axl.plot(crit_loss_all,color = 'red', label='Critic Loss')\n",
    "            axl.set_xlabel('Number of Steps')\n",
    "            axl.set_ylabel('Loss')\n",
    "            axl.legend()\n",
    "            \n",
    "            ipythondisplay.clear_output(wait=True)\n",
    "            ipythondisplay.display(plt.gcf())\n",
    "\n",
    "        if (step + 1) % update2 == 0:\n",
    "\n",
    "            saved_predictions.append(generate_predictions(generator, n_generated_samples, all_data.shape[1], \n",
    "                                                          input_realdata_dist, ordered_labels))\n",
    "            # See density and coverage and crossLID (divided by 25 to be in the same order as the rest) \n",
    "            # of latest predictions\n",
    "            den, cov = gem.evaluation_coverage_density(test_data, saved_predictions[-1], k= k_cov_den, metric='euclidean')\n",
    "            clid = gem.cross_LID_estimator_byMLE(test_data, saved_predictions[-1], k=k_crossLID, metric='euclidean')/25\n",
    "            density.append(den)\n",
    "            coverage.append(cov)\n",
    "            crossLID.append(clid)\n",
    "\n",
    "            # PCA of the latest predictions and training data\n",
    "            # Divide by twice the standard deviation to be the same as the generated data\n",
    "            dfs_temp = pd.concat((train_data_o/(2*train_data_o.values.std()),pd.DataFrame(\n",
    "                saved_predictions[-1].numpy(), columns=train_data_o.columns))) \n",
    "            temp_lbls = train_lbls.copy()\n",
    "            for l in ordered_labels:\n",
    "                temp_lbls.extend([l+' - GAN']*(n_generated_samples//len(ordered_labels)))\n",
    "            principaldf = gem.pca_sample_projection(dfs_temp, temp_lbls, pca, whiten=True, \n",
    "                                                samp_number=len(train_data_o.index))\n",
    "            lcolors = label_colors_test\n",
    "\n",
    "            # Hierarchical clustering of the latest predictions and testing data, \n",
    "            # saving the correct 1st cluster fraction results\n",
    "            dfs_temp = np.concatenate((test_data, saved_predictions[-1].numpy()))\n",
    "            temp_lbls = ['real']*len(test_data) + ['gen']*len(saved_predictions[-1])\n",
    "            hca_results = gem.perform_HCA(dfs_temp, temp_lbls, metric='euclidean', method='ward')\n",
    "            corr1stcluster.append(hca_results['correct 1st clustering'])\n",
    "            \n",
    "            # Plots\n",
    "            axc.clear()\n",
    "            axc.plot(range(update2, step+2, update2), coverage, label='coverage')\n",
    "            axc.plot(range(update2, step+2, update2), density, label='density')\n",
    "            axc.plot(range(update2, step+2, update2), crossLID, color='red', label='crossLID')\n",
    "            axc.plot(range(update2, step+2, update2), corr1stcluster, color='purple', label='corr_cluster')\n",
    "            axc.legend()\n",
    "\n",
    "            axr.clear()\n",
    "            gem.plot_PCA(principaldf, lcolors, components=(1,2), title='', ax=axr)\n",
    "            axr.legend(loc='upper right', ncol=1, framealpha=1)\n",
    "            \n",
    "            ipythondisplay.clear_output(wait=True)\n",
    "            ipythondisplay.display(plt.gcf())\n",
    "            print(lbl, fold)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59bae48a",
   "metadata": {},
   "source": [
    "### Training the GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9878f258",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "GENERATE=True\n",
    "epochs = 500\n",
    "batch_size = 32\n",
    "k_cov_den = 20\n",
    "k_crossLID = 15\n",
    "random_seed = 145\n",
    "n_generated_samples = 48*len(pd.unique(aug_lbl_storage_train[0.6][1]))\n",
    "\n",
    "if GENERATE:\n",
    "    generator_train = {}\n",
    "    critic_train = {}\n",
    "\n",
    "    results_train = {}\n",
    "\n",
    "    for lbl in real_samples.keys():\n",
    "        generator_train[lbl] = {}\n",
    "        critic_train[lbl] = {}\n",
    "\n",
    "        results_train[lbl] = {}\n",
    "        for fold in real_samples[lbl].keys():\n",
    "\n",
    "            print(lbl, fold)\n",
    "            # Store results\n",
    "            gen_loss_all = []\n",
    "            crit_loss_all = []\n",
    "            saved_predictions = []\n",
    "            coverage = []\n",
    "            density = []\n",
    "            crossLID = []\n",
    "            corr1stcluster = []\n",
    "            \n",
    "            # Get distribution of intensity values of the dataset\n",
    "            hist = np.histogram(real_samples[lbl][fold].values.flatten(), bins=100)\n",
    "            input_realdata_dist = stats.rv_histogram(hist)\n",
    "\n",
    "            df = real_samples[lbl][fold]\n",
    "            pca = PCA(n_components=2, svd_solver='full', whiten=True)\n",
    "            pc_coords = pca.fit_transform(df)\n",
    "\n",
    "            generator_optimizer = tf.keras.optimizers.RMSprop(1e-4)\n",
    "            critic_optimizer = tf.keras.optimizers.RMSprop(1e-4)\n",
    "\n",
    "            generator_train[lbl][fold] = generator_model(aug_df_storage_train[lbl][fold].shape[1],\n",
    "                                                 aug_df_storage_train[lbl][fold].shape[1], 128, 2)\n",
    "            critic_train[lbl][fold] = critic_model(aug_df_storage_train[lbl][fold].shape[1], 512, 2)\n",
    "\n",
    "            training_montage(aug_df_storage_train[lbl][fold], aug_lbl_storage_train[lbl][fold],\n",
    "                             real_samples[lbl][fold], lbl_storage_train[lbl][fold],\n",
    "                             epochs, generator_train[lbl][fold], critic_train[lbl][fold],\n",
    "                             generator_optimizer, critic_optimizer, input_realdata_dist, batch_size,\n",
    "                             grad_pen_weight=10, k_cov_den=k_cov_den, k_crossLID=k_crossLID,\n",
    "                             random_seed=random_seed, n_generated_samples=n_generated_samples)\n",
    "\n",
    "            results_train[lbl][fold]={'gen_loss': gen_loss_all, 'crit_loss': crit_loss_all, 'saved_pred': saved_predictions,\n",
    "                     'coverage': coverage, 'density': density, 'crossLID': crossLID, 'corr1st_cluster': corr1stcluster}\n",
    "            \n",
    "            generator_train[lbl][fold].save_weights('gan_models/Synthetic_gen_imb_'+str(lbl)+str(fold))\n",
    "            critic_train[lbl][fold].save_weights('gan_models/Synthetic_crit_imb_'+str(lbl)+str(fold))\n",
    "    \n",
    "            # Save the results from GAN training\n",
    "            with open('gan_models/Synthetic_results_imb_'+str(lbl)+str(fold)+'.pickle', 'wb') as handle:\n",
    "                pickle.dump(results_train[lbl][fold], handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2ab5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not GENERATE:\n",
    "\n",
    "    generator_train = {}\n",
    "    critic_train = {}\n",
    "\n",
    "    results_train = {}\n",
    "\n",
    "    for lbl in df_storage_train:\n",
    "        generator_train[lbl] = {}\n",
    "        critic_train[lbl] = {}\n",
    "\n",
    "        results_train[lbl] = {}\n",
    "        for fold in df_storage_train[lbl]:\n",
    "            # Read back the saved model\n",
    "            generator_optimizer = tf.keras.optimizers.RMSprop(1e-4)\n",
    "            critic_optimizer = tf.keras.optimizers.RMSprop(1e-4)\n",
    "\n",
    "            generator_train[lbl][fold] = generator_model(df_storage_train[lbl][fold].shape[1],\n",
    "                                                         df_storage_train[lbl][fold].shape[1], 128, 2)\n",
    "            critic_train[lbl][fold] = critic_model(df_storage_train[lbl][fold].shape[1],\n",
    "                                                   512, 2)\n",
    "\n",
    "            # Load previously saved models\n",
    "            generator_train[lbl][fold].load_weights('./gan_models/Synthetic_gen_imb_'+str(lbl)+str(fold))\n",
    "            critic_train[lbl][fold].load_weights('./gan_models/Synthetic_crit_imb_'+str(lbl)+str(fold))\n",
    "            \n",
    "            # Load previously saved results\n",
    "            with open('gan_models/Synthetic_results_imb_'+str(lbl)+str(fold)+'.pickle', 'rb') as handle:\n",
    "                results_train[lbl][fold] = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0084f3d3",
   "metadata": {},
   "source": [
    "### Comparison of Classification Accuracy\n",
    "\n",
    "With the train set, we build and train a GAN model from them. Then we build models with the train set and with generated samples from the GAN models and compare the performance in discriminating the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6c8634",
   "metadata": {},
   "source": [
    "#### Generate a lot of samples and make Random Forests and PLS-DA models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd66945",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(5402)\n",
    "# Generate sample for each fold\n",
    "generated_samples = {}\n",
    "\n",
    "for i in generator_train:\n",
    "    generated_samples[i] = {}\n",
    "    for fold in generator_train[i]:\n",
    "        # Input to the generator\n",
    "        num_examples_to_generate = 2048\n",
    "        # Get distribution of intensity values of the dataset\n",
    "        hist = np.histogram(real_samples[i][fold].values.flatten(), bins=100)\n",
    "        input_realdata_dist = stats.rv_histogram(hist)\n",
    "\n",
    "        test_input = tf.constant(input_realdata_dist.rvs(\n",
    "            size=len(df_storage_train[i][fold].columns)*num_examples_to_generate),\n",
    "                                 shape=[num_examples_to_generate,len(df_storage_train[i][fold].columns)])\n",
    "\n",
    "        test_labels = tf.constant([0]*(num_examples_to_generate//2) + [1]*(num_examples_to_generate//2), shape=[\n",
    "            num_examples_to_generate,1])\n",
    "\n",
    "        # Generate GAN samples\n",
    "        predictions = generator_train[i][fold]([test_input, test_labels], training=False)\n",
    "        # Reverse the division done to the data\n",
    "        predictions = predictions * 2* aug_df_storage_train[i][fold].values.std()\n",
    "        \n",
    "        ordered_labels_fold = pd.get_dummies(\n",
    "            np.array(lbl_storage_train[i][fold])[np.random.RandomState(seed=random_seed).permutation(\n",
    "                len(lbl_storage_train[i][fold]))]).columns\n",
    "\n",
    "        generated_samples[i][fold] = [pd.DataFrame(np.array(predictions), columns=df_storage_train[i][fold].columns),\n",
    "                                [ordered_labels_fold[1],]*(num_examples_to_generate//2) + [ordered_labels_fold[0],]*(\n",
    "                                num_examples_to_generate//2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8e135e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To store for each fold\n",
    "bal_datasets = {}\n",
    "np.random.seed(325)\n",
    "rng = np.random.default_rng(7519)\n",
    "for i in generated_samples.keys():\n",
    "    bal_datasets[i] = {}\n",
    "    for fold in generator_train[i]:\n",
    "        bal_datasets[i][fold] = {}\n",
    "        df = real_samples[i][fold].loc[np.array(lbl_storage_train[i][fold]) == '1']\n",
    "        # Calculate all correlations between all samples of experimental and GAN data and store them in a dataframe\n",
    "        correlations = pd.DataFrame(index=generated_samples[i][fold][0].index, columns=df.index).astype('float')\n",
    "\n",
    "        for a in df.index:\n",
    "            for j in generated_samples[i][fold][0].index:\n",
    "                correlations.loc[j,a] = stats.pearsonr(df.loc[a],\n",
    "                                                       generated_samples[i][fold][0].loc[j])[0]\n",
    "\n",
    "        correlated_samples = pd.DataFrame(columns=df.index)\n",
    "        for a in correlations:\n",
    "            correlated_samples[a] = correlations[a].sort_values(ascending=False).index\n",
    "            \n",
    "        permutated = correlated_samples.copy()\n",
    "        for l in correlated_samples.index:\n",
    "            permutated.loc[l] = rng.permutation(correlated_samples.loc[l])\n",
    "        #print(permutated)\n",
    "\n",
    "        corr_idxs = pd.unique(permutated.values.flatten())\n",
    "        \n",
    "        dataset_len = real_samples[i][fold].shape[0]\n",
    "        \n",
    "        n_min_class = (np.array(lbl_storage_train[i][fold]) == '1').sum()\n",
    "        n_max_class = (dataset_len - n_min_class)//(len(pd.unique(lbl_storage_train[i][fold]))-1)\n",
    "        \n",
    "        # Slowly add samples - 3 at a time until a total of 60\n",
    "        for num in range(0, n_max_class - n_min_class+1, (n_max_class - n_min_class+1)//20):\n",
    "            idx_to_keep = corr_idxs[:num]\n",
    "\n",
    "            corr_preds = generated_samples[i][fold][0].loc[list(pd.unique(idx_to_keep))]\n",
    "            corr_lbls  = np.array(generated_samples[i][fold][1])[list(pd.unique(idx_to_keep))]\n",
    "\n",
    "            # Slowly add the GAN correlated GAN samples to the the imbalanced dataset, making it a balanced dataset\n",
    "            concat_df = pd.concat((corr_preds, real_samples[i][fold]))\n",
    "            # All GAN samples added are from the '1' minority class\n",
    "            concat_lbls = ['1',]*len(set(idx_to_keep)) + lbl_storage_train[i][fold]\n",
    "            bal_datasets[i][fold][num] = [concat_df.copy(), concat_lbls.copy()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb43461",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Samples added to the minority class\n",
    "bal_datasets[i][fold].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b0fa66",
   "metadata": {},
   "source": [
    "### Fitting RF and PLS-DA models to Imbalance Datasets and Evaluating them\n",
    "\n",
    "RF and PLS-DA models are built for each minority class, each fold and each number of GAN samples added.\n",
    "\n",
    "#### RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb01c9c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Fitting and storing Random Forest models for each fold\n",
    "RF_models_bal = {}\n",
    "\n",
    "# Train the Models\n",
    "for min_class in bal_datasets:\n",
    "    RF_models_bal[min_class] = {}\n",
    "    for size in bal_datasets[0.6][1].keys():\n",
    "        RF_models_bal[min_class][size] = {}\n",
    "        for fold in bal_datasets[min_class]:\n",
    "            rf_mod = ma.RF_model(bal_datasets[min_class][fold][size][0], bal_datasets[min_class][fold][size][1],\n",
    "                                 return_cv=False, n_trees=200)\n",
    "            RF_models_bal[min_class][size][fold] = rf_mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bfb2985",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Testing the RF models with the test data for each fold for real, GAN and CorrGAN Data\n",
    "RF_results_bal = {'Accuracy':{}, 'F1-Score':{}, 'Precision':{}, 'Recall':{}}\n",
    "# Evaluate the Models\n",
    "for min_class in RF_models_bal:\n",
    "    RF_results_bal['Accuracy'][min_class] = {}\n",
    "    RF_results_bal['F1-Score'][min_class] = {}\n",
    "    RF_results_bal['Precision'][min_class] = {}\n",
    "    RF_results_bal['Recall'][min_class] = {}\n",
    "    for size in RF_models_bal[min_class].keys():\n",
    "        RF_results_bal['Accuracy'][min_class][size] = {}\n",
    "        RF_results_bal['F1-Score'][min_class][size] = {}\n",
    "        RF_results_bal['Precision'][min_class][size] = {}\n",
    "        RF_results_bal['Recall'][min_class][size] = {}\n",
    "        for fold in RF_models_bal[min_class][size]:\n",
    "            RF_results_bal['Accuracy'][min_class][size][fold] = RF_models_bal[min_class][size][fold].score(\n",
    "                                                                                    df_storage_test[min_class][fold],\n",
    "                                                                                    lbl_storage_test[min_class][fold])\n",
    "            preds = RF_models_bal[min_class][size][fold].predict(df_storage_test[min_class][fold])\n",
    "            prec, rec, f1, sup = precision_recall_fscore_support(lbl_storage_test[min_class][fold], preds,\n",
    "                                                                pos_label='1', average='binary',\n",
    "                                                                zero_division=1)\n",
    "            RF_results_bal['F1-Score'][min_class][size][fold] = f1\n",
    "            RF_results_bal['Precision'][min_class][size][fold] = prec\n",
    "            RF_results_bal['Recall'][min_class][size][fold] = rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f5a583",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.from_dict(RF_results_bal['Accuracy'][0.6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a488da",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.from_dict(RF_results_bal['Accuracy'][2.0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ba3844",
   "metadata": {},
   "source": [
    "Random Forest results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec77e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the results for each fold\n",
    "f, axs = plt.subplots(2,2,figsize=(12,12))\n",
    "for min_class in RF_results_bal['Accuracy'].keys():\n",
    "    for t, ax in zip(RF_results_bal.keys(), axs.ravel()):\n",
    "        ax.plot((pd.DataFrame.from_dict(RF_results_bal[t][min_class]).columns + 20)/n_max_class*100,\n",
    "                 pd.DataFrame.from_dict(RF_results_bal[t][min_class]).mean(), \n",
    "                 label=str(min_class))\n",
    "        ax.set_title(t, fontsize=15)\n",
    "        ax.set_xlabel('% of Augmentation', fontsize=15)\n",
    "        ax.set_ylim([0, 1.01])\n",
    "\n",
    "axs[0][0].set_ylabel('Performance', fontsize=15)\n",
    "axs[1][0].set_ylabel('Performance', fontsize=15)\n",
    "axs[0][1].legend(fontsize=15, bbox_to_anchor=(1,1))\n",
    "\n",
    "plt.suptitle('Random Forest', fontsize=18, y=0.93)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fcc976a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the results for each fold\n",
    "f, axs = plt.subplots(2,2,figsize=(12,12))\n",
    "for min_class in RF_results_bal['Accuracy'].keys():\n",
    "    for t, ax in zip(RF_results_bal.keys(), axs.ravel()):\n",
    "        ax.plot((pd.DataFrame.from_dict(RF_results_bal[t][min_class]).columns + 20)/n_max_class*100,\n",
    "                 pd.DataFrame.from_dict(RF_results_bal[t][min_class]).mean() - pd.DataFrame.from_dict(\n",
    "                     RF_results_bal[t][min_class]).mean()[0], \n",
    "                 label=str(min_class))\n",
    "        ax.set_title(t, fontsize=15)\n",
    "        ax.set_xlabel('% of Augmentation', fontsize=15)\n",
    "        ax.set_ylim([-0.5, 0.5])\n",
    "\n",
    "axs[0][0].set_ylabel('Difference in Performance', fontsize=15)\n",
    "axs[1][0].set_ylabel('Difference in Performance', fontsize=15)\n",
    "axs[0][1].legend(fontsize=15, bbox_to_anchor=(1,1))\n",
    "\n",
    "plt.suptitle('Random Forest', fontsize=18, y=0.93)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfabcbc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the results for each fold\n",
    "f, axs = plt.subplots(2,2,figsize=(12,12))\n",
    "for min_class in RF_results_bal['Accuracy'].keys():\n",
    "    for t, ax in zip(RF_results_bal.keys(), axs.ravel()):\n",
    "        ax.plot((pd.DataFrame.from_dict(RF_results_bal[t][min_class]).columns + 20)/n_max_class*100,\n",
    "                 (pd.DataFrame.from_dict(RF_results_bal[t][min_class]).mean() - pd.DataFrame.from_dict(\n",
    "                     RF_results_bal[t][min_class]).mean()[0]) / (1-pd.DataFrame.from_dict(\n",
    "                     RF_results_bal[t][min_class]).mean()[0]), \n",
    "                 label=str(min_class))\n",
    "        ax.set_title(t, fontsize=15)\n",
    "        ax.set_xlabel('% of Augmentation', fontsize=15)\n",
    "        ax.set_ylim([-0.5, 1])\n",
    "\n",
    "f.supylabel('Difference in Performance (fraction based on maximum possible)', fontsize=15, x=0.07)\n",
    "axs[0][1].legend(fontsize=15, bbox_to_anchor=(1,1))\n",
    "\n",
    "plt.suptitle('Random Forest', fontsize=18, y=0.93)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6187f3",
   "metadata": {},
   "source": [
    "#### PLS-DA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da52116",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decision_rule(y_pred, y_true, pos_label, average='binary'):\n",
    "    \"Decision rule for PLS-DA classification.\"\n",
    "    # Decision rule for classification\n",
    "    # Decision rule chosen: sample belongs to group where it has max y_pred (closer to 1)\n",
    "    # In case of 1,0 encoding for two groups, round to nearest integer to compare\n",
    "    nright = 0\n",
    "    rounded = np.round(y_pred)\n",
    "\n",
    "    for p in range(len(y_pred)):\n",
    "        if rounded[p] >= 1:\n",
    "            pred = 1\n",
    "            rounded[p] = 1\n",
    "        else:\n",
    "            pred = 0\n",
    "            rounded[p] = 0\n",
    "        if pred == y_true[p]:\n",
    "            nright += 1  # Correct prediction\n",
    "    \n",
    "    # Calculate accuracy for this iteration\n",
    "    accuracy = (nright / len(y_pred))\n",
    "    prec, rec, f1, sup = precision_recall_fscore_support(y_true, rounded, pos_label=pos_label, average=average,\n",
    "                                                         zero_division=1)\n",
    "    return accuracy, f1, prec, rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c57e188",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "PLSDA_models_bal = {}\n",
    "PLSDA_results_bal = {'Accuracy':{}, 'F1-Score':{}, 'Precision':{}, 'Recall':{}}\n",
    "\n",
    "np.random.seed(325)\n",
    "\n",
    "# Train and Evaluate the models\n",
    "for min_class in bal_datasets:\n",
    "    PLSDA_models_bal[min_class] = {}\n",
    "    PLSDA_results_bal['Accuracy'][min_class] = {}\n",
    "    PLSDA_results_bal['F1-Score'][min_class] = {}\n",
    "    PLSDA_results_bal['Precision'][min_class] = {}\n",
    "    PLSDA_results_bal['Recall'][min_class] = {}\n",
    "    for size in bal_datasets[min_class][1].keys():\n",
    "        PLSDA_models_bal[min_class][size] = {}\n",
    "        PLSDA_results_bal['Accuracy'][min_class][size] = {}\n",
    "        PLSDA_results_bal['F1-Score'][min_class][size] = {}\n",
    "        PLSDA_results_bal['Precision'][min_class][size] = {}\n",
    "        PLSDA_results_bal['Recall'][min_class][size] = {}\n",
    "        for fold in bal_datasets[min_class]:\n",
    "\n",
    "            PLSDA_models_bal[min_class][size][fold] = ma.fit_PLSDA_model(bal_datasets[min_class][fold][size][0],\n",
    "                                                                   bal_datasets[min_class][fold][size][1],\n",
    "                                                              n_comp=4,\n",
    "                                                      return_scores=False, scale=False, encode2as1vector=True)\n",
    "            plsda = PLSDA_models_bal[min_class][size][fold]\n",
    "            # Obtain results with the test group\n",
    "            y_pred = plsda.predict(df_storage_test[min_class][fold])\n",
    "            y_true = ma._generate_y_PLSDA(lbl_storage_test[min_class][fold],\n",
    "                                          pd.unique(bal_datasets[min_class][fold][size][1]),\n",
    "                                          True)\n",
    "            pos_label = np.where(pd.unique(bal_datasets[min_class][fold][size][1]) != '1')[0][0]\n",
    "            # Calculate accuracy\n",
    "            accuracy, f1, prec, rec = decision_rule(y_pred, y_true, pos_label=pos_label, average='binary')\n",
    "            PLSDA_results_bal['Accuracy'][min_class][size][fold] = accuracy\n",
    "            PLSDA_results_bal['F1-Score'][min_class][size][fold] = f1\n",
    "            PLSDA_results_bal['Precision'][min_class][size][fold] = prec\n",
    "            PLSDA_results_bal['Recall'][min_class][size][fold] = rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d150be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.from_dict(PLSDA_results_bal['Accuracy'][0.6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af103c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.from_dict(PLSDA_results_bal['Accuracy'][2.0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05713b3",
   "metadata": {},
   "source": [
    "PLS-DA Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc188b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the results for each fold\n",
    "f, axs = plt.subplots(2,2,figsize=(12,12))\n",
    "for min_class in PLSDA_results_bal['Accuracy'].keys():\n",
    "    for t, ax in zip(PLSDA_results_bal.keys(), axs.ravel()):\n",
    "        ax.plot((pd.DataFrame.from_dict(PLSDA_results_bal[t][min_class]).columns + 20)/n_max_class*100,\n",
    "                 pd.DataFrame.from_dict(PLSDA_results_bal[t][min_class]).mean(), \n",
    "                 label=str(min_class))\n",
    "        ax.set_title(t, fontsize=15)\n",
    "        ax.set_xlabel('% of Augmentation', fontsize=15)\n",
    "        ax.set_ylim([0, 1.01])\n",
    "\n",
    "axs[0][0].set_ylabel('Performance', fontsize=15)\n",
    "axs[1][0].set_ylabel('Performance', fontsize=15)\n",
    "axs[0][1].legend(fontsize=15, bbox_to_anchor=(1,1))\n",
    "\n",
    "plt.suptitle('PLS-DA', fontsize=18, y=0.93)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240782e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the results for each fold\n",
    "f, axs = plt.subplots(2,2,figsize=(12,12))\n",
    "for min_class in PLSDA_results_bal['Accuracy'].keys():\n",
    "    for t, ax in zip(PLSDA_results_bal.keys(), axs.ravel()):\n",
    "        ax.plot((pd.DataFrame.from_dict(PLSDA_results_bal[t][min_class]).columns + 20)/n_max_class*100,\n",
    "                 pd.DataFrame.from_dict(PLSDA_results_bal[t][min_class]).mean() - pd.DataFrame.from_dict(\n",
    "                     PLSDA_results_bal[t][min_class]).mean()[0], \n",
    "                 label=str(min_class))\n",
    "        ax.set_title(t, fontsize=15)\n",
    "        ax.set_xlabel('% of Augmentation', fontsize=15)\n",
    "        ax.set_ylim([-0.5, 0.5])\n",
    "\n",
    "axs[0][0].set_ylabel('Difference in Performance', fontsize=15)\n",
    "axs[1][0].set_ylabel('Difference in Performance', fontsize=15)\n",
    "axs[0][1].legend(fontsize=15, bbox_to_anchor=(1,1))\n",
    "\n",
    "plt.suptitle('PLS-DA', fontsize=18, y=0.93)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6b0b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the results for each fold\n",
    "f, axs = plt.subplots(2,2,figsize=(12,12))\n",
    "for min_class in PLSDA_results_bal['Accuracy'].keys():\n",
    "    for t, ax in zip(PLSDA_results_bal.keys(), axs.ravel()):\n",
    "        ax.plot((pd.DataFrame.from_dict(PLSDA_results_bal[t][min_class]).columns + 20)/n_max_class*100,\n",
    "                 (pd.DataFrame.from_dict(PLSDA_results_bal[t][min_class]).mean() - pd.DataFrame.from_dict(\n",
    "                     PLSDA_results_bal[t][min_class]).mean()[0]) / (1-pd.DataFrame.from_dict(\n",
    "                     PLSDA_results_bal[t][min_class]).mean()[0]), \n",
    "                 label=str(min_class))\n",
    "        ax.set_title(t, fontsize=15)\n",
    "        ax.set_xlabel('% of Augmentation', fontsize=15)\n",
    "        ax.set_ylim([-0.5, 1])\n",
    "\n",
    "f.supylabel('Difference in Performance (fraction based on maximum possible)', fontsize=15, x=0.07)\n",
    "axs[0][1].legend(fontsize=15, bbox_to_anchor=(1,1))\n",
    "\n",
    "plt.suptitle('PLS-DA', fontsize=18, y=0.93)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3559a076",
   "metadata": {},
   "source": [
    "#### Summary of Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54e7c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the results for each fold\n",
    "with plt.style.context('seaborn-whitegrid'):\n",
    "    f = plt.figure(figsize=(12,10), constrained_layout=True)\n",
    "    subfigs = f.subfigures(nrows=2, ncols=1)\n",
    "    \n",
    "\n",
    "    axs = subfigs[0].subplots(nrows=1, ncols=3)\n",
    "    for min_class in RF_results_bal['Accuracy'].keys():\n",
    "        for t, ax in zip(list(RF_results_bal.keys())[1:], axs.ravel()):\n",
    "            ax.plot((pd.DataFrame.from_dict(RF_results_bal[t][min_class]).columns + 20)/n_max_class*100,\n",
    "                     pd.DataFrame.from_dict(RF_results_bal[t][min_class]).mean(), \n",
    "                     label=str(min_class))\n",
    "            ax.set_title(t, fontsize=15)\n",
    "            ax.set_ylim([0, 1.05])\n",
    "            ax.tick_params(axis='both', which='major', labelsize=14)\n",
    "    \n",
    "    subfigs[0].supxlabel('% of Samples of the Minority Class in Comparison to the Majority Class', fontsize = 15)\n",
    "    axs[0].set_ylabel('Performance', fontsize=15)\n",
    "    axs[1].legend(fontsize=15)\n",
    "    subfigs[0].suptitle('      Random Forest', fontsize=18)\n",
    "    \n",
    "    axs = subfigs[1].subplots(nrows=1, ncols=3)\n",
    "    for min_class in PLSDA_results_bal['Accuracy'].keys():\n",
    "        for t, ax in zip(list(PLSDA_results_bal.keys())[1:], axs.ravel()):\n",
    "            ax.plot((pd.DataFrame.from_dict(PLSDA_results_bal[t][min_class]).columns + 20)/n_max_class*100,\n",
    "                     pd.DataFrame.from_dict(PLSDA_results_bal[t][min_class]).mean(), \n",
    "                     label=str(min_class))\n",
    "            ax.set_title(t, fontsize=15)\n",
    "            ax.set_ylim([0, 1.05])\n",
    "            ax.tick_params(axis='both', which='major', labelsize=14)\n",
    "    \n",
    "    subfigs[1].supxlabel('% of Samples of the Minority Class in Comparison to the Majority Class', fontsize = 15)\n",
    "    axs[0].set_ylabel('Performance', fontsize=15)\n",
    "    axs[1].legend(fontsize=15)\n",
    "\n",
    "    subfigs[1].suptitle('        PLS-DA', fontsize=18)\n",
    "    plt.show()\n",
    "    f.savefig('Images/Synthethic_Imbalanced_Performance_plot.png', dpi=400)\n",
    "    f.savefig('Images/Synthethic_Imbalanced_Performance_plot.pdf', dpi=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012f2934",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('2.0 min RF', pd.DataFrame.from_dict(RF_results_bal['F1-Score'][2.0]).mean()[0])\n",
    "print('2.0 max RF', pd.DataFrame.from_dict(RF_results_bal['F1-Score'][2.0]).mean()[60])\n",
    "print('0.6 min RF', pd.DataFrame.from_dict(RF_results_bal['F1-Score'][0.6]).mean()[0])\n",
    "print('0.6 max RF', pd.DataFrame.from_dict(RF_results_bal['F1-Score'][0.6]).mean()[60])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b35dad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('2.0 min PLS-DA', pd.DataFrame.from_dict(PLSDA_results_bal['F1-Score'][2.0]).mean()[0])\n",
    "print('2.0 max PLS-DA', pd.DataFrame.from_dict(PLSDA_results_bal['F1-Score'][2.0]).mean()[60])\n",
    "print('0.6 min PLS-DA', pd.DataFrame.from_dict(PLSDA_results_bal['F1-Score'][0.6]).mean()[0])\n",
    "print('0.6 max PLS-DA', pd.DataFrame.from_dict(PLSDA_results_bal['F1-Score'][0.6]).mean()[60])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac2ea40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the results for each fold\n",
    "with plt.style.context('seaborn-whitegrid'):\n",
    "    f = plt.figure(figsize=(8,10), constrained_layout=True)\n",
    "    subfigs = f.subfigures(nrows=2, ncols=1)\n",
    "    \n",
    "\n",
    "    axs = subfigs[0].subplots(nrows=1, ncols=2)\n",
    "    for min_class in RF_results_bal['Accuracy'].keys():\n",
    "        for t, ax in zip(['F1-Score', 'Recall'], axs.ravel()):\n",
    "            ax.plot((pd.DataFrame.from_dict(RF_results_bal[t][min_class]).columns + 20)/n_max_class*100,\n",
    "                 (pd.DataFrame.from_dict(RF_results_bal[t][min_class]).mean() - pd.DataFrame.from_dict(\n",
    "                     RF_results_bal[t][min_class]).mean()[0]) / (1-pd.DataFrame.from_dict(\n",
    "                     RF_results_bal[t][min_class]).mean()[0]), \n",
    "                 label=str(min_class))\n",
    "            ax.set_title(t, fontsize=15)\n",
    "            ax.set_ylim([-0.5, 1])\n",
    "            ax.tick_params(axis='both', which='major', labelsize=14)\n",
    "    \n",
    "    subfigs[0].supxlabel('% of Samples of the Minority Class in Comparison to the Majority Class', fontsize = 15)\n",
    "    f.supylabel('Difference in Performance (fraction based on maximum possible)', fontsize=15)\n",
    "    axs[1].legend(fontsize=15, loc='lower right', ncol=2)\n",
    "    #axs[1].legend(fontsize=15, bbox_to_anchor=(1,1), frameon=True)\n",
    "    subfigs[0].suptitle('Random Forest          ', fontsize=18)\n",
    "    \n",
    "    axs = subfigs[1].subplots(nrows=1, ncols=2)\n",
    "    for min_class in PLSDA_results_bal['Accuracy'].keys():\n",
    "        for t, ax in zip(['F1-Score', 'Recall'], axs.ravel()):\n",
    "            ax.plot((pd.DataFrame.from_dict(PLSDA_results_bal[t][min_class]).columns + 20)/n_max_class*100,\n",
    "                 (pd.DataFrame.from_dict(PLSDA_results_bal[t][min_class]).mean() - pd.DataFrame.from_dict(\n",
    "                     PLSDA_results_bal[t][min_class]).mean()[0]) / (1-pd.DataFrame.from_dict(\n",
    "                     PLSDA_results_bal[t][min_class]).mean()[0]), \n",
    "                 label=str(min_class))\n",
    "            ax.set_title(t, fontsize=15)\n",
    "            ax.set_ylim([-0.5, 1])\n",
    "            ax.tick_params(axis='both', which='major', labelsize=14)\n",
    "    \n",
    "    subfigs[1].supxlabel('% of Samples of the Minority Class in Comparison to the Majority Class', fontsize = 15)\n",
    "    axs[1].legend(fontsize=15, loc='lower right', ncol=2)\n",
    "    #axs[1].legend(fontsize=15, bbox_to_anchor=(1,1), frameon=True)\n",
    "\n",
    "    subfigs[1].suptitle('PLS-DA          ', fontsize=18)\n",
    "    plt.show()\n",
    "    f.savefig('Images/Synthethic_Imbalanced_PerfDiff_plot.png', dpi=400)\n",
    "    f.savefig('Images/Synthethic_Imbalanced_PerfDiff_plot.pdf', dpi=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79698328",
   "metadata": {},
   "outputs": [],
   "source": [
    "difs_f1 = (pd.DataFrame.from_dict(RF_results_bal['F1-Score'][2.0]).mean() - pd.DataFrame.from_dict(\n",
    "                     RF_results_bal['F1-Score'][2.0]).mean()[0]) / (1-pd.DataFrame.from_dict(\n",
    "                     RF_results_bal['F1-Score'][2.0]).mean()[0])\n",
    "print('2.0 possible improvement RF', difs_f1[60])\n",
    "difs_f1 = (pd.DataFrame.from_dict(RF_results_bal['F1-Score'][0.6]).mean() - pd.DataFrame.from_dict(\n",
    "                     RF_results_bal['F1-Score'][0.6]).mean()[0]) / (1-pd.DataFrame.from_dict(\n",
    "                     RF_results_bal['F1-Score'][0.6]).mean()[0])\n",
    "print('0.6 possible improvement RF', difs_f1[60])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f35c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "difs_f1 = (pd.DataFrame.from_dict(PLSDA_results_bal['F1-Score'][2.0]).mean() - pd.DataFrame.from_dict(\n",
    "                     PLSDA_results_bal['F1-Score'][2.0]).mean()[0]) / (1-pd.DataFrame.from_dict(\n",
    "                     PLSDA_results_bal['F1-Score'][2.0]).mean()[0])\n",
    "print('2.0 possible improvement PLS-DA', difs_f1[60])\n",
    "difs_f1 = (pd.DataFrame.from_dict(PLSDA_results_bal['F1-Score'][0.6]).mean() - pd.DataFrame.from_dict(\n",
    "                     PLSDA_results_bal['F1-Score'][0.6]).mean()[0]) / (1-pd.DataFrame.from_dict(\n",
    "                     PLSDA_results_bal['F1-Score'][0.6]).mean()[0])\n",
    "print('0.6 possible improvement PLS-DA', difs_f1[60])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "428ec98c",
   "metadata": {},
   "source": [
    "### Fitting RF and PLS-DA models to the complete synthetic datasets\n",
    "\n",
    "Results and Feature Importance are estimated by stratified n_fold-cross validation averaged across 20 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f9ba90",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(485)\n",
    "n_fold = 5\n",
    "RF_model_real = {}\n",
    "RF_feats_real = {}\n",
    "for sep in dfs:\n",
    "    RF_model_real[sep] = ma.RF_model_CV(dfs[sep], lbls_orig[sep],\n",
    "                                   iter_num=20, n_fold=n_fold, n_trees=200) \n",
    "    RF_feats_real[sep] = pd.DataFrame(RF_model_real[sep][\n",
    "        'important_features']).set_index(0).sort_values(by=1, ascending=False)\n",
    "    RF_feats_real[sep].index = [dfs[sep].columns[i] for i in RF_feats_real[sep].index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e28c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(485)\n",
    "n_fold = 5\n",
    "PLSDA_model_real = {}\n",
    "PLSDA_feats_real = {}\n",
    "for sep in dfs:\n",
    "    PLSDA_model_real[sep] = ma.PLSDA_model_CV(dfs[sep], lbls_orig[sep],\n",
    "                                   n_comp=4, iter_num=20, n_fold=n_fold, feat_type='VIP')\n",
    "    PLSDA_feats_real[sep] = pd.DataFrame(PLSDA_model_real[sep][\n",
    "        'important_features']).set_index(0).sort_values(by=1, ascending=False)\n",
    "    PLSDA_feats_real[sep].index = [dfs[sep].columns[i] for i in PLSDA_feats_real[sep].index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc4c1d4",
   "metadata": {},
   "source": [
    "### Extracting Important Features for RF models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e54df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting Important Features and averaging them across the 5 folds for each synthetic dataset\n",
    "rf_feats = {}\n",
    "for cl, models in RF_models_bal.items():\n",
    "    rf_feats[cl] = {}\n",
    "    for size, folds in models.items():\n",
    "        rf_feats[cl][size] = {}\n",
    "        for fold, mod in folds.items():\n",
    "            if fold == 1:\n",
    "                temp_df_bal = mod.feature_importances_\n",
    "            else:\n",
    "                temp_df_bal = temp_df_bal + mod.feature_importances_\n",
    "\n",
    "        temp_df_bal = temp_df_bal / len(folds)\n",
    "\n",
    "        #RF_models[fold][size].feature_importances_\n",
    "        rf_feats[cl][size] = dict(zip(range(1, len(bal_datasets[cl][1][size][0].columns)+1), temp_df_bal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1995f142",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the top 5% of important features\n",
    "rf_feats_abrev = {}\n",
    "for min_class in rf_feats:\n",
    "    temp_df = pd.DataFrame.from_dict(rf_feats[min_class])\n",
    "    rf_feats_abrev[min_class] = pd.Series(index=temp_df.columns)\n",
    "    top10 = int(0.05*len(temp_df.index))\n",
    "    for i in temp_df.columns:\n",
    "        rf_feats_abrev[min_class][i] = temp_df[i].sort_values(ascending=False)[:top10].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d08d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "top10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4199284c",
   "metadata": {},
   "source": [
    "Relative importance values depending on the number of GAN samples added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002225a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the results and adjusting parameters of the plot\n",
    "with plt.style.context('seaborn-whitegrid'):\n",
    "    f, ax = plt.subplots(figsize=(6,6))\n",
    "    for i in rf_feats_abrev:\n",
    "        plt.plot((np.array(list(rf_feats_abrev[i].index)+n_min_class))/n_max_class*100,\n",
    "             rf_feats_abrev[i].values/rf_feats_abrev[i].values[0]*100,\n",
    "                 label=i)\n",
    "\n",
    "    plt.ylabel('Avg. Gini Importance of top 5% Imp. features change (%)', fontsize = 12)\n",
    "    plt.xlabel('% of Augmentation', fontsize = 15)\n",
    "    plt.ylim(60,140)\n",
    "    plt.legend(fontsize=13)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=13)\n",
    "    plt.title('Random Forest', fontsize=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7961c1",
   "metadata": {},
   "source": [
    "Common top 5% important features compared to the complete models depending on the number of GAN samples added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffca2921",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the results and adjusting parameters of the plot\n",
    "with plt.style.context('seaborn-whitegrid'):\n",
    "    f, (axl, axc, axr) = plt.subplots(1,3,figsize=(15,5))\n",
    "\n",
    "    for min_class in rf_feats:\n",
    "        rf_line_min = []\n",
    "        for i in rf_feats[min_class]:\n",
    "            idxs = []\n",
    "            for l in pd.DataFrame(rf_feats[min_class])[i].sort_values(ascending=False)[:top10].index:\n",
    "                idxs.append(bal_datasets[min_class][1][0][0].columns[l-1])\n",
    "            rf_line_min.append(len(np.intersect1d(idxs, RF_feats_real[min_class].index[:top10])))\n",
    "\n",
    "        rf_line_uni_min = []\n",
    "        for i in rf_feats[min_class]:\n",
    "            idxs = []\n",
    "            for l in pd.DataFrame(rf_feats[min_class])[i].sort_values(ascending=False)[:top10].index:\n",
    "                idxs.append(bal_datasets[min_class][1][0][0].columns[l-1])\n",
    "            rf_line_uni_min.append(len(np.intersect1d(idxs, uni_results_filt[min_class].index)))\n",
    "            \n",
    "        rf_line_unitop_min = []\n",
    "        for i in rf_feats[min_class]:\n",
    "            idxs = []\n",
    "            for l in pd.DataFrame(rf_feats[min_class])[i].sort_values(ascending=False)[:top10].index:\n",
    "                idxs.append(bal_datasets[min_class][1][0][0].columns[l-1])\n",
    "            rf_line_unitop_min.append(len(np.intersect1d(idxs, uni_results_filt[min_class].index[:top10])))\n",
    "\n",
    "        axl.plot((np.array(list(rf_feats[min_class].keys()))+n_min_class)/n_max_class*100, np.array(rf_line_min)/top10*100,\n",
    "                 label=min_class)\n",
    "        axc.plot((np.array(list(rf_feats[min_class].keys()))+n_min_class)/n_max_class*100,\n",
    "                 np.array(rf_line_uni_min)/top10*100,\n",
    "                 label=min_class)\n",
    "        axr.plot((np.array(list(rf_feats[min_class].keys()))+n_min_class)/n_max_class*100,\n",
    "                 np.array(rf_line_unitop_min)/top10*100,\n",
    "                 label=min_class)\n",
    "\n",
    "axl.set_title('Against RF Top 5% Imp. Feats', fontsize=15)\n",
    "axc.set_title('Against Univariate Significant Feat.', fontsize=15)\n",
    "axr.set_title('Against Univariate Top 5% Significant Feat.', fontsize=14)\n",
    "axl.set_ylabel('% of Common Features with Real Model', fontsize = 15)\n",
    "axc.set_xlabel('% of Samples of the Minority Class in Comparison to the Majority Class', fontsize = 15)\n",
    "axl.set_ylim(40,102)\n",
    "axc.set_ylim(40,102)\n",
    "axr.set_ylim(40,102)\n",
    "#plt.legend(loc='upper left', fontsize=13, bbox_to_anchor=(1,1))\n",
    "axr.legend(loc='upper left', fontsize=15, bbox_to_anchor=(1,1))\n",
    "\n",
    "plt.suptitle('Random Forest', fontsize=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75ced05",
   "metadata": {},
   "source": [
    "### Extracting Important Features for PLS-DA models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f86058",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting Important Features and averaging them across the 5 folds for each synthetic dataset\n",
    "plsda_feats = {}\n",
    "\n",
    "for cl, models in PLSDA_models_bal.items():\n",
    "    plsda_feats[cl] = {}\n",
    "    for size, folds in models.items():\n",
    "        plsda_feats[cl][size] = {}\n",
    "        for fold, mod in folds.items():\n",
    "            if fold == 1:\n",
    "                temp_df_bal = ma._calculate_vips(mod)\n",
    "            else:\n",
    "                temp_df_bal = temp_df_bal + ma._calculate_vips(mod)\n",
    "\n",
    "        temp_df_bal = temp_df_bal / len(folds)\n",
    "\n",
    "        plsda_feats[cl][size] = dict(zip(range(1, len(bal_datasets[cl][1][size][0].columns)+1), temp_df_bal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2bb13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the top 5% of important features\n",
    "plsda_feats_abrev = {}\n",
    "for min_class in plsda_feats:\n",
    "    temp_df = pd.DataFrame.from_dict(plsda_feats[min_class])\n",
    "    plsda_feats_abrev[min_class] = pd.Series(index=temp_df.columns)\n",
    "    top10 = int(0.05*len(temp_df.index))\n",
    "    for i in temp_df.columns:\n",
    "        plsda_feats_abrev[min_class][i] = temp_df[i].sort_values(ascending=False)[:top10].mean()\n",
    "        #print(rf_feats[i].sort_values(ascending=False)[:top10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c0fa7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "top10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0f8726",
   "metadata": {},
   "source": [
    "Relative importance values depending on the number of GAN samples added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3bb812",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the results and adjusting parameters of the plot\n",
    "with plt.style.context('seaborn-whitegrid'):\n",
    "    f, ax = plt.subplots(figsize=(6,6))\n",
    "\n",
    "    for i in plsda_feats_abrev:\n",
    "        plt.plot((np.array(list(plsda_feats_abrev[i].index)+n_min_class))/n_max_class*100,\n",
    "             plsda_feats_abrev[i].values/plsda_feats_abrev[i].values[0]*100,\n",
    "                 label=i)\n",
    "\n",
    "    plt.ylabel('Avg. VIP Score of top 2% Imp. features change (%)', fontsize = 12)\n",
    "    plt.xlabel('% of Augmentation', fontsize = 15)\n",
    "    plt.ylim(90,120)\n",
    "    plt.legend(fontsize=13)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=13)\n",
    "    plt.title('PLS-DA', fontsize=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb2c823",
   "metadata": {},
   "source": [
    "Common top 5% important features compared to the complete models depending on the number of GAN samples added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e549d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the results and adjusting parameters of the plot\n",
    "with plt.style.context('seaborn-whitegrid'):\n",
    "    f, (axl, axc, axr) = plt.subplots(1,3,figsize=(15,5))\n",
    "\n",
    "    for min_class in plsda_feats:\n",
    "        plsda_line_min = []\n",
    "        for i in plsda_feats[min_class]:\n",
    "            idxs = []\n",
    "            for l in pd.DataFrame(plsda_feats[min_class])[i].sort_values(ascending=False)[:top10].index:\n",
    "                idxs.append(bal_datasets[min_class][1][0][0].columns[l-1])\n",
    "            plsda_line_min.append(len(np.intersect1d(idxs, PLSDA_feats_real[min_class].index[:top10])))\n",
    "\n",
    "        plsda_line_uni_min = []\n",
    "        for i in plsda_feats[min_class]:\n",
    "            idxs = []\n",
    "            for l in pd.DataFrame(plsda_feats[min_class])[i].sort_values(ascending=False)[:top10].index:\n",
    "                idxs.append(bal_datasets[min_class][1][0][0].columns[l-1])\n",
    "            plsda_line_uni_min.append(len(np.intersect1d(idxs, uni_results_filt[min_class].index)))\n",
    "            \n",
    "        plsda_line_unitop_min = []\n",
    "        for i in plsda_feats[min_class]:\n",
    "            idxs = []\n",
    "            for l in pd.DataFrame(plsda_feats[min_class])[i].sort_values(ascending=False)[:top10].index:\n",
    "                idxs.append(bal_datasets[min_class][1][0][0].columns[l-1])\n",
    "            plsda_line_unitop_min.append(len(np.intersect1d(idxs, uni_results_filt[min_class].index[:top10])))\n",
    "\n",
    "        axl.plot((np.array(list(plsda_feats[min_class].keys()))+n_min_class)/n_max_class*100,\n",
    "                 np.array(plsda_line_min)/top10*100,\n",
    "                 label=min_class)\n",
    "        axc.plot((np.array(list(plsda_feats[min_class].keys()))+n_min_class)/n_max_class*100,\n",
    "                 np.array(plsda_line_uni_min)/top10*100,\n",
    "                 label=min_class)\n",
    "        axr.plot((np.array(list(plsda_feats[min_class].keys()))+n_min_class)/n_max_class*100,\n",
    "                 np.array(plsda_line_unitop_min)/top10*100,\n",
    "                 label=min_class)\n",
    "\n",
    "axl.set_title('Against PLS-DA Top 5% Imp. Feats', fontsize=15)\n",
    "axc.set_title('Against Univariate Significant Feat.', fontsize=15)\n",
    "axr.set_title('Against Univariate Top 5% Significant Feat.', fontsize=14)\n",
    "axl.set_ylabel('% of Common Features with Real Model', fontsize = 15)\n",
    "axc.set_xlabel('% of Samples of the Minority Class in Comparison to the Majority Class', fontsize = 15)\n",
    "axl.set_ylim(40,102)\n",
    "axc.set_ylim(40,102)\n",
    "axr.set_ylim(40,102)\n",
    "#plt.legend(loc='upper left', fontsize=13, bbox_to_anchor=(1,1))\n",
    "axc.legend(loc='lower left', fontsize=13)\n",
    "\n",
    "plt.suptitle('PLS-DA', fontsize=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c998f07",
   "metadata": {},
   "source": [
    "#### Summary of Results of Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d294ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the results and adjusting parameters of the plot\n",
    "with plt.style.context('seaborn-whitegrid'):\n",
    "    f, (axl, axr) = plt.subplots(1,2, figsize=(8,4), constrained_layout=True)\n",
    "\n",
    "    for i in rf_feats_abrev:\n",
    "        axl.plot((np.array(list(rf_feats_abrev[i].index)+n_min_class))/n_max_class*100,\n",
    "             rf_feats_abrev[i].values/rf_feats_abrev[i].values[0]*100,\n",
    "                 label=i)\n",
    "\n",
    "    axl.set_ylabel('Avg. Gini Imp. of top 5% Imp. Feat. change (%)', fontsize = 10)\n",
    "    axl.set_ylim(60,140)\n",
    "    axl.legend(fontsize=12, ncol=2)\n",
    "    axl.tick_params(axis='both', which='major', labelsize=11)\n",
    "    axl.set_title('Random Forest', fontsize=15)\n",
    "    \n",
    "    for i in plsda_feats_abrev:\n",
    "        axr.plot((np.array(list(plsda_feats_abrev[i].index)+n_min_class))/n_max_class*100,\n",
    "             plsda_feats_abrev[i].values/plsda_feats_abrev[i].values[0]*100,\n",
    "                 label=i)\n",
    "\n",
    "    axr.set_ylabel('Avg. VIP Score of top 5% Imp. Feat. change (%)', fontsize = 10)\n",
    "    f.supxlabel('% of Samples of the Minority Class in Comparison to the Majority Class', fontsize = 15)\n",
    "    axr.set_ylim(60,140)\n",
    "    axr.tick_params(axis='both', which='major', labelsize=11)\n",
    "    axr.set_title('PLS-DA', fontsize=15)\n",
    "    #plt.suptitle('YD', fontsize=18)\n",
    "    f.savefig('images/Synthetic_Imbalanced_ImpFeatChange_plot.png', dpi=400)\n",
    "    f.savefig('images/Synthetic_Imbalanced_ImpFeatChange_plot.pdf', dpi=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d01215",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the results and adjusting parameters of the plot\n",
    "with plt.style.context('seaborn-whitegrid'):\n",
    "    f, (axu, axd) = plt.subplots(2,2,figsize=(10,10), constrained_layout=True)\n",
    "    axul, axur = axu\n",
    "    axdl, axdr = axd\n",
    "    \n",
    "    for min_class in rf_feats:\n",
    "        rf_line_min = []\n",
    "        for i in rf_feats[min_class]:\n",
    "            idxs = []\n",
    "            for l in pd.DataFrame(rf_feats[min_class])[i].sort_values(ascending=False)[:top10].index:\n",
    "                idxs.append(bal_datasets[min_class][1][0][0].columns[l-1])\n",
    "            rf_line_min.append(len(np.intersect1d(idxs, RF_feats_real[min_class].index[:top10])))\n",
    "\n",
    "        rf_line_uni_min = []\n",
    "        for i in rf_feats[min_class]:\n",
    "            idxs = []\n",
    "            for l in pd.DataFrame(rf_feats[min_class])[i].sort_values(ascending=False)[:top10].index:\n",
    "                idxs.append(bal_datasets[min_class][1][0][0].columns[l-1])\n",
    "            rf_line_uni_min.append(len(np.intersect1d(idxs, uni_results_filt[min_class].index)))\n",
    "\n",
    "        axul.plot((np.array(list(rf_feats[min_class].keys()))+n_min_class)/n_max_class*100, np.array(rf_line_min)/top10*100,\n",
    "                 label=min_class)\n",
    "        axur.plot((np.array(list(rf_feats[min_class].keys()))+n_min_class)/n_max_class*100,\n",
    "                 np.array(rf_line_uni_min)/top10*100,\n",
    "                 label=min_class)\n",
    "\n",
    "    axul.set_title('Against RF Top 5% Imp. Feat.', fontsize=15)\n",
    "    axur.set_title('Against Univariate Significant Feat.', fontsize=15)\n",
    "    axul.set_ylabel('% of Common Features with RF Real Model', fontsize = 14)\n",
    "    #f.set_xlabel('% of Samples of the Minority Class in Comparison to the Majority Class', fontsize = 15)\n",
    "    axul.set_ylim(0,105)\n",
    "    axur.set_ylim(0,105)\n",
    "    #plt.legend(loc='upper left', fontsize=13, bbox_to_anchor=(1,1))\n",
    "    axur.legend(loc='lower left', fontsize=15, ncol=2)\n",
    "    axul.tick_params(axis='both', which='major', labelsize=14)\n",
    "    axur.tick_params(axis='both', which='major', labelsize=14)\n",
    "    \n",
    "    for min_class in plsda_feats:\n",
    "        plsda_line_min = []\n",
    "        for i in plsda_feats[min_class]:\n",
    "            idxs = []\n",
    "            for l in pd.DataFrame(plsda_feats[min_class])[i].sort_values(ascending=False)[:top10].index:\n",
    "                idxs.append(bal_datasets[min_class][1][0][0].columns[l-1])\n",
    "            plsda_line_min.append(len(np.intersect1d(idxs, PLSDA_feats_real[min_class].index[:top10])))\n",
    "\n",
    "        plsda_line_uni_min = []\n",
    "        for i in plsda_feats[min_class]:\n",
    "            idxs = []\n",
    "            for l in pd.DataFrame(plsda_feats[min_class])[i].sort_values(ascending=False)[:top10].index:\n",
    "                idxs.append(bal_datasets[min_class][1][0][0].columns[l-1])\n",
    "            plsda_line_uni_min.append(len(np.intersect1d(idxs, uni_results_filt[min_class].index)))\n",
    "\n",
    "        axdl.plot((np.array(list(plsda_feats[min_class].keys()))+n_min_class)/n_max_class*100,\n",
    "                 np.array(plsda_line_min)/top10*100,\n",
    "                 label=min_class)\n",
    "        axdr.plot((np.array(list(plsda_feats[min_class].keys()))+n_min_class)/n_max_class*100,\n",
    "                 np.array(plsda_line_uni_min)/top10*100,\n",
    "                 label=min_class)\n",
    "\n",
    "    axdl.set_title('Against PLS-DA Top 5% Imp. Feat.', fontsize=15)\n",
    "    axdr.set_title('Against Univariate Significant Feat.', fontsize=15)\n",
    "    axdl.set_ylabel('% of Common Features with Real PLS-DA Model', fontsize = 14)\n",
    "    f.supxlabel('% of Samples of the Minority Class in Comparison to the Majority Class', fontsize = 15)\n",
    "    axdl.set_ylim(0,105)\n",
    "    axdr.set_ylim(0,105)\n",
    "    axdl.tick_params(axis='both', which='major', labelsize=14)\n",
    "    axdr.tick_params(axis='both', which='major', labelsize=14)\n",
    "    #plt.suptitle('YD', fontsize=18)\n",
    "    f.savefig('Images/Synthetic_Imbalanced_ImpFeatCommon_plot.png', dpi=400)\n",
    "    f.savefig('Images/Synthetic_Imbalanced_ImpFeatCommon_plot.pdf', dpi=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ff3156",
   "metadata": {},
   "source": [
    "### Comparing Imbalanced Datasets, Imb + Min. Cl. GAN Samples and only GAN samples\n",
    "\n",
    "Creating the models and evaluating them for GAN data and compiling results for the imbalanced datasets (with no augmentation) and the imbalanced datasets made balanced with minority class GAN samples.\n",
    "\n",
    "##### RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4426b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting and storing Random Forest models for each fold\n",
    "RF_models_GAN = {}\n",
    "RF_models_real = {}\n",
    "RF_models_GAN_bal = {}\n",
    "\n",
    "# Train the Models\n",
    "for i in generated_samples:\n",
    "    RF_models_GAN[i] = {}\n",
    "    RF_models_real[i] = {}\n",
    "    RF_models_GAN_bal[i] = {}\n",
    "    for fold in generated_samples[i]:\n",
    "        rf_mod = ma.RF_model(generated_samples[i][fold][0], generated_samples[i][fold][1], return_cv=False, n_trees=200)\n",
    "        RF_models_GAN[i][fold] = rf_mod\n",
    "\n",
    "        RF_models_real[i][fold] = RF_models_bal[i][0][fold]\n",
    "\n",
    "        RF_models_GAN_bal[i][fold] = RF_models_bal[i][n_max_class-n_min_class][fold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952fdfb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the RF models with the test data for each fold\n",
    "RF_results_GAN = {'Accuracy':{}, 'F1-Score':{},\n",
    "                  'Precision':{}, 'Recall':{}}\n",
    "RF_results_real = {'Accuracy':{}, 'F1-Score':{},\n",
    "                   'Precision':{}, 'Recall':{}}\n",
    "RF_results_GAN_bal = {'Accuracy':{}, 'F1-Score':{},\n",
    "                  'Precision':{}, 'Recall':{}}\n",
    "\n",
    "for min_class in RF_models_bal:\n",
    "    for l in RF_results_GAN:\n",
    "        RF_results_GAN[l][min_class] = {}\n",
    "        RF_results_real[l][min_class] = {}\n",
    "        RF_results_GAN_bal[l][min_class] = {}\n",
    "    \n",
    "    for fold in generated_samples[min_class]:\n",
    "        RF_results_GAN['Accuracy'][min_class][fold] = RF_models_GAN[min_class][fold].score(\n",
    "                                                                                df_storage_test[min_class][fold],\n",
    "                                                                                lbl_storage_test[min_class][fold])\n",
    "        preds = RF_models_GAN[min_class][fold].predict(df_storage_test[min_class][fold])\n",
    "        prec, rec, f1, sup = precision_recall_fscore_support(lbl_storage_test[min_class][fold], preds,\n",
    "                                                            pos_label='1', average='binary', zero_division=1)\n",
    "        RF_results_GAN['F1-Score'][min_class][fold] = f1\n",
    "        RF_results_GAN['Precision'][min_class][fold] = prec\n",
    "        RF_results_GAN['Recall'][min_class][fold] = rec\n",
    "\n",
    "        RF_results_real['Accuracy'][min_class][fold] = RF_results_bal['Accuracy'][min_class][0][fold]\n",
    "        RF_results_real['F1-Score'][min_class][fold] = RF_results_bal['F1-Score'][min_class][0][fold]\n",
    "        RF_results_real['Precision'][min_class][fold] = RF_results_bal['Precision'][min_class][0][fold]\n",
    "        RF_results_real['Recall'][min_class][fold] = RF_results_bal['Recall'][min_class][0][fold]\n",
    "        \n",
    "        RF_results_GAN_bal['Accuracy'][min_class][fold] = RF_results_bal[\n",
    "            'Accuracy'][min_class][n_max_class-n_min_class][fold]\n",
    "        RF_results_GAN_bal['F1-Score'][min_class][fold] = RF_results_bal[\n",
    "            'F1-Score'][min_class][n_max_class-n_min_class][fold]\n",
    "        RF_results_GAN_bal['Precision'][min_class][fold] = RF_results_bal[\n",
    "            'Precision'][min_class][n_max_class-n_min_class][fold]\n",
    "        RF_results_GAN_bal['Recall'][min_class][fold] = RF_results_bal['Recall'][min_class][n_max_class-n_min_class][fold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804555d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "PLSDA_models_GAN = {}\n",
    "PLSDA_results_GAN = {'Accuracy':{}, 'F1-Score':{},\n",
    "                  'Precision':{}, 'Recall':{}}\n",
    "\n",
    "PLSDA_models_real = {}\n",
    "PLSDA_results_real = {'Accuracy':{}, 'F1-Score':{},\n",
    "                  'Precision':{}, 'Recall':{}}\n",
    "\n",
    "PLSDA_models_GAN_bal = {}\n",
    "PLSDA_results_GAN_bal = {'Accuracy':{}, 'F1-Score':{},\n",
    "                  'Precision':{}, 'Recall':{}}\n",
    "\n",
    "# Train the Models\n",
    "for min_class in bal_datasets:\n",
    "    for l in PLSDA_results_GAN:\n",
    "        PLSDA_results_GAN[l][min_class] = {}\n",
    "        PLSDA_results_real[l][min_class] = {}\n",
    "        PLSDA_results_GAN_bal[l][min_class] = {}\n",
    "    PLSDA_models_GAN[min_class] = {}\n",
    "    PLSDA_models_real[min_class] = {}\n",
    "    PLSDA_models_GAN_bal[min_class] = {}\n",
    "\n",
    "    for fold in bal_datasets[min_class]:\n",
    "\n",
    "        PLSDA_models_GAN[min_class][fold] = ma.fit_PLSDA_model(generated_samples[min_class][fold][0],\n",
    "                                                               generated_samples[min_class][fold][1],\n",
    "                                                          n_comp=4,\n",
    "                                                  return_scores=False, scale=False, encode2as1vector=True)\n",
    "        plsda = PLSDA_models_GAN[min_class][fold]\n",
    "        # Obtain results with the test group\n",
    "        y_pred = plsda.predict(df_storage_test[min_class][fold])\n",
    "        y_true = ma._generate_y_PLSDA(lbl_storage_test[min_class][fold], pd.unique(generated_samples[min_class][fold][1]),\n",
    "                                      True)\n",
    "        pos_label = np.where(pd.unique(generated_samples[min_class][fold][1]) != '1')[0][0]\n",
    "        # Calculate accuracy\n",
    "        accuracy, f1, prec, rec = decision_rule(y_pred, y_true, pos_label=pos_label, average='binary')\n",
    "        PLSDA_results_GAN['Accuracy'][min_class][fold] = accuracy\n",
    "        PLSDA_results_GAN['F1-Score'][min_class][fold] = f1\n",
    "        PLSDA_results_GAN['Precision'][min_class][fold] = prec\n",
    "        PLSDA_results_GAN['Recall'][min_class][fold] = rec\n",
    "        \n",
    "        PLSDA_models_real[min_class][fold] = PLSDA_models_bal[min_class][0][fold]\n",
    "        \n",
    "        PLSDA_results_real['Accuracy'][min_class][fold] = PLSDA_results_bal['Accuracy'][min_class][0][fold]\n",
    "        PLSDA_results_real['F1-Score'][min_class][fold] = PLSDA_results_bal['F1-Score'][min_class][0][fold]\n",
    "        PLSDA_results_real['Precision'][min_class][fold] = PLSDA_results_bal['Precision'][min_class][0][fold]\n",
    "        PLSDA_results_real['Recall'][min_class][fold] = PLSDA_results_bal['Recall'][min_class][0][fold]\n",
    "        \n",
    "        PLSDA_models_GAN_bal[min_class][fold] = PLSDA_models_bal[min_class][n_max_class-n_min_class][fold]\n",
    "        \n",
    "        PLSDA_results_GAN_bal['Accuracy'][min_class][fold] = PLSDA_results_bal[\n",
    "            'Accuracy'][min_class][n_max_class-n_min_class][fold]\n",
    "        PLSDA_results_GAN_bal['F1-Score'][min_class][fold] = PLSDA_results_bal[\n",
    "            'F1-Score'][min_class][n_max_class-n_min_class][fold]\n",
    "        PLSDA_results_GAN_bal['Precision'][min_class][fold] = PLSDA_results_bal[\n",
    "            'Precision'][min_class][n_max_class-n_min_class][fold]\n",
    "        PLSDA_results_GAN_bal['Recall'][min_class][fold] = PLSDA_results_bal[\n",
    "            'Recall'][min_class][n_max_class-n_min_class][fold]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f990fc79",
   "metadata": {},
   "source": [
    "##### Summarising Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1956497d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the results and adjusting parameters of the plot\n",
    "with plt.style.context('seaborn-whitegrid'):\n",
    "    f, (axl, axr) = plt.subplots(1,2, figsize=(8,4), constrained_layout=True)\n",
    "    \n",
    "    axl.plot(F1s_stats_RF['treatment'], F1s_stats_RF['Average accuracy'],\n",
    "       label = 'Complete Dataset (CV)', color = 'black')\n",
    "    axl.errorbar(F1s_stats_RF['treatment'], y=F1s_stats_RF['Average accuracy'],\n",
    "                yerr=F1s_stats_RF['STD'],\n",
    "                                ls='none', ecolor='0.2', capsize=3)\n",
    "    metric = 'F1-Score'\n",
    "\n",
    "    axl.plot(pd.DataFrame(RF_results_real[metric]).mean().index,\n",
    "             pd.DataFrame(RF_results_real[metric]).mean().values,\n",
    "           label = 'Real Imbalanced Data', color = 'green')\n",
    "    axl.errorbar(pd.DataFrame(RF_results_real[metric]).mean().index, y=pd.DataFrame(RF_results_real[metric]).mean(), \n",
    "                yerr=pd.DataFrame(RF_results_real[metric]).std(),\n",
    "                                ls='none', ecolor='0.2', capsize=3)\n",
    "    \n",
    "    axl.plot(pd.DataFrame(RF_results_GAN_bal[metric]).mean().index,\n",
    "             pd.DataFrame(RF_results_GAN_bal[metric]).mean().values,\n",
    "           label = 'Imb. + Min. Cl. GAN Data', color = 'red')\n",
    "    axl.errorbar(pd.DataFrame(RF_results_GAN_bal[metric]).mean().index, y=pd.DataFrame(RF_results_GAN_bal[metric]).mean(), \n",
    "                yerr=pd.DataFrame(RF_results_GAN_bal[metric]).std(),\n",
    "                                ls='none', ecolor='0.2', capsize=3)\n",
    "\n",
    "    \n",
    "    axl.plot(pd.DataFrame(RF_results_GAN[metric]).mean().index,\n",
    "             pd.DataFrame(RF_results_GAN[metric]).mean().values,\n",
    "           label = 'GAN Data', color = 'blue')\n",
    "    axl.errorbar(pd.DataFrame(RF_results_GAN[metric]).mean().index, y=pd.DataFrame(RF_results_GAN[metric]).mean(), \n",
    "                yerr=pd.DataFrame(RF_results_GAN[metric]).std(),\n",
    "                                ls='none', ecolor='0.2', capsize=3)\n",
    "    \n",
    "    axl.set_title('Random Forest', fontsize=15)\n",
    "    axl.set_xlabel('Class Separation', fontsize=15)\n",
    "    axl.set_ylabel(metric, fontsize=15)\n",
    "    \n",
    "    axr.plot(F1s_stats_PLSDA['treatment'], F1s_stats_PLSDA['Average accuracy'],\n",
    "       label = 'Complete Dataset', color = 'black')\n",
    "    axr.errorbar(F1s_stats_PLSDA['treatment'], y=F1s_stats_PLSDA['Average accuracy'],\n",
    "                yerr=F1s_stats_PLSDA['STD'],\n",
    "                                ls='none', ecolor='0.2', capsize=3)\n",
    "    \n",
    "    axr.plot(pd.DataFrame(PLSDA_results_real[metric]).mean().index,\n",
    "             pd.DataFrame(PLSDA_results_real[metric]).mean().values,\n",
    "           label = 'Real Imbalanced Data', color = 'green')\n",
    "    axr.errorbar(pd.DataFrame(PLSDA_results_real[metric]).mean().index, y=pd.DataFrame(PLSDA_results_real[metric]).mean(), \n",
    "                yerr=pd.DataFrame(PLSDA_results_real[metric]).std(),\n",
    "                                ls='none', ecolor='0.2', capsize=3)\n",
    "    \n",
    "    axr.plot(pd.DataFrame(PLSDA_results_GAN_bal[metric]).mean().index,\n",
    "             pd.DataFrame(PLSDA_results_GAN_bal[metric]).mean().values,\n",
    "           label = 'Imb. + Min. Cl. GAN Data', color = 'red')\n",
    "    axr.errorbar(pd.DataFrame(PLSDA_results_GAN_bal[metric]).mean().index,\n",
    "                y=pd.DataFrame(PLSDA_results_GAN_bal[metric]).mean(), \n",
    "                yerr=pd.DataFrame(PLSDA_results_GAN_bal[metric]).std(),\n",
    "                                ls='none', ecolor='0.2', capsize=3)\n",
    "    \n",
    "    axr.plot(pd.DataFrame(PLSDA_results_GAN[metric]).mean().index,\n",
    "             pd.DataFrame(PLSDA_results_GAN[metric]).mean().values,\n",
    "           label = 'GAN Data', color = 'blue')\n",
    "    axr.errorbar(pd.DataFrame(PLSDA_results_GAN[metric]).mean().index, y=pd.DataFrame(PLSDA_results_GAN[metric]).mean(), \n",
    "                yerr=pd.DataFrame(PLSDA_results_GAN[metric]).std(),\n",
    "                                ls='none', ecolor='0.2', capsize=3)\n",
    "    \n",
    "    axr.set_title('PLS-DA', fontsize=15)\n",
    "    axr.set_xlabel('Class Separation', fontsize=15)\n",
    "    axr.set_ylim([0, 1.05])\n",
    "    axr.legend(fontsize=13)\n",
    "    #plt.suptitle('YD', fontsize=18)\n",
    "    f.savefig('images/Synthetic_Imbalanced_F1Score_plot.png', dpi=400)\n",
    "    f.savefig('images/Synthetic_Imbalanced_F1Score_plot.pdf', dpi=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28931e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the results for each fold\n",
    "f, axs = plt.subplots(2,2,figsize=(12,12))\n",
    "for metric, ax in zip(RF_results_GAN, axs.ravel()):\n",
    "    ax.plot(pd.DataFrame(RF_results_GAN[metric]).mean().index,\n",
    "             pd.DataFrame(RF_results_GAN[metric]).mean().values,\n",
    "           label = 'GAN data', color = 'blue')\n",
    "    ax.errorbar(pd.DataFrame(RF_results_GAN[metric]).mean().index, y=pd.DataFrame(RF_results_GAN[metric]).mean(), \n",
    "                yerr=pd.DataFrame(RF_results_GAN[metric]).std(),\n",
    "                                ls='none', ecolor='0.2', capsize=3)\n",
    "    \n",
    "    ax.plot(pd.DataFrame(RF_results_GAN_bal[metric]).mean().index,\n",
    "             pd.DataFrame(RF_results_GAN_bal[metric]).mean().values,\n",
    "           label = 'Balanced Data', color = 'red')\n",
    "    ax.errorbar(pd.DataFrame(RF_results_GAN_bal[metric]).mean().index, y=pd.DataFrame(RF_results_GAN_bal[metric]).mean(), \n",
    "                yerr=pd.DataFrame(RF_results_GAN_bal[metric]).std(),\n",
    "                                ls='none', ecolor='0.2', capsize=3)\n",
    "    \n",
    "    ax.plot(pd.DataFrame(RF_results_real[metric]).mean().index,\n",
    "             pd.DataFrame(RF_results_real[metric]).mean().values,\n",
    "           label = 'Imbalanced Data', color = 'green')\n",
    "    ax.errorbar(pd.DataFrame(RF_results_real[metric]).mean().index, y=pd.DataFrame(RF_results_real[metric]).mean(), \n",
    "                yerr=pd.DataFrame(RF_results_real[metric]).std(),\n",
    "                                ls='none', ecolor='0.2', capsize=3)\n",
    "    \n",
    "    ax.set_title(metric, fontsize=15)\n",
    "    ax.set_xlabel('Class Separation', fontsize=15)\n",
    "    ax.set_ylim([0, 1.05])\n",
    "    \n",
    "axs[0][0].plot(accuracy_stats_RF['treatment'], accuracy_stats_RF['Average accuracy'],\n",
    "       label = 'Full Data (CV)', color = 'black')\n",
    "axs[0][0].errorbar(accuracy_stats_RF['treatment'], y=accuracy_stats_RF['Average accuracy'],\n",
    "            yerr=accuracy_stats_RF['STD'],\n",
    "                            ls='none', ecolor='0.2', capsize=3)\n",
    "\n",
    "axs[0][1].plot(F1s_stats_RF['treatment'], F1s_stats_RF['Average accuracy'],\n",
    "       label = 'Full Data (CV)', color = 'black')\n",
    "axs[0][1].errorbar(F1s_stats_RF['treatment'], y=F1s_stats_RF['Average accuracy'],\n",
    "            yerr=F1s_stats_RF['STD'],\n",
    "                            ls='none', ecolor='0.2', capsize=3)\n",
    "    \n",
    "axs[0][1].legend(bbox_to_anchor=(1,1), fontsize=15)\n",
    "f.supylabel('Performance', fontsize=15, x=0.07)\n",
    "\n",
    "plt.suptitle('RF', fontsize=18, y=0.93)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8eb064",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the results for each fold\n",
    "f, axs = plt.subplots(2,2,figsize=(12,12))\n",
    "for metric, ax in zip(PLSDA_results_GAN, axs.ravel()):\n",
    "    ax.plot(pd.DataFrame(PLSDA_results_GAN[metric]).mean().index,\n",
    "             pd.DataFrame(PLSDA_results_GAN[metric]).mean().values,\n",
    "           label = 'GAN data', color = 'blue')\n",
    "    ax.errorbar(pd.DataFrame(PLSDA_results_GAN[metric]).mean().index, y=pd.DataFrame(PLSDA_results_GAN[metric]).mean(), \n",
    "                yerr=pd.DataFrame(PLSDA_results_GAN[metric]).std(),\n",
    "                                ls='none', ecolor='0.2', capsize=3)\n",
    "    \n",
    "    ax.plot(pd.DataFrame(PLSDA_results_GAN_bal[metric]).mean().index,\n",
    "             pd.DataFrame(PLSDA_results_GAN_bal[metric]).mean().values,\n",
    "           label = 'Balanced Data', color = 'red')\n",
    "    ax.errorbar(pd.DataFrame(PLSDA_results_GAN_bal[metric]).mean().index,\n",
    "                y=pd.DataFrame(PLSDA_results_GAN_bal[metric]).mean(), \n",
    "                yerr=pd.DataFrame(PLSDA_results_GAN_bal[metric]).std(),\n",
    "                                ls='none', ecolor='0.2', capsize=3)\n",
    "    \n",
    "    ax.plot(pd.DataFrame(PLSDA_results_real[metric]).mean().index,\n",
    "             pd.DataFrame(PLSDA_results_real[metric]).mean().values,\n",
    "           label = 'Imbalanced Data', color = 'green')\n",
    "    ax.errorbar(pd.DataFrame(PLSDA_results_real[metric]).mean().index, y=pd.DataFrame(PLSDA_results_real[metric]).mean(), \n",
    "                yerr=pd.DataFrame(PLSDA_results_real[metric]).std(),\n",
    "                                ls='none', ecolor='0.2', capsize=3)\n",
    "    \n",
    "    ax.set_title(metric, fontsize=15)\n",
    "    ax.set_xlabel('Class Separation', fontsize=15)\n",
    "    ax.set_ylim([0, 1.05])\n",
    "    \n",
    "axs[0][0].plot(accuracy_stats_PLSDA['treatment'], accuracy_stats_PLSDA['Average accuracy'],\n",
    "       label = 'Full Data (CV)', color = 'black')\n",
    "axs[0][0].errorbar(accuracy_stats_PLSDA['treatment'], y=accuracy_stats_PLSDA['Average accuracy'],\n",
    "            yerr=accuracy_stats_PLSDA['STD'],\n",
    "                            ls='none', ecolor='0.2', capsize=3)\n",
    "axs[0][1].legend(bbox_to_anchor=(1,1), fontsize=15)\n",
    "f.supylabel('Performance', fontsize=15, x=0.07)\n",
    "\n",
    "plt.suptitle('PLS-DA', fontsize=18, y=0.93)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5444be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results\n",
    "with sns.axes_style(\"whitegrid\"):\n",
    "    with sns.plotting_context(\"notebook\", font_scale=1.5):\n",
    "        f, axs = plt.subplots(6, 4, figsize=(20, 18), constrained_layout=True)\n",
    "        \n",
    "        for sep,i in zip(RF_results_GAN['Accuracy'], range(0,24,4)):\n",
    "            for metric, axu in zip(PLSDA_results_GAN, axs.ravel()[i:i+4]):\n",
    "                x = np.arange(2)  # the label locations\n",
    "                l = ['RF', 'PLSDA']\n",
    "                width = 0.23  # the width of the bars\n",
    "\n",
    "                offset = - 0.25 + 0 * 0.25\n",
    "                accuracy_stats = pd.DataFrame({'Average accuracy': \n",
    "                                               [pd.Series(RF_results_GAN[metric][sep]).values.mean(), \n",
    "                                                     pd.Series(PLSDA_results_GAN[metric][sep]).values.mean()],\n",
    "                                             'STD': [pd.Series(RF_results_GAN[metric][sep]).values.std(), \n",
    "                                                     pd.Series(PLSDA_results_GAN[metric][sep]).values.std()]})\n",
    "                rects = axu.bar(x + offset, accuracy_stats['Average accuracy'], width, label='GAN data', color='blue')\n",
    "                axu.errorbar(x + offset, y=accuracy_stats['Average accuracy'], yerr=accuracy_stats['STD'],\n",
    "                                ls='none', ecolor='0.2', capsize=3)\n",
    "\n",
    "                offset = - 0.25 + 1 * 0.25\n",
    "                accuracy_stats = pd.DataFrame({'Average accuracy': \n",
    "                                               [pd.Series(RF_results_GAN_bal[metric][sep]).values.mean(), \n",
    "                                                    pd.Series(PLSDA_results_GAN_bal[metric][sep]).values.mean()],\n",
    "                                             'STD': [pd.Series(RF_results_GAN_bal[metric][sep]).values.std(), \n",
    "                                                     pd.Series(PLSDA_results_GAN_bal[metric][sep]).values.std()]})\n",
    "                rects = axu.bar(x + offset, accuracy_stats['Average accuracy'],\n",
    "                                width, label='Balanced data', color='red')\n",
    "                axu.errorbar(x + offset, y=accuracy_stats['Average accuracy'], yerr=accuracy_stats['STD'],\n",
    "                                ls='none', ecolor='0.2', capsize=3)\n",
    "\n",
    "                offset = - 0.25 + 2 * 0.25\n",
    "                accuracy_stats = pd.DataFrame({'Average accuracy': \n",
    "                                               [pd.Series(RF_results_real[metric][sep]).values.mean(), \n",
    "                                                    pd.Series(PLSDA_results_real[metric][sep]).values.mean()],\n",
    "                                             'STD': [pd.Series(RF_results_real[metric][sep]).values.std(), \n",
    "                                                     pd.Series(PLSDA_results_real[metric][sep]).values.std()]})\n",
    "                rects = axu.bar(x + offset, accuracy_stats['Average accuracy'], width, label='Real data', color='green')\n",
    "                axu.errorbar(x + offset, y=accuracy_stats['Average accuracy'], yerr=accuracy_stats['STD'],\n",
    "                                ls='none', ecolor='0.2', capsize=3)\n",
    "\n",
    "                axu.set_xticks(x)\n",
    "                axu.set_xticklabels(l, fontsize=16)\n",
    "                axu.set(ylabel=sep, title=metric, ylim=(0,1.05))\n",
    "                for spine in axu.spines.values():\n",
    "                    spine.set_edgecolor('0.1')\n",
    "\n",
    "    axs[0][3].legend(loc='upper left', fontsize=13, bbox_to_anchor=(1,1))\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29e6112",
   "metadata": {},
   "source": [
    "### Comparing Important features of models built from Imbalanced Datasets, Imb + Min. Cl. GAN Samples and only GAN samples\n",
    "\n",
    "#### Comparing Against Important Features of the Complete  Synthetic Dataset models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b4edc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 2nd complete model but with only 1 iteration to compare the important features\n",
    "np.random.seed(11485)\n",
    "n_fold = 5\n",
    "RF_model_real1 = {}\n",
    "RF_feats_real1 = {}\n",
    "for sep in dfs:\n",
    "    RF_model_real1[sep] = ma.RF_model_CV(dfs[sep], lbls_orig[sep],\n",
    "                                   iter_num=1, n_fold=n_fold, n_trees=200) \n",
    "    RF_feats_real1[sep] = pd.DataFrame(RF_model_real1[sep][\n",
    "        'important_features']).set_index(0).sort_values(by=1, ascending=False)\n",
    "    RF_feats_real1[sep].index = [dfs[sep].columns[i] for i in RF_feats_real1[sep].index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1486dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_feats_GAN = {}\n",
    "RF_feats_imb = {}\n",
    "RF_feats_GAN_bal = {}\n",
    "\n",
    "for i in RF_models_real.keys():\n",
    "    RF_feats_GAN[i] = {}\n",
    "    RF_feats_imb[i] = {}\n",
    "    RF_feats_GAN_bal[i] = {}\n",
    "    for fold in RF_models_real[i]:\n",
    "        temp_df = pd.DataFrame(zip(range(len(generated_samples[i][fold][0].columns)),\n",
    "                                   RF_models_real[i][fold].feature_importances_))\n",
    "        temp_df = temp_df.set_index(0).sort_values(by=1, ascending=False)\n",
    "        temp_df.index = [generated_samples[i][fold][0].columns[a] for a in temp_df.index]\n",
    "        RF_feats_imb[i][fold] = temp_df.copy()\n",
    "\n",
    "        temp_df = pd.DataFrame(zip(range(len(generated_samples[i][fold][0].columns)),\n",
    "                                   RF_models_GAN[i][fold].feature_importances_))\n",
    "        temp_df = temp_df.set_index(0).sort_values(by=1, ascending=False)\n",
    "        temp_df.index = [generated_samples[i][fold][0].columns[a] for a in temp_df.index]\n",
    "        RF_feats_GAN[i][fold] = temp_df.copy()\n",
    "\n",
    "        temp_df = pd.DataFrame(zip(range(len(generated_samples[i][fold][0].columns)),\n",
    "                                   RF_models_GAN_bal[i][fold].feature_importances_))\n",
    "        temp_df = temp_df.set_index(0).sort_values(by=1, ascending=False)\n",
    "        temp_df.index = [generated_samples[i][fold][0].columns[a] for a in temp_df.index]\n",
    "        RF_feats_GAN_bal[i][fold] = temp_df.copy()\n",
    "\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49b4b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_feats_GAN_mean = {}\n",
    "RF_feats_imb_mean = {}\n",
    "RF_feats_GAN_bal_mean = {}\n",
    "\n",
    "for i in RF_models_real.keys():\n",
    "    RF_feats_GAN_mean[i] = []\n",
    "    RF_feats_imb_mean[i] = []\n",
    "    RF_feats_GAN_bal_mean[i] = []\n",
    "    for fold in RF_models_real[i]:\n",
    "        if fold == 1:\n",
    "            temp_df_imb = RF_models_real[i][fold].feature_importances_\n",
    "            temp_df_bal = RF_models_GAN_bal[i][fold].feature_importances_\n",
    "            temp_df_GAN = RF_models_GAN[i][fold].feature_importances_\n",
    "        else:\n",
    "            temp_df_imb = temp_df_imb + RF_models_real[i][fold].feature_importances_\n",
    "            temp_df_bal = temp_df_bal + RF_models_GAN_bal[i][fold].feature_importances_\n",
    "            temp_df_GAN = temp_df_GAN + RF_models_GAN[i][fold].feature_importances_\n",
    "\n",
    "    temp_df_imb = temp_df_imb / len(RF_models_real[i])\n",
    "    temp_df_imb = pd.DataFrame(zip(range(len(generated_samples[i][fold][0].columns)), temp_df_imb))\n",
    "    temp_df_imb = temp_df_imb.set_index(0).sort_values(by=1, ascending=False)\n",
    "    temp_df_imb.index = [generated_samples[i][fold][0].columns[a] for a in temp_df_imb.index]\n",
    "    RF_feats_imb_mean[i] = temp_df_imb.copy()\n",
    "\n",
    "    temp_df_bal = temp_df_bal / len(RF_models_GAN_bal[i])\n",
    "    temp_df_bal = pd.DataFrame(zip(range(len(generated_samples[i][fold][0].columns)), temp_df_bal))\n",
    "    temp_df_bal = temp_df_bal.set_index(0).sort_values(by=1, ascending=False)\n",
    "    temp_df_bal.index = [generated_samples[i][fold][0].columns[a] for a in temp_df_bal.index]\n",
    "    RF_feats_GAN_bal_mean[i] = temp_df_bal.copy()\n",
    "\n",
    "    temp_df_GAN = temp_df_GAN / len(RF_models_GAN[i])\n",
    "    temp_df_GAN = pd.DataFrame(zip(range(len(generated_samples[i][fold][0].columns)), temp_df_GAN))\n",
    "    temp_df_GAN = temp_df_GAN.set_index(0).sort_values(by=1, ascending=False)\n",
    "    temp_df_GAN.index = [generated_samples[i][fold][0].columns[a] for a in temp_df_GAN.index]\n",
    "    RF_feats_GAN_mean[i] = temp_df_GAN.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd734b8",
   "metadata": {},
   "source": [
    "Calculate intersection of important features from top 1 to top (number of features in the dataset) between the complete dataset (averaged over 20 iterations) and a iteration of the complete dataset, the imbalanced dataset, the balanced dataset and the GAN dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3dce16",
   "metadata": {},
   "outputs": [],
   "source": [
    "intersections_RF = {}\n",
    "for sep in RF_feats_real1:\n",
    "    intersections_RF[sep] = []\n",
    "    for i in range(1, len(RF_feats_real1[sep])):\n",
    "        intersections_RF[sep].append(len(np.intersect1d(RF_feats_real1[sep].index[:i], RF_feats_real[sep].index[:i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac96ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "intersections_RF_bal = {}\n",
    "for cl in RF_feats_GAN_bal_mean:\n",
    "    int_bal = []\n",
    "    for i in range(1, len(RF_feats_real[cl])):\n",
    "        int_bal.append(len(np.intersect1d(RF_feats_real[cl].index[:i], RF_feats_GAN_bal_mean[cl].index[:i])))\n",
    "    intersections_RF_bal[cl] = int_bal\n",
    "\n",
    "print('Intersections - Dataset Balanced - Finished')\n",
    "\n",
    "intersections_RF_GAN = {}\n",
    "for cl in RF_feats_GAN_mean:\n",
    "    int_GAN = []\n",
    "    for i in range(1, len(RF_feats_real[cl])):\n",
    "        int_GAN.append(len(np.intersect1d(RF_feats_real[cl].index[:i], RF_feats_GAN_mean[cl].index[:i])))\n",
    "    intersections_RF_GAN[cl] = int_GAN\n",
    "\n",
    "print('Intersections - Dataset GAN - Finished')\n",
    "\n",
    "intersections_RF_imb = {}\n",
    "for cl in RF_feats_imb_mean:\n",
    "    int_imb = []\n",
    "    for i in range(1, len(RF_feats_real[cl])):\n",
    "        int_imb.append(len(np.intersect1d(RF_feats_real[cl].index[:i], RF_feats_imb_mean[cl].index[:i])))\n",
    "    intersections_RF_imb[cl] = int_imb\n",
    "\n",
    "print('Intersections - Dataset Imbalanced - Finished')\n",
    "\n",
    "# See intersections if features were randomly shuffled\n",
    "random_intersections_RF = []\n",
    "copy_shuffle = list(RF_feats_real[1.2].index).copy()\n",
    "np.random.shuffle(copy_shuffle)\n",
    "for i in range(1, len(RF_feats_real[1.2])):\n",
    "    random_intersections_RF.append(len(np.intersect1d(RF_feats_real[1.2].index[:i], copy_shuffle[:i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f59237",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axs = plt.subplots(3,2,figsize=(12,12))\n",
    "# Graph depicting intersection of important features\n",
    "for i,ax in zip(intersections_RF_imb, axs.ravel()):\n",
    "    ax.scatter(range(1,len(intersections_RF[i])+1), \n",
    "               np.array(intersections_RF[i]) / np.array(range(1,len(intersections_RF[i])+1)),\n",
    "            label = 'Real-Real Intersections', color='Black', s=5)\n",
    "    \n",
    "    ax.scatter(range(1,len(intersections_RF[i])+1),\n",
    "               np.array(intersections_RF_imb[i]) / np.array(range(1,len(intersections_RF[i])+1)),\n",
    "                label = 'Imbalanced Real Dataset', color='Green', s=5)\n",
    "    ax.scatter(range(1,len(intersections_RF[i])+1),\n",
    "               np.array(intersections_RF_bal[i]) / np.array(range(1,len(intersections_RF[i])+1)),\n",
    "                label = 'GAN Augmented Real Dataset', color='Red', s=5)\n",
    "    ax.scatter(range(1,len(intersections_RF[i])+1),\n",
    "               np.array(intersections_RF_GAN[i]) / np.array(range(1,len(intersections_RF[i])+1)),\n",
    "                label = 'GAN Samples Dataset', color='Blue', s=5)\n",
    "    \n",
    "    ax.scatter(range(1,len(intersections_RF[i])+1),\n",
    "               np.array(random_intersections_RF) / np.array(range(1,len(intersections_RF[i])+1)),\n",
    "            label = 'Random Intersections', color='Orange', s=5)\n",
    "    \n",
    "    ax.set_title(i, fontsize=15)\n",
    "    ax.set_xlim([0,len(intersections_RF[i])//4])\n",
    "f.supxlabel('Nº of Top Important (Gini Importance) Compounds', fontsize=15, y=0.05)\n",
    "f.supylabel('Fraction of Common Compounds', fontsize=15, x=0.06)\n",
    "\n",
    "axs[0][1].legend(loc='upper left', fontsize=11, bbox_to_anchor=(1,1), ncol=1, markerscale=3)\n",
    "plt.suptitle('Random Forest', fontsize=18, y=0.93)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39a5ce8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "f, axs = plt.subplots(1,3,figsize=(16,4), constrained_layout=True)\n",
    "# Graph depicting intersection of important features\n",
    "for i in intersections_RF_imb:\n",
    "    axs[0].scatter(range(1,len(intersections_RF[i])+1),\n",
    "               np.array(intersections_RF_imb[i]) / np.array(range(1,len(intersections_RF[i])+1)), label =i, s=5)\n",
    "    \n",
    "    axs[1].scatter(range(1,len(intersections_RF[i])+1),\n",
    "               np.array(intersections_RF_bal[i]) / np.array(range(1,len(intersections_RF[i])+1)), label =i, s=5)\n",
    "    \n",
    "    axs[2].scatter(range(1,len(intersections_RF[i])+1),\n",
    "               np.array(intersections_RF_GAN[i]) / np.array(range(1,len(intersections_RF[i])+1)), label =i, s=5)\n",
    "\n",
    "axs[0].set_title('Imbalanced Real Dataset', fontsize=15)\n",
    "axs[1].set_title('GAN Augmented Real Dataset', fontsize=15)\n",
    "axs[2].set_title('GAN Samples Dataset', fontsize=15)\n",
    "    \n",
    "axs[0].set_xlim([0,len(intersections_RF[i])//4])\n",
    "axs[1].set_xlim([0,len(intersections_RF[i])//4])\n",
    "axs[2].set_xlim([0,len(intersections_RF[i])//4])\n",
    "\n",
    "f.supxlabel('Nº of Top Important (Gini Importance) Compounds', fontsize=15)\n",
    "f.supylabel('Fraction of Common Compounds', fontsize=15)\n",
    "\n",
    "axs[2].legend(loc='upper left', fontsize=11, bbox_to_anchor=(1,1), ncol=1, markerscale=3)\n",
    "plt.suptitle('Random Forest', fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c767752",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 2nd complete model but with only 1 iteration to compare the important features\n",
    "np.random.seed(11485)\n",
    "n_fold = 5\n",
    "PLSDA_model_real1 = {}\n",
    "PLSDA_feats_real1 = {}\n",
    "for sep in dfs:\n",
    "    PLSDA_model_real1[sep] = ma.PLSDA_model_CV(dfs[sep], lbls_orig[sep],\n",
    "                                   n_comp=4, iter_num=1, n_fold=n_fold, feat_type='VIP')\n",
    "    PLSDA_feats_real1[sep] = pd.DataFrame(PLSDA_model_real1[sep][\n",
    "        'important_features']).set_index(0).sort_values(by=1, ascending=False)\n",
    "    PLSDA_feats_real1[sep].index = [dfs[sep].columns[i] for i in PLSDA_feats_real1[sep].index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb917c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "PLSDA_feats_GAN = {}\n",
    "PLSDA_feats_imb = {}\n",
    "PLSDA_feats_GAN_bal = {}\n",
    "\n",
    "for i in PLSDA_models_real.keys():\n",
    "    PLSDA_feats_GAN[i] = {}\n",
    "    PLSDA_feats_imb[i] = {}\n",
    "    PLSDA_feats_GAN_bal[i] = {}\n",
    "    for fold in PLSDA_models_real[i]:\n",
    "        vips = ma._calculate_vips(PLSDA_models_real[i][fold])\n",
    "        temp_df = pd.DataFrame(zip(range(len(generated_samples[i][fold][0].columns)), vips))\n",
    "        temp_df = temp_df.set_index(0).sort_values(by=1, ascending=False)\n",
    "        temp_df.index = [generated_samples[i][fold][0].columns[a] for a in temp_df.index]\n",
    "        PLSDA_feats_imb[i][fold] = temp_df.copy()\n",
    "\n",
    "        vips = ma._calculate_vips(PLSDA_models_GAN[i][fold])\n",
    "        temp_df = pd.DataFrame(zip(range(len(generated_samples[i][fold][0].columns)), vips))\n",
    "        temp_df = temp_df.set_index(0).sort_values(by=1, ascending=False)\n",
    "        temp_df.index = [generated_samples[i][fold][0].columns[a] for a in temp_df.index]\n",
    "        PLSDA_feats_GAN[i][fold] = temp_df.copy()\n",
    "\n",
    "        vips = ma._calculate_vips(PLSDA_models_GAN_bal[i][fold])\n",
    "        temp_df = pd.DataFrame(zip(range(len(generated_samples[i][fold][0].columns)), vips))\n",
    "        temp_df = temp_df.set_index(0).sort_values(by=1, ascending=False)\n",
    "        temp_df.index = [generated_samples[i][fold][0].columns[a] for a in temp_df.index]\n",
    "        PLSDA_feats_GAN_bal[i][fold] = temp_df.copy()\n",
    "    \n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8753531d",
   "metadata": {},
   "outputs": [],
   "source": [
    "PLSDA_feats_GAN_mean = {}\n",
    "PLSDA_feats_imb_mean = {}\n",
    "PLSDA_feats_GAN_bal_mean = {}\n",
    "\n",
    "for i in PLSDA_models_real.keys():\n",
    "    PLSDA_feats_GAN_mean[i] = []\n",
    "    PLSDA_feats_imb_mean[i] = []\n",
    "    PLSDA_feats_GAN_bal_mean[i] = []\n",
    "    for fold in PLSDA_models_real[i]:\n",
    "        if fold == 1:\n",
    "            vips = ma._calculate_vips(PLSDA_models_real[i][fold])\n",
    "            temp_df_imb = vips\n",
    "            vips = ma._calculate_vips(PLSDA_models_GAN_bal[i][fold])\n",
    "            temp_df_bal = vips\n",
    "            vips = ma._calculate_vips(PLSDA_models_GAN[i][fold])\n",
    "            temp_df_GAN = vips\n",
    "        else:\n",
    "            vips = ma._calculate_vips(PLSDA_models_real[i][fold])\n",
    "            temp_df_imb = temp_df_imb + vips\n",
    "            vips = ma._calculate_vips(PLSDA_models_GAN_bal[i][fold])\n",
    "            temp_df_bal = temp_df_bal + vips\n",
    "            vips = ma._calculate_vips(PLSDA_models_GAN[i][fold])\n",
    "            temp_df_GAN = temp_df_GAN + vips\n",
    "\n",
    "    temp_df_imb = temp_df_imb / len(PLSDA_models_real[i])\n",
    "    temp_df_imb = pd.DataFrame(zip(range(len(generated_samples[i][fold][0].columns)), temp_df_imb))\n",
    "    temp_df_imb = temp_df_imb.set_index(0).sort_values(by=1, ascending=False)\n",
    "    temp_df_imb.index = [generated_samples[i][fold][0].columns[a] for a in temp_df_imb.index]\n",
    "    PLSDA_feats_imb_mean[i] = temp_df_imb.copy()\n",
    "\n",
    "    temp_df_bal = temp_df_bal / len(PLSDA_models_GAN_bal[i])\n",
    "    temp_df_bal = pd.DataFrame(zip(range(len(generated_samples[i][fold][0].columns)), temp_df_bal))\n",
    "    temp_df_bal = temp_df_bal.set_index(0).sort_values(by=1, ascending=False)\n",
    "    temp_df_bal.index = [generated_samples[i][fold][0].columns[a] for a in temp_df_bal.index]\n",
    "    PLSDA_feats_GAN_bal_mean[i] = temp_df_bal.copy()\n",
    "\n",
    "    temp_df_GAN = temp_df_GAN / len(PLSDA_models_GAN[i])\n",
    "    temp_df_GAN = pd.DataFrame(zip(range(len(generated_samples[i][fold][0].columns)), temp_df_GAN))\n",
    "    temp_df_GAN = temp_df_GAN.set_index(0).sort_values(by=1, ascending=False)\n",
    "    temp_df_GAN.index = [generated_samples[i][fold][0].columns[a] for a in temp_df_GAN.index]\n",
    "    PLSDA_feats_GAN_mean[i] = temp_df_GAN.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3f892e",
   "metadata": {},
   "source": [
    "Calculate intersection of important features from top 1 to top (number of features in the dataset) between the complete dataset (averaged over 20 iterations) and a iteration of the complete dataset, the imbalanced dataset, the balanced dataset and the GAN dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8dedcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "intersections_PLSDA = {}\n",
    "for sep in PLSDA_feats_real1:\n",
    "    intersections_PLSDA[sep] = []\n",
    "    for i in range(1, len(PLSDA_feats_real1[sep])):\n",
    "        intersections_PLSDA[sep].append(len(np.intersect1d(PLSDA_feats_real1[sep].index[:i],\n",
    "                                                           PLSDA_feats_real[sep].index[:i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd22122e",
   "metadata": {},
   "outputs": [],
   "source": [
    "intersections_PLSDA_bal = {}\n",
    "for cl in PLSDA_feats_GAN_bal_mean:\n",
    "    int_bal = []\n",
    "    for i in range(1, len(PLSDA_feats_real[cl])):\n",
    "        int_bal.append(len(np.intersect1d(PLSDA_feats_real[cl].index[:i], PLSDA_feats_GAN_bal_mean[cl].index[:i])))\n",
    "    intersections_PLSDA_bal[cl] = int_bal\n",
    "\n",
    "print('Intersections - Dataset Balanced - Finished')\n",
    "\n",
    "intersections_PLSDA_GAN = {}\n",
    "for cl in PLSDA_feats_GAN_mean:\n",
    "    int_GAN = []\n",
    "    for i in range(1, len(PLSDA_feats_real[cl])):\n",
    "        int_GAN.append(len(np.intersect1d(PLSDA_feats_real[cl].index[:i], PLSDA_feats_GAN_mean[cl].index[:i])))\n",
    "    intersections_PLSDA_GAN[cl] = int_GAN\n",
    "\n",
    "print('Intersections - Dataset GAN - Finished')\n",
    "\n",
    "intersections_PLSDA_imb = {}\n",
    "for cl in PLSDA_feats_imb_mean:\n",
    "    int_imb = []\n",
    "    for i in range(1, len(PLSDA_feats_real[cl])):\n",
    "        int_imb.append(len(np.intersect1d(PLSDA_feats_real[cl].index[:i], PLSDA_feats_imb_mean[cl].index[:i])))\n",
    "    intersections_PLSDA_imb[cl] = int_imb\n",
    "\n",
    "print('Intersections - Dataset Imbalanced - Finished')\n",
    "\n",
    "# See intersections if features were randomly shuffled\n",
    "random_intersections_PLSDA = []\n",
    "copy_shuffle = list(PLSDA_feats_real[1.2].index).copy()\n",
    "np.random.shuffle(copy_shuffle)\n",
    "for i in range(1, len(PLSDA_feats_real[1.2])):\n",
    "    random_intersections_PLSDA.append(len(np.intersect1d(PLSDA_feats_real[1.2].index[:i], copy_shuffle[:i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b2a7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axs = plt.subplots(3,2,figsize=(12,12))\n",
    "# Graph depicting intersection of important features\n",
    "for i,ax in zip(intersections_PLSDA_imb, axs.ravel()):\n",
    "    ax.scatter(range(1,len(intersections_PLSDA[i])+1), \n",
    "               np.array(intersections_PLSDA[i]) / np.array(range(1,len(intersections_PLSDA[i])+1)),\n",
    "            label = 'Real-Real Intersections', color='Black', s=5)\n",
    "    \n",
    "    ax.scatter(range(1,len(intersections_PLSDA[i])+1),\n",
    "               np.array(intersections_PLSDA_imb[i]) / np.array(range(1,len(intersections_PLSDA[i])+1)),\n",
    "                label = 'Imbalanced Real Dataset', color='Green', s=5)\n",
    "    ax.scatter(range(1,len(intersections_PLSDA[i])+1),\n",
    "               np.array(intersections_PLSDA_bal[i]) / np.array(range(1,len(intersections_PLSDA[i])+1)),\n",
    "                label = 'GAN Augmented Real Dataset', color='Red', s=5)\n",
    "    ax.scatter(range(1,len(intersections_PLSDA[i])+1),\n",
    "               np.array(intersections_PLSDA_GAN[i]) / np.array(range(1,len(intersections_PLSDA[i])+1)),\n",
    "                label = 'GAN Samples Dataset', color='Blue', s=5)\n",
    "    \n",
    "    ax.scatter(range(1,len(intersections_PLSDA[i])+1),\n",
    "               np.array(random_intersections_PLSDA) / np.array(range(1,len(intersections_PLSDA[i])+1)),\n",
    "            label = 'Random Intersections', color='Orange', s=5)\n",
    "    \n",
    "    ax.set_title(i, fontsize=15)\n",
    "    ax.set_xlim([0,len(intersections_PLSDA[i])//4])\n",
    "\n",
    "f.supxlabel('Nº of Top Important (VIP Score) Compounds', fontsize=15, y=0.05)\n",
    "f.supylabel('Fraction of Common Compounds', fontsize=15, x=0.06)\n",
    "\n",
    "axs[0][1].legend(loc='upper left', fontsize=11, bbox_to_anchor=(1,1), ncol=1, markerscale=3)\n",
    "plt.suptitle('PLS-DA', fontsize=18, y=0.93)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff52805",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axs = plt.subplots(1,3,figsize=(16,4), constrained_layout=True)\n",
    "# Graph depicting intersection of important features\n",
    "for i in intersections_RF_imb:\n",
    "    axs[0].scatter(range(1,len(intersections_PLSDA[i])+1),\n",
    "               np.array(intersections_PLSDA_imb[i]) / np.array(range(1,len(intersections_PLSDA[i])+1)), label =i, s=5)\n",
    "    \n",
    "    axs[1].scatter(range(1,len(intersections_PLSDA[i])+1),\n",
    "               np.array(intersections_PLSDA_bal[i]) / np.array(range(1,len(intersections_PLSDA[i])+1)), label =i, s=5)\n",
    "    \n",
    "    axs[2].scatter(range(1,len(intersections_PLSDA[i])+1),\n",
    "               np.array(intersections_PLSDA_GAN[i]) / np.array(range(1,len(intersections_PLSDA[i])+1)), label =i, s=5)\n",
    "\n",
    "axs[0].set_title('Imbalanced Real Dataset', fontsize=15)\n",
    "axs[1].set_title('GAN Augmented Real Dataset', fontsize=15)\n",
    "axs[2].set_title('GAN Samples Dataset', fontsize=15)\n",
    "    \n",
    "axs[0].set_xlim([0,len(intersections_PLSDA[i])//4])\n",
    "axs[1].set_xlim([0,len(intersections_PLSDA[i])//4])\n",
    "axs[2].set_xlim([0,len(intersections_PLSDA[i])//4])\n",
    "\n",
    "f.supxlabel('Nº of Top Important (VIP Score) Compounds', fontsize=15)\n",
    "f.supylabel('Fraction of Common Compounds', fontsize=15)\n",
    "\n",
    "axs[2].legend(loc='upper left', fontsize=11, bbox_to_anchor=(1,1), ncol=1, markerscale=3)\n",
    "plt.suptitle('PLS-DA', fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d4a06e",
   "metadata": {},
   "source": [
    "#### Comparing Against Significant Feature (by Univariate Analysis) of the Complete models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28368e71",
   "metadata": {},
   "source": [
    "Calculate intersection of important features from top 1 to top (number of features in the dataset) between the complete dataset (averaged over 20 iterations) and a iteration of the complete dataset, the imbalanced dataset, the balanced dataset and the GAN dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a08e86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "intersections_RF_uni = {}\n",
    "for sep in uni_results:\n",
    "    intersections_RF_uni[sep] = []\n",
    "    for i in range(1, len(uni_results[sep])):\n",
    "        intersections_RF_uni[sep].append(len(np.intersect1d(RF_feats_real1[sep].index[:i], uni_results[sep].index[:i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93e7ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "intersections_RF_bal_uni = {}\n",
    "for cl in RF_feats_GAN_bal_mean:\n",
    "    int_bal = []\n",
    "    for i in range(1, len(uni_results[cl])):\n",
    "        int_bal.append(len(np.intersect1d(uni_results[cl].index[:i], RF_feats_GAN_bal_mean[cl].index[:i])))\n",
    "    intersections_RF_bal_uni[cl] = int_bal\n",
    "\n",
    "print('Intersections - Dataset Balanced - Finished')\n",
    "\n",
    "intersections_RF_GAN_uni = {}\n",
    "for cl in RF_feats_GAN_mean:\n",
    "    int_GAN = []\n",
    "    for i in range(1, len(uni_results[cl])):\n",
    "        int_GAN.append(len(np.intersect1d(uni_results[cl].index[:i], RF_feats_GAN_mean[cl].index[:i])))\n",
    "    intersections_RF_GAN_uni[cl] = int_GAN\n",
    "\n",
    "print('Intersections - Dataset GAN - Finished')\n",
    "\n",
    "intersections_RF_imb_uni = {}\n",
    "for cl in RF_feats_imb_mean:\n",
    "    int_imb = []\n",
    "    for i in range(1, len(uni_results[cl])):\n",
    "        int_imb.append(len(np.intersect1d(uni_results[cl].index[:i], RF_feats_imb_mean[cl].index[:i])))\n",
    "    intersections_RF_imb_uni[cl] = int_imb\n",
    "\n",
    "print('Intersections - Dataset Imbalanced - Finished')\n",
    "\n",
    "# See intersections if features were randomly shuffled\n",
    "random_intersections_RF_uni = []\n",
    "copy_shuffle = list(uni_results[cl].index).copy()\n",
    "np.random.shuffle(copy_shuffle)\n",
    "for i in range(1, len(uni_results[cl])):\n",
    "    random_intersections_RF_uni.append(len(np.intersect1d(uni_results[cl].index[:i], copy_shuffle[:i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c634ec25",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axs = plt.subplots(3,2,figsize=(12,12))\n",
    "# Graph depicting intersection of important features\n",
    "for i,ax in zip(intersections_RF_imb_uni, axs.ravel()):\n",
    "    ax.scatter(range(1,len(intersections_RF_uni[i])+1), \n",
    "               np.array(intersections_RF_uni[i]) / np.array(range(1,len(intersections_RF_uni[i])+1)),\n",
    "            label = 'Real-Real Intersections', color='Black', s=5)\n",
    "    \n",
    "    ax.scatter(range(1,len(intersections_RF_uni[i])+1),\n",
    "               np.array(intersections_RF_imb_uni[i]) / np.array(range(1,len(intersections_RF_uni[i])+1)),\n",
    "                label = 'Imbalanced Real Dataset', color='Green', s=5)\n",
    "    ax.scatter(range(1,len(intersections_RF_uni[i])+1),\n",
    "               np.array(intersections_RF_bal_uni[i]) / np.array(range(1,len(intersections_RF_uni[i])+1)),\n",
    "                label = 'GAN Augmented Real Dataset', color='Red', s=5)\n",
    "    ax.scatter(range(1,len(intersections_RF_uni[i])+1),\n",
    "               np.array(intersections_RF_GAN_uni[i]) / np.array(range(1,len(intersections_RF_uni[i])+1)),\n",
    "                label = 'GAN Samples Dataset', color='Blue', s=5)\n",
    "    \n",
    "    ax.scatter(range(1,len(intersections_RF_uni[i])+1),\n",
    "               np.array(random_intersections_RF_uni) / np.array(range(1,len(intersections_RF_uni[i])+1)),\n",
    "            label = 'Random Intersections', color='Orange', s=5)\n",
    "    \n",
    "    ax.set_title(i, fontsize=15)\n",
    "    ax.set_xlim([0,len(intersections_RF_uni[i])//4])\n",
    "\n",
    "f.supxlabel('Nº of Top Important (Gini Importance) Compounds', fontsize=15, y=0.05)\n",
    "f.supylabel('Fraction of Common Compounds', fontsize=15, x=0.06)\n",
    "\n",
    "axs[0][1].legend(loc='upper left', fontsize=11, bbox_to_anchor=(1,1), ncol=1, markerscale=3)\n",
    "plt.suptitle('Random Forest - Univariate Analysis Comparison', fontsize=18, y=0.93)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b767ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axs = plt.subplots(1,3,figsize=(16,4), constrained_layout=True)\n",
    "# Graph depicting intersection of important features\n",
    "for i in intersections_RF_imb_uni:\n",
    "    axs[0].scatter(range(1,len(intersections_RF_uni[i])+1),\n",
    "               np.array(intersections_RF_imb_uni[i]) / np.array(range(1,len(intersections_RF_uni[i])+1)), label =i, s=5)\n",
    "    \n",
    "    axs[1].scatter(range(1,len(intersections_RF[i])+1),\n",
    "               np.array(intersections_RF_bal_uni[i]) / np.array(range(1,len(intersections_RF_uni[i])+1)), label =i, s=5)\n",
    "    \n",
    "    axs[2].scatter(range(1,len(intersections_RF[i])+1),\n",
    "               np.array(intersections_RF_GAN_uni[i]) / np.array(range(1,len(intersections_RF_uni[i])+1)), label =i, s=5)\n",
    "\n",
    "axs[0].set_title('Imbalanced Real Dataset', fontsize=15)\n",
    "axs[1].set_title('GAN Augmented Real Dataset', fontsize=15)\n",
    "axs[2].set_title('GAN Samples Dataset', fontsize=15)\n",
    "    \n",
    "axs[0].set_xlim([0,len(intersections_RF_uni[i])//4])\n",
    "axs[1].set_xlim([0,len(intersections_RF_uni[i])//4])\n",
    "axs[2].set_xlim([0,len(intersections_RF_uni[i])//4])\n",
    "\n",
    "f.supxlabel('Nº of Top Important (Gini Importance) Compounds', fontsize=15)\n",
    "f.supylabel('Fraction of Common Compounds', fontsize=15)\n",
    "\n",
    "axs[2].legend(loc='upper left', fontsize=11, bbox_to_anchor=(1,1), ncol=1, markerscale=3)\n",
    "plt.suptitle('Random Forest', fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a6257d",
   "metadata": {},
   "source": [
    "Calculate intersection of important features from top 1 to top (number of features in the dataset) between the complete dataset (averaged over 20 iterations) and a iteration of the complete dataset, the imbalanced dataset, the balanced dataset and the GAN dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65e8e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "intersections_PLSDA_uni = {}\n",
    "for sep in uni_results:\n",
    "    intersections_PLSDA_uni[sep] = []\n",
    "    for i in range(1, len(uni_results[sep])):\n",
    "        intersections_PLSDA_uni[sep].append(len(np.intersect1d(PLSDA_feats_real1[sep].index[:i],\n",
    "                                                               uni_results[sep].index[:i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d37a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "intersections_PLSDA_bal_uni = {}\n",
    "for cl in PLSDA_feats_GAN_bal_mean:\n",
    "    int_bal = []\n",
    "    for i in range(1, len(uni_results[cl])):\n",
    "        int_bal.append(len(np.intersect1d(uni_results[cl].index[:i], PLSDA_feats_GAN_bal_mean[cl].index[:i])))\n",
    "    intersections_PLSDA_bal_uni[cl] = int_bal\n",
    "\n",
    "print('Intersections - Dataset Balanced - Finished')\n",
    "\n",
    "intersections_PLSDA_GAN_uni = {}\n",
    "for cl in PLSDA_feats_GAN_mean:\n",
    "    int_GAN = []\n",
    "    for i in range(1, len(uni_results[cl])):\n",
    "        int_GAN.append(len(np.intersect1d(uni_results[cl].index[:i], PLSDA_feats_GAN_mean[cl].index[:i])))\n",
    "    intersections_PLSDA_GAN_uni[cl] = int_GAN\n",
    "\n",
    "print('Intersections - Dataset GAN - Finished')\n",
    "\n",
    "intersections_PLSDA_imb_uni = {}\n",
    "for cl in PLSDA_feats_imb_mean:\n",
    "    int_imb = []\n",
    "    for i in range(1, len(uni_results[cl])):\n",
    "        int_imb.append(len(np.intersect1d(uni_results[cl].index[:i], PLSDA_feats_imb_mean[cl].index[:i])))\n",
    "    intersections_PLSDA_imb_uni[cl] = int_imb\n",
    "\n",
    "print('Intersections - Dataset Imbalanced - Finished')\n",
    "\n",
    "# See intersections if features were randomly shuffled\n",
    "random_intersections_PLSDA_uni = []\n",
    "copy_shuffle = list(uni_results[cl].index).copy()\n",
    "np.random.shuffle(copy_shuffle)\n",
    "for i in range(1, len(uni_results[cl])):\n",
    "    random_intersections_PLSDA_uni.append(len(np.intersect1d(uni_results[cl].index[:i], copy_shuffle[:i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d08075",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axs = plt.subplots(3,2,figsize=(12,12))\n",
    "# Graph depicting intersection of important features\n",
    "for i,ax in zip(intersections_PLSDA_imb_uni, axs.ravel()):\n",
    "    ax.scatter(range(1,len(intersections_PLSDA_uni[i])+1), \n",
    "               np.array(intersections_PLSDA_uni[i]) / np.array(range(1,len(intersections_PLSDA_uni[i])+1)),\n",
    "            label = 'Real-Real Intersections', color='Black', s=5)\n",
    "    \n",
    "    ax.scatter(range(1,len(intersections_PLSDA_uni[i])+1),\n",
    "               np.array(intersections_PLSDA_imb_uni[i]) / np.array(range(1,len(intersections_PLSDA_uni[i])+1)),\n",
    "                label = 'Imbalanced Real Dataset', color='Green', s=5)\n",
    "    ax.scatter(range(1,len(intersections_PLSDA_uni[i])+1),\n",
    "               np.array(intersections_PLSDA_bal_uni[i]) / np.array(range(1,len(intersections_PLSDA_uni[i])+1)),\n",
    "                label = 'GAN Augmented Real Dataset', color='Red', s=5)\n",
    "    ax.scatter(range(1,len(intersections_PLSDA_uni[i])+1),\n",
    "               np.array(intersections_PLSDA_GAN_uni[i]) / np.array(range(1,len(intersections_PLSDA_uni[i])+1)),\n",
    "                label = 'GAN Samples Dataset', color='Blue', s=5)\n",
    "    ax.scatter(range(1,len(intersections_PLSDA_uni[i])+1),\n",
    "               np.array(intersections_PLSDA_RUS_uni[i]) / np.array(range(1,len(intersections_PLSDA_uni[i])+1)),\n",
    "                label = 'UnderSampled Dataset', color='Purple', s=5)\n",
    "    \n",
    "    ax.scatter(range(1,len(intersections_PLSDA_uni[i])+1),\n",
    "               np.array(random_intersections_PLSDA_uni) / np.array(range(1,len(intersections_PLSDA_uni[i])+1)),\n",
    "            label = 'Random Intersections', color='Orange', s=5)\n",
    "    \n",
    "    ax.set_title(i, fontsize=15)\n",
    "    ax.set_xlim([0,len(intersections_PLSDA_uni[i])//4])\n",
    "\n",
    "#axl.legend(loc='center left', fontsize=11, bbox_to_anchor=(-0.2,-0.15), ncol=5)\n",
    "#axl.set_ylabel('Fraction of Common Compounds', fontsize=15)\n",
    "#axl.set_xlim([0,len(intersections_RF)//8])\n",
    "#axl.set_ylim([0,1.01])\n",
    "\n",
    "f.supxlabel('Nº of Top Important (VIP Score) Compounds', fontsize=15, y=0.05)\n",
    "f.supylabel('Fraction of Common Compounds', fontsize=15, x=0.06)\n",
    "\n",
    "#axr.set_xlim([0,len(intersections_RF)//8])\n",
    "#axr.set_ylim([0,1.01])\n",
    "axs[0][1].legend(loc='upper left', fontsize=11, bbox_to_anchor=(1,1), ncol=1, markerscale=3)\n",
    "plt.suptitle('PLS-DA - Univariate Analysis Comparison', fontsize=18, y=0.93)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a12bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axs = plt.subplots(1,3,figsize=(16,4), constrained_layout=True)\n",
    "# Graph depicting intersection of important features\n",
    "for i in intersections_RF_imb:\n",
    "    axs[0].scatter(range(1,len(intersections_PLSDA_uni[i])+1),\n",
    "               np.array(intersections_PLSDA_imb_uni[i]) / np.array(range(1,len(intersections_PLSDA_uni[i])+1)),\n",
    "                   label =i, s=5)\n",
    "    \n",
    "    axs[1].scatter(range(1,len(intersections_PLSDA_uni[i])+1),\n",
    "               np.array(intersections_PLSDA_bal_uni[i]) / np.array(range(1,len(intersections_PLSDA_uni[i])+1)),\n",
    "                   label =i, s=5)\n",
    "    \n",
    "    axs[2].scatter(range(1,len(intersections_PLSDA_uni[i])+1),\n",
    "               np.array(intersections_PLSDA_GAN_uni[i]) / np.array(range(1,len(intersections_PLSDA_uni[i])+1)),\n",
    "                   label =i, s=5)\n",
    "\n",
    "axs[0].set_title('Imbalanced Real Dataset', fontsize=15)\n",
    "axs[1].set_title('GAN Augmented Real Dataset', fontsize=15)\n",
    "axs[2].set_title('GAN Samples Dataset', fontsize=15)\n",
    "    \n",
    "axs[0].set_xlim([0,len(intersections_PLSDA_uni[i])//4])\n",
    "axs[1].set_xlim([0,len(intersections_PLSDA_uni[i])//4])\n",
    "axs[2].set_xlim([0,len(intersections_PLSDA_uni[i])//4])\n",
    "\n",
    "f.supxlabel('Nº of Top Important (VIP Score) Compounds', fontsize=15)\n",
    "f.supylabel('Fraction of Common Compounds', fontsize=15)\n",
    "\n",
    "axs[2].legend(loc='upper left', fontsize=11, bbox_to_anchor=(1,1), ncol=1, markerscale=3)\n",
    "plt.suptitle('PLS-DA', fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2e935d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
